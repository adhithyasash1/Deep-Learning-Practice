{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":95041,"databundleVersionId":11298874,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pandas numpy matplotlib tqdm torch torchvision scikit-learn albumentations Pillow","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import necessary libraries\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nimport copy\nimport gc\nimport random\nimport logging\nfrom datetime import datetime\nfrom collections import Counter\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split, WeightedRandomSampler\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.models import resnet50, alexnet, vgg16, googlenet\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom sklearn.metrics import f1_score, confusion_matrix, classification_report\nfrom PIL import Image\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler(\"training.log\"),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\n\n# Set random seeds for reproducibility\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    logger.info(f\"Set random seed to {seed}\")\n\n\nseed_everything(42)\n\n# Set device (GPU if available, else CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nlogger.info(f\"Using device: {device}\")\n\n# Define paths\nTRAIN_DIR = '/kaggle/input/deep-learning-practice-week-9-image-c-lassifica/train'\nTEST_DIR = '/kaggle/input/deep-learning-practice-week-9-image-c-lassifica/test'\nOUTPUT_DIR = './'\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nOUTPUT_FILE = os.path.join(OUTPUT_DIR, '21F3000611.csv')  # Replace with your roll number\nMODEL_DIR = os.path.join(OUTPUT_DIR, 'models')\nos.makedirs(MODEL_DIR, exist_ok=True)\nLOG_DIR = os.path.join(OUTPUT_DIR, 'logs')\nos.makedirs(LOG_DIR, exist_ok=True)\n\n\ndef get_train_transforms(height=512, width=512):\n    return A.Compose([\n        A.RandomResizedCrop(height=height, width=width, scale=(0.7, 1.0)),\n        A.Flip(p=0.5),\n        A.RandomRotate90(p=0.5),\n        A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=30, p=0.7),  # Increased limits and probability\n        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.7),   # Increased limits and probability\n        A.HueSaturationValue(hue_shift_limit=15, sat_shift_limit=40, val_shift_limit=30, p=0.5),\n        A.RGBShift(r_shift_limit=20, g_shift_limit=20, b_shift_limit=20, p=0.5),\n        A.OneOf([\n            A.CLAHE(clip_limit=4.0, p=1.0),\n            A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=1.0),  # Enhanced sharpening\n            A.Emboss(alpha=(0.2, 0.5), strength=(0.5, 1.0), p=1.0),\n            A.MotionBlur(blur_limit=7, p=1.0),  # Added motion blur\n        ], p=0.5),\n        A.OneOf([\n            A.GridDistortion(p=1.0),  # Added grid distortion for biological specimens\n            A.ElasticTransform(p=1.0, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),  # Added elastic transform\n            A.OpticalDistortion(distort_limit=0.2, shift_limit=0.15, p=1.0),  # Added optical distortion\n        ], p=0.3),\n        A.CoarseDropout(max_holes=12, max_height=30, max_width=30, fill_value=0, p=0.5),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2(),\n    ])\n\ndef get_valid_transforms(height=384, width=384):  # Match training size\n    return A.Compose([\n        A.Resize(height=height, width=width),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2(),\n    ])\n\n\n# TTA transformations\ndef get_tta_transforms(height=224, width=224, n_transforms=5):\n    tta_transforms = []\n    # Original validation transform\n    tta_transforms.append(get_valid_transforms(height, width))\n\n    # Add different augmentations for TTA\n    tta_transforms.append(A.Compose([\n        A.Resize(height=height, width=width),\n        A.HorizontalFlip(p=1.0),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2(),\n    ]))\n\n    tta_transforms.append(A.Compose([\n        A.Resize(height=height, width=width),\n        A.VerticalFlip(p=1.0),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2(),\n    ]))\n\n    tta_transforms.append(A.Compose([\n        A.Resize(height=height, width=width),\n        A.RandomRotate90(p=1.0),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2(),\n    ]))\n\n    tta_transforms.append(A.Compose([\n        A.Resize(height=height, width=width),\n        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=1.0),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2(),\n    ]))\n\n    return tta_transforms[:n_transforms]  # Return the first n transforms\n\nclass LabelSmoothingCrossEntropyLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.1, class_weights=None):\n        super(LabelSmoothingCrossEntropyLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.classes = classes\n        self.class_weights = class_weights  # Add class weights\n        \n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=-1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing / (self.classes - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n            \n        if self.class_weights is not None:\n            # Apply class weights\n            weights = self.class_weights[target]\n            return torch.mean(torch.sum(-true_dist * pred, dim=-1) * weights)\n        else:\n            return torch.mean(torch.sum(-true_dist * pred, dim=-1))\n\n\n\n# Custom Dataset with Albumentations support\nclass AlbumentationsDataset(Dataset):\n    def __init__(self, root_dir, transform=None, is_test=False):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.is_test = is_test\n\n        if not is_test:\n            # Training/validation mode\n            self.dataset = ImageFolder(root_dir)\n            self.classes = self.dataset.classes\n            self.class_to_idx = self.dataset.class_to_idx\n            self.idx_to_class = {v: k for k, v in self.class_to_idx.items()}\n            self.samples = self.dataset.samples\n\n            # Count samples per class\n            self.class_counts = Counter([label for _, label in self.samples])\n            logger.info(f\"Class distribution: {dict(self.class_counts)}\")\n        else:\n            # Test mode\n            self.image_files = sorted(\n                [f for f in os.listdir(root_dir) if f.lower().endswith(('.jpg', '.png', '.jpeg'))])\n\n    def __len__(self):\n        if not self.is_test:\n            return len(self.samples)\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        if not self.is_test:\n            img_path, label = self.samples[idx]\n            image = Image.open(img_path).convert('RGB')\n            image = np.array(image)\n\n            if self.transform:\n                augmented = self.transform(image=image)\n                image = augmented['image']\n\n            return image, label\n        else:\n            img_path = os.path.join(self.root_dir, self.image_files[idx])\n            image = Image.open(img_path).convert('RGB')\n            image = np.array(image)\n            image_id = os.path.splitext(self.image_files[idx])[0]\n\n            if self.transform:\n                augmented = self.transform(image=image)\n                image = augmented['image']\n\n            return image, image_id\n\n\n# Create datasets with Albumentations\ntrain_dataset = AlbumentationsDataset(TRAIN_DIR, transform=get_train_transforms())\ntest_dataset = AlbumentationsDataset(TEST_DIR, transform=get_valid_transforms(), is_test=True)\n\n# Print dataset info\nlogger.info(f\"Number of training samples: {len(train_dataset)}\")\nlogger.info(f\"Number of test samples: {len(test_dataset)}\")\nlogger.info(f\"Classes: {train_dataset.classes}\")\n\n# Define model creation functions\ndef create_model(model_name, num_classes=10, pretrained=True):\n    if model_name == 'resnet50':\n        model = resnet50(weights='DEFAULT' if pretrained else None)\n        num_ftrs = model.fc.in_features\n        model.fc = nn.Linear(num_ftrs, num_classes)\n\n    elif model_name == 'alexnet':\n        model = alexnet(weights='DEFAULT' if pretrained else None)\n        num_ftrs = model.classifier[6].in_features\n        model.classifier[6] = nn.Linear(num_ftrs, num_classes)\n\n    elif model_name == 'vgg16':\n        model = vgg16(weights='DEFAULT' if pretrained else None)\n        num_ftrs = model.classifier[6].in_features\n        model.classifier[6] = nn.Linear(num_ftrs, num_classes)\n\n    elif model_name == 'googlenet':\n        model = googlenet(weights='DEFAULT' if pretrained else None)\n        num_ftrs = model.fc.in_features\n        model.fc = nn.Linear(num_ftrs, num_classes)\n\n    elif model_name == 'efficientnet_b0':\n        from torchvision.models import efficientnet_b0\n        model = efficientnet_b0(weights='DEFAULT' if pretrained else None)\n        num_ftrs = model.classifier[1].in_features\n        model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n\n    else:\n        raise ValueError(f\"Unsupported model name: {model_name}\")\n\n    return model\n\n\n# Define model configurations\nmodel_configs = [\n    # format: (model_name, learning_rate, weight_decay, batch_size)\n    ('resnet50', 1e-4, 1e-4, 48),\n    ('alexnet', 2e-4, 1e-4, 128),\n    ('vgg16', 5e-5, 1e-4, 24),\n    ('googlenet', 1e-4, 1e-4, 64),\n    ('efficientnet_b0', 8e-5, 1e-4, 64)\n]\n\n\n# Early stopping class\nclass EarlyStopping:\n    \"\"\"Early stops the training if validation F1 doesn't improve after a given patience.\"\"\"\n\n    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last improvement.\n            verbose (bool): If True, prints a message for each improvement.\n            delta (float): Minimum change to qualify as an improvement.\n            path (str): Path to save the checkpoint.\n            trace_func (function): Function used for logging.\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_f1_max = -np.Inf\n        self.delta = delta\n        self.path = path\n        self.trace_func = trace_func\n\n    def __call__(self, val_f1, model):\n        score = val_f1\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_f1, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_f1, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_f1, model):\n        '''Saves model when validation F1 increases.'''\n        if self.verbose:\n            self.trace_func(f'Validation F1 increased ({self.val_f1_max:.6f} --> {val_f1:.6f}). Saving model...')\n        torch.save(model.state_dict(), self.path)\n        self.val_f1_max = val_f1\n\n\n# Improved training function with TensorBoard, early stopping, and gradient clipping\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scheduler,\n                model_name, num_epochs=10, mixup_alpha=0.2, clip_grad_norm=1.0,\n                early_stopping_patience=5):\n    # Set up TensorBoard\n    writer = SummaryWriter(log_dir=os.path.join(LOG_DIR, f\"{model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"))\n\n    # Early stopping\n    early_stopping = EarlyStopping(\n        patience=early_stopping_patience,\n        verbose=True,\n        path=os.path.join(MODEL_DIR, f\"{model_name}_early_stop.pth\"),\n        trace_func=logger.info\n    )\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_f1 = 0.0\n\n    # Define mixup function\n    def mixup_data(x, y, alpha=0.2):\n        '''Returns mixed inputs, pairs of targets, and lambda'''\n        if alpha > 0:\n            lam = np.random.beta(alpha, alpha)\n        else:\n            lam = 1\n\n        batch_size = x.size()[0]\n        index = torch.randperm(batch_size).to(device)\n\n        mixed_x = lam * x + (1 - lam) * x[index, :]\n        y_a, y_b = y, y[index]\n        return mixed_x, y_a, y_b, lam\n\n    def mixup_criterion(criterion, pred, y_a, y_b, lam):\n        return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n\n    # Log the model architecture\n    logger.info(f\"Model architecture: {model_name}\")\n    dummy_input = torch.randn(1, 3, 224, 224).to(device)\n    writer.add_graph(model, dummy_input)\n\n    for epoch in range(num_epochs):\n        logger.info(f'Epoch {epoch + 1}/{num_epochs}')\n        logger.info('-' * 10)\n\n        # Training phase\n        model.train()\n        running_loss = 0.0\n        running_corrects = 0\n        processed_samples = 0\n        all_labels = []\n        all_preds = []\n\n        # Training loop\n        loop = tqdm(train_loader, desc=f'Training')\n        for inputs, labels in loop:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            # Apply mixup with probability 0.5\n            if random.random() < 0.5 and mixup_alpha > 0:\n                inputs, labels_a, labels_b, lam = mixup_data(inputs, labels, alpha=mixup_alpha)\n                # Forward pass\n                outputs = model(inputs)\n                loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n                # For accuracy calculation during mixup, use the dominant label\n                _, preds = torch.max(outputs, 1)\n                running_corrects += torch.sum(preds == labels_a.data).item() * lam\n                running_corrects += torch.sum(preds == labels_b.data).item() * (1 - lam)\n\n                # For F1 calculation, use dominant label\n                dominant_labels = labels_a if lam > 0.5 else labels_b\n                all_labels.extend(dominant_labels.cpu().numpy())\n                all_preds.extend(preds.cpu().numpy())\n            else:\n                # Standard forward pass\n                outputs = model(inputs)\n                _, preds = torch.max(outputs, 1)\n                loss = criterion(outputs, labels)\n                running_corrects += torch.sum(preds == labels.data).item()\n\n                # Store labels and predictions for F1 calculation\n                all_labels.extend(labels.cpu().numpy())\n                all_preds.extend(preds.cpu().numpy())\n\n            # Zero the parameter gradients\n            optimizer.zero_grad()\n\n            # Backward pass\n            loss.backward()\n\n            # Gradient clipping\n            if clip_grad_norm > 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n\n            # Optimize\n            optimizer.step()\n\n            # Statistics\n            batch_size = inputs.size(0)\n            running_loss += loss.item() * batch_size\n            processed_samples += batch_size\n\n            # Update progress bar with minimal info\n            loop.set_postfix(loss=loss.item())\n\n            # Clear GPU cache to free memory\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n\n        epoch_loss = running_loss / processed_samples\n        epoch_acc = running_corrects / processed_samples * 100\n        epoch_f1 = f1_score(all_labels, all_preds, average='weighted')\n\n        logger.info(f'Training Loss: {epoch_loss:.4f} Acc: {epoch_acc:.2f}% F1: {epoch_f1:.4f}')\n\n        # Log metrics to TensorBoard\n        writer.add_scalar('Loss/train', epoch_loss, epoch)\n        writer.add_scalar('Accuracy/train', epoch_acc, epoch)\n        writer.add_scalar('F1/train', epoch_f1, epoch)\n\n        # Validation phase\n        model.eval()\n        all_labels = []\n        all_preds = []\n        val_loss = 0.0\n        val_corrects = 0\n\n        with torch.no_grad():\n            for inputs, labels in tqdm(val_loader, desc=f'Validating'):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                outputs = model(inputs)\n                _, preds = torch.max(outputs, 1)\n                loss = criterion(outputs, labels)\n\n                val_loss += loss.item() * inputs.size(0)\n                val_corrects += torch.sum(preds == labels.data).item()\n\n                # Store predictions and labels for F1 calculation\n                all_labels.extend(labels.cpu().numpy())\n                all_preds.extend(preds.cpu().numpy())\n\n        epoch_loss = val_loss / len(val_loader.dataset)\n        epoch_acc = val_corrects / len(val_loader.dataset) * 100\n        epoch_f1 = f1_score(all_labels, all_preds, average='weighted')\n\n        logger.info(f'Validation Loss: {epoch_loss:.4f} Acc: {epoch_acc:.2f}% F1: {epoch_f1:.4f}')\n\n        # Log validation metrics to TensorBoard\n        writer.add_scalar('Loss/val', epoch_loss, epoch)\n        writer.add_scalar('Accuracy/val', epoch_acc, epoch)\n        writer.add_scalar('F1/val', epoch_f1, epoch)\n\n        # Add confusion matrix to TensorBoard\n        cm = confusion_matrix(all_labels, all_preds)\n        cm_figure = plt.figure(figsize=(10, 8))\n        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n        plt.title('Confusion matrix')\n        plt.colorbar()\n        tick_marks = np.arange(len(train_dataset.classes))\n        plt.xticks(tick_marks, train_dataset.classes, rotation=45)\n        plt.yticks(tick_marks, train_dataset.classes)\n        plt.tight_layout()\n        plt.ylabel('True label')\n        plt.xlabel('Predicted label')\n        writer.add_figure('Confusion Matrix/val', cm_figure, epoch)\n\n        # Log class report\n        class_report = classification_report(all_labels, all_preds, target_names=train_dataset.classes,\n                                             output_dict=True)\n        for class_name, metrics in class_report.items():\n            if class_name in train_dataset.classes:\n                writer.add_scalar(f'Class/{class_name}/precision', metrics['precision'], epoch)\n                writer.add_scalar(f'Class/{class_name}/recall', metrics['recall'], epoch)\n                writer.add_scalar(f'Class/{class_name}/f1-score', metrics['f1-score'], epoch)\n\n        # Update scheduler\n        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n            scheduler.step(epoch_f1)\n        else:\n            scheduler.step()\n\n        # Log learning rate\n        current_lr = optimizer.param_groups[0]['lr']\n        writer.add_scalar('LearningRate', current_lr, epoch)\n        logger.info(f'Current LR: {current_lr:.7f}')\n\n        # Check early stopping\n        early_stopping(epoch_f1, model)\n        if early_stopping.early_stop:\n            logger.info(\"Early stopping triggered!\")\n            break\n\n        # Save best model\n        if epoch_f1 > best_f1:\n            best_f1 = epoch_f1\n            best_model_wts = copy.deepcopy(model.state_dict())\n            # Save the model checkpoint\n            model_path = os.path.join(MODEL_DIR, f\"{model_name}_best.pth\")\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': best_model_wts,\n                'f1': best_f1,\n            }, model_path)\n            logger.info(f'New best model saved with F1: {best_f1:.4f}')\n\n        # Clear memory\n        del all_labels, all_preds\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n    # Close TensorBoard writer\n    writer.close()\n\n    # If early stopping was triggered, load the early stopping checkpoint\n    if early_stopping.early_stop:\n        model.load_state_dict(torch.load(early_stopping.path))\n        logger.info(f\"Loaded early stopping model with F1: {early_stopping.val_f1_max:.4f}\")\n        return model, early_stopping.val_f1_max\n\n    # Otherwise, load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, best_f1\n\n\n# Improved training function with holdout validation\ndef train_models_holdout(model_configs, val_split=0.2, num_epochs=10):\n    logger.info(f\"\\n{'=' * 50}\")\n    logger.info(f\"Training models with improved holdout validation\")\n    logger.info(f\"{'=' * 50}\")\n\n    # Create a validation split\n    train_size = int((1 - val_split) * len(train_dataset))\n    val_size = len(train_dataset) - train_size\n    temp_train_dataset, temp_val_dataset = random_split(\n        train_dataset, [train_size, val_size],\n        generator=torch.Generator().manual_seed(42)\n    )\n\n    logger.info(f\"Training set size: {train_size}, Validation set size: {val_size}\")\n\n    model_results = []\n\n    for model_name, lr, weight_decay, batch_size in model_configs:\n        logger.info(f\"\\n{'-' * 40}\")\n        logger.info(f\"Training {model_name}\")\n        logger.info(f\"{'-' * 40}\")\n\n        # Create data loaders\n        train_loader = DataLoader(\n            temp_train_dataset,\n            batch_size=batch_size,\n            shuffle=True,\n            num_workers=2,\n            pin_memory=True\n        )\n\n        val_loader = DataLoader(\n            temp_val_dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=2,\n            pin_memory=True\n        )\n\n        # Create model\n        model = create_model(model_name, num_classes=len(train_dataset.classes), pretrained=True)\n        model = model.to(device)\n\n        # Define loss function and optimizer\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n\n        # Define scheduler - CosineAnnealingWarmRestarts\n        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n            optimizer, T_0=5, T_mult=1, eta_min=lr / 10\n        )\n\n        # Train model with early stopping and gradient clipping\n        model, best_f1 = train_model(\n            model=model,\n            train_loader=train_loader,\n            val_loader=val_loader,\n            criterion=criterion,\n            optimizer=optimizer,\n            scheduler=scheduler,\n            model_name=model_name,\n            num_epochs=num_epochs,\n            mixup_alpha=0.2,\n            clip_grad_norm=1.0,  # Apply gradient clipping\n            early_stopping_patience=5  # Use early stopping\n        )\n\n        # Store model and F1 score\n        model_results.append((model_name, model, best_f1))\n\n        # Free up memory\n        del train_loader, val_loader, model, optimizer, scheduler\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n    return model_results\n\n\n# Test Time Augmentation function\ndef tta_predict(model, dataset, n_transforms=5):\n    \"\"\"\n    Perform Test Time Augmentation for more robust predictions\n    \"\"\"\n    model.eval()\n\n    # Create TTA transforms\n    tta_transforms = get_tta_transforms(n_transforms=n_transforms)\n\n    all_probs = []\n    all_ids = []\n\n    # First collect all image IDs\n    for _, image_id in dataset:\n        all_ids.append(image_id)\n\n    # For each TTA transform\n    for transform_idx, transform in enumerate(tta_transforms):\n        logger.info(f\"Applying TTA transform {transform_idx + 1}/{len(tta_transforms)}\")\n\n        # Create a new dataset with this transform\n        tta_dataset = AlbumentationsDataset(TEST_DIR, transform=transform, is_test=True)\n\n        # Create a loader\n        tta_loader = DataLoader(\n            tta_dataset,\n            batch_size=64,\n            shuffle=False,\n            num_workers=2,\n            pin_memory=True\n        )\n\n        # Collect predictions for this transform\n        transform_probs = []\n\n        with torch.cuda.amp.autocast(enabled=True):\n            with torch.no_grad():\n                for inputs, _ in tqdm(tta_loader, desc=f\"TTA {transform_idx + 1}\"):\n                    inputs = inputs.to(device)\n                    outputs = model(inputs)\n                    probs = torch.softmax(outputs, dim=1)\n                    transform_probs.append(probs.cpu().numpy())\n\n                    # Clear GPU cache\n                    if torch.cuda.is_available():\n                        torch.cuda.empty_cache()\n\n        # Stack all batches\n        transform_probs = np.vstack(transform_probs)\n        all_probs.append(transform_probs)\n\n        # Free memory\n        del tta_dataset, tta_loader, transform_probs\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n    # Average predictions across all TTA transforms\n    avg_probs = np.mean(all_probs, axis=0)\n\n    return all_ids, avg_probs\n\n\n# Improved ensemble prediction with confidence\ndef robust_ensemble_predict(model_results, test_dataset, use_tta=True):\n    \"\"\"\n    Make more robust ensemble predictions using model confidence and TTA\n    \"\"\"\n    # Initialize predictions\n    all_probs = None\n    total_weight = 0\n\n    # Loop through each model\n    for model_name, model, f1_score in model_results:\n        logger.info(f\"Getting predictions for {model_name} (F1: {f1_score:.4f})...\")\n\n        # Use weighted F1 score \n        weight = f1_score ** 3\n        total_weight += weight\n\n        # Get predictions with TTA if enabled\n        if use_tta:\n            ids, probs = tta_predict(model, test_dataset, n_transforms=3)\n        else:\n            # Create a regular test loader\n            test_loader = DataLoader(\n                test_dataset,\n                batch_size=64,\n                shuffle=False,\n                num_workers=2,\n                pin_memory=True\n            )\n            # Get predictions\n            ids, probs = predict_model(model, test_loader)\n\n            # Free memory\n            del test_loader\n            gc.collect()\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n\n        # Initialize all_probs\n        if all_probs is None:\n            all_ids = ids\n            all_probs = np.zeros_like(probs)\n\n        # Log model confidence metrics\n        confidence = np.max(probs, axis=1)\n        avg_confidence = np.mean(confidence)\n        logger.info(f\"Model {model_name} average confidence: {avg_confidence:.4f}\")\n\n        # Add weighted probabilities\n        all_probs += weight * probs\n\n        # Free memory\n        del probs\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n    # Normalize by total weight\n    all_probs /= total_weight\n\n    # Get the predicted class indices and confidence\n    predicted_classes = np.argmax(all_probs, axis=1)\n    confidence = np.max(all_probs, axis=1)\n\n    # Log ensemble confidence\n    avg_confidence = np.mean(confidence)\n    logger.info(f\"Ensemble average confidence: {avg_confidence:.4f}\")\n\n    # Create a dataframe with results\n    results = pd.DataFrame({\n        'image_id': all_ids,\n        'class_id': predicted_classes,\n        'confidence': confidence\n    })\n\n    # Convert numeric class_id to original class name\n    results['class_name'] = results['class_id'].apply(lambda x: train_dataset.idx_to_class[x])\n\n    # Sort by image_id to ensure consistent order\n    results = results.sort_values('image_id')\n\n    return results\n\n\n# Basic prediction function (for non-TTA)\ndef predict_model(model, test_loader):\n    \"\"\"Make predictions with a single model without TTA\"\"\"\n    model.eval()\n    all_probs = []\n    all_ids = []\n\n    with torch.cuda.amp.autocast(enabled=True):\n        with torch.no_grad():\n            for inputs, image_ids in tqdm(test_loader, desc=\"Predicting\"):\n                inputs = inputs.to(device)\n                outputs = model(inputs)\n                probs = torch.softmax(outputs, dim=1)\n\n                all_probs.append(probs.cpu().numpy())\n                all_ids.extend(image_ids)\n\n                # Clear GPU cache\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n\n    # Stack all batches\n    all_probs = np.vstack(all_probs)\n\n    return all_ids, all_probs\n\n\n# Main training pipeline\ndef main():\n    logger.info(f\"Starting the training pipeline at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n\n    # Train models with holdout validation\n    model_results = train_models_holdout(\n        model_configs=model_configs,\n        val_split=0.2,  # Simple holdout validation with 20% of data\n        num_epochs=15  # Limit epochs for faster training\n    )\n\n    # Find the best model based on validation F1 score\n    best_model_name, best_model, best_f1 = max(model_results, key=lambda x: x[2])\n    logger.info(f\"Best model: {best_model_name} with F1 score: {best_f1:.4f}\")\n\n    # Make predictions using ensemble\n    logger.info(\"Making ensemble predictions...\")\n    results = robust_ensemble_predict(\n        model_results=model_results,\n        test_dataset=test_dataset,\n        use_tta=True  # Use Test Time Augmentation for better results\n    )\n\n    # Generate submission file\n    logger.info(f\"Generating submission file at {OUTPUT_FILE}\")\n    submission = results[['image_id', 'class_name']]\n    submission.columns = ['image_id', 'label']\n    submission.to_csv(OUTPUT_FILE, index=False)\n\n    logger.info(f\"Pipeline completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n\n    # Optionally, plot distribution of predicted classes\n    fig, ax = plt.subplots(figsize=(10, 6))\n    results['class_name'].value_counts().plot(kind='bar', ax=ax)\n    plt.title('Distribution of Predicted Classes')\n    plt.ylabel('Count')\n    plt.tight_layout()\n    plt.savefig(os.path.join(OUTPUT_DIR, 'predicted_class_distribution.png'))\n\n    # Plot confidence distribution\n    fig, ax = plt.subplots(figsize=(10, 6))\n    plt.hist(results['confidence'], bins=20)\n    plt.title('Distribution of Prediction Confidence')\n    plt.xlabel('Confidence')\n    plt.ylabel('Count')\n    plt.tight_layout()\n    plt.savefig(os.path.join(OUTPUT_DIR, 'confidence_distribution.png'))\n\n    return results\n\n\n# Execute main function if this script is run directly\nif __name__ == \"__main__\":\n    try:\n        results = main()\n        logger.info(\"Script completed successfully!\")\n    except Exception as e:\n        logger.exception(f\"An error occurred: {e}\")\n        raise\n\n\n# Additional utility: Load and evaluate a saved model\ndef load_and_evaluate_model(model_name, model_path, val_loader):\n    \"\"\"\n    Load a saved model checkpoint and evaluate it on validation data\n    \"\"\"\n    logger.info(f\"Loading model {model_name} from {model_path}\")\n\n    # Create model architecture\n    model = create_model(model_name, num_classes=len(train_dataset.classes), pretrained=False)\n\n    # Load weights\n    checkpoint = torch.load(model_path)\n    if 'model_state_dict' in checkpoint:\n        model.load_state_dict(checkpoint['model_state_dict'])\n    else:\n        model.load_state_dict(checkpoint)\n\n    model = model.to(device)\n    model.eval()\n\n    # Evaluate\n    criterion = nn.CrossEntropyLoss()\n    all_labels = []\n    all_preds = []\n    all_probs = []\n    val_loss = 0.0\n\n    with torch.no_grad():\n        for inputs, labels in tqdm(val_loader, desc=f'Evaluating {model_name}'):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            probs = torch.softmax(outputs, dim=1)\n            loss = criterion(outputs, labels)\n\n            val_loss += loss.item() * inputs.size(0)\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds.cpu().numpy())\n            all_probs.append(probs.cpu().numpy())\n\n    # Calculate metrics\n    all_probs = np.vstack(all_probs)\n    avg_loss = val_loss / len(val_loader.dataset)\n    avg_acc = accuracy_score(all_labels, all_preds) * 100\n    avg_f1 = f1_score(all_labels, all_preds, average='weighted')\n\n    # Calculate per-class metrics\n    class_report = classification_report(all_labels, all_preds,\n                                         target_names=train_dataset.classes,\n                                         output_dict=True)\n\n    # Log results\n    logger.info(f\"{model_name} Evaluation Results:\")\n    logger.info(f\"Loss: {avg_loss:.4f} | Accuracy: {avg_acc:.2f}% | F1: {avg_f1:.4f}\")\n\n    # Print confusion matrix\n    cm = confusion_matrix(all_labels, all_preds)\n    plt.figure(figsize=(12, 10))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title(f'{model_name} Confusion Matrix')\n    plt.colorbar()\n    tick_marks = np.arange(len(train_dataset.classes))\n    plt.xticks(tick_marks, train_dataset.classes, rotation=45)\n    plt.yticks(tick_marks, train_dataset.classes)\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.savefig(os.path.join(OUTPUT_DIR, f'{model_name}_confusion_matrix.png'))\n\n    return {\n        'model': model,\n        'loss': avg_loss,\n        'accuracy': avg_acc,\n        'f1': avg_f1,\n        'class_report': class_report,\n        'confusion_matrix': cm\n    }\n\n\n# Function to analyze model mistakes\ndef analyze_mistakes(model, val_loader, top_n=10):\n    \"\"\"\n    Analyze the most common mistakes made by a model\n    \"\"\"\n    model.eval()\n    mistakes = []\n\n    with torch.no_grad():\n        for inputs, labels in tqdm(val_loader, desc=\"Analyzing mistakes\"):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            probs = torch.softmax(outputs, dim=1)\n\n            # Find mistakes\n            for i, (pred, label) in enumerate(zip(preds, labels)):\n                if pred != label:\n                    confidence = probs[i, pred].item()\n                    true_class = train_dataset.idx_to_class[label.item()]\n                    pred_class = train_dataset.idx_to_class[pred.item()]\n\n                    mistakes.append({\n                        'true_class': true_class,\n                        'pred_class': pred_class,\n                        'confidence': confidence,\n                    })\n\n    # Analyze most common error patterns\n    error_pairs = Counter([(m['true_class'], m['pred_class']) for m in mistakes])\n    top_errors = error_pairs.most_common(top_n)\n\n    logger.info(\"Most common errors:\")\n    for (true_class, pred_class), count in top_errors:\n        logger.info(f\"True: {true_class}, Predicted: {pred_class}, Count: {count}\")\n\n    return mistakes, top_errors\n\n\n# Data augmentation experiment utility\ndef experiment_with_augmentation(base_model_name, train_dataset, val_dataset, augmentation_configs):\n    \"\"\"\n    Experiment with different augmentation strategies\n    \"\"\"\n    results = []\n\n    for aug_name, transform in augmentation_configs.items():\n        logger.info(f\"\\n{'-' * 40}\")\n        logger.info(f\"Testing augmentation: {aug_name}\")\n        logger.info(f\"{'-' * 40}\")\n\n        # Create dataset with this augmentation\n        train_dataset_aug = AlbumentationsDataset(TRAIN_DIR, transform=transform)\n\n        train_loader = DataLoader(\n            train_dataset_aug,\n            batch_size=64,\n            shuffle=True,  # Use shuffle=True instead of the sampler\n            num_workers=2,\n            pin_memory=True\n        )\n\n        val_loader = DataLoader(\n            val_dataset,\n            batch_size=64,\n            shuffle=False,\n            num_workers=2,\n            pin_memory=True\n        )\n\n        # Create model\n        model = create_model(base_model_name, num_classes=len(train_dataset.classes), pretrained=True)\n        model = model.to(device)\n\n        class_weights = torch.tensor([\n        1.1,  # Reptilia\n        1.0,  # Animalia\n        1.2,  # Arachnida\n        1.1,  # Amphibia\n        1.0,  # Aves\n        1.2,  # Mollusca\n        1.3,  # Fungi\n        1.1,  # Insecta\n        1.3,  # Plantae\n        1.0,  # Mammalia\n        ], device=device)\n\n        # Define loss function and optimizer\n        criterion = LabelSmoothingCrossEntropyLoss(classes=len(train_dataset.classes), \n                                              smoothing=0.15,\n                                              class_weights=class_weights)\n        optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n\n        # Train model for fewer epochs to quickly compare augmentations\n        model, best_f1 = train_model(\n            model=model,\n            train_loader=train_loader,\n            val_loader=val_loader,\n            criterion=criterion,\n            optimizer=optimizer,\n            scheduler=scheduler,\n            model_name=f\"{base_model_name}_{aug_name}\",\n            num_epochs=5,  # Fewer epochs for faster comparison\n            mixup_alpha=0.0,  # Disable mixup to isolate augmentation effects\n            early_stopping_patience=3\n        )\n\n        # Store result\n        results.append({\n            'augmentation': aug_name,\n            'f1_score': best_f1\n        })\n\n        # Clean up\n        del model, train_loader, val_loader, train_dataset_aug\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n    # Sort and display results\n    results = sorted(results, key=lambda x: x['f1_score'], reverse=True)\n    logger.info(\"\\nAugmentation Results:\")\n    for result in results:\n        logger.info(f\"Augmentation: {result['augmentation']}, F1: {result['f1_score']:.4f}\")\n\n    return results","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}