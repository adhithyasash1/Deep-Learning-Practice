{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f345bc20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T18:44:02.955088Z",
     "iopub.status.busy": "2025-03-23T18:44:02.954831Z",
     "iopub.status.idle": "2025-03-23T18:44:08.593738Z",
     "shell.execute_reply": "2025-03-23T18:44:08.592672Z"
    },
    "papermill": {
     "duration": 5.643734,
     "end_time": "2025-03-23T18:44:08.595488",
     "exception": false,
     "start_time": "2025-03-23T18:44:02.951754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ultralytics\r\n",
      "  Downloading ultralytics-8.3.94-py3-none-any.whl.metadata (35 kB)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\r\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.5)\r\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\r\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (6.0.2)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\r\n",
      "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\r\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\r\n",
      "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\r\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\r\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\r\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\r\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\r\n",
      "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.12.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\r\n",
      "Downloading ultralytics-8.3.94-py3-none-any.whl (949 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m949.8/949.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\r\n",
      "Installing collected packages: ultralytics-thop, ultralytics\r\n",
      "Successfully installed ultralytics-8.3.94 ultralytics-thop-2.0.14\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics numpy pandas matplotlib pillow opencv-python pyyaml tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "418bcee4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T18:44:08.603341Z",
     "iopub.status.busy": "2025-03-23T18:44:08.603065Z",
     "iopub.status.idle": "2025-03-23T18:44:59.245134Z",
     "shell.execute_reply": "2025-03-23T18:44:59.244077Z"
    },
    "papermill": {
     "duration": 50.648145,
     "end_time": "2025-03-23T18:44:59.246875",
     "exception": false,
     "start_time": "2025-03-23T18:44:08.598730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008c2ef38cda4113b55e436d5e66e9ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyzing dataset:   0%|          | 0/7500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2988c09102bc4acfa235f9d259b5e0f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing stratification:   0%|          | 0/7500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (0, 0)\n",
      "DataFrame columns: []\n",
      "Warning: 'primary_class' column missing or DataFrame is empty. Using basic fold splitting.\n",
      "Successfully set up weighted dataset for training\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8x.pt to 'yolov8x.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 131M/131M [00:01<00:00, 124MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.94 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8x.pt, data=yolo_data/fold_0/data.yaml, epochs=30, time=None, patience=100, batch=2, imgsz=1024, save=True, save_period=-1, cache=True, device=None, workers=8, project=outputs, name=yolov8x_fold0, exist_ok=False, pretrained=True, optimizer=AdamW, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=True, close_mosaic=5, resume=False, amp=True, fraction=1.0, profile=False, freeze=10, multi_scale=True, overlap_mask=True, mask_ratio=4, dropout=0.1, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.001, momentum=0.937, weight_decay=0.0005, warmup_epochs=5, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.8, hsv_v=0.5, degrees=15, translate=0.2, scale=0.6, shear=0.0, perspective=0.0, flipud=0.2, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.3, copy_paste=0.3, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=outputs/yolov8x_fold0\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 30.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=6\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      2320  ultralytics.nn.modules.conv.Conv             [3, 80, 3, 2]                 \n",
      "  1                  -1  1    115520  ultralytics.nn.modules.conv.Conv             [80, 160, 3, 2]               \n",
      "  2                  -1  3    436800  ultralytics.nn.modules.block.C2f             [160, 160, 3, True]           \n",
      "  3                  -1  1    461440  ultralytics.nn.modules.conv.Conv             [160, 320, 3, 2]              \n",
      "  4                  -1  6   3281920  ultralytics.nn.modules.block.C2f             [320, 320, 6, True]           \n",
      "  5                  -1  1   1844480  ultralytics.nn.modules.conv.Conv             [320, 640, 3, 2]              \n",
      "  6                  -1  6  13117440  ultralytics.nn.modules.block.C2f             [640, 640, 6, True]           \n",
      "  7                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n",
      "  8                  -1  3   6969600  ultralytics.nn.modules.block.C2f             [640, 640, 3, True]           \n",
      "  9                  -1  1   1025920  ultralytics.nn.modules.block.SPPF            [640, 640, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  3   1948800  ultralytics.nn.modules.block.C2f             [960, 320, 3]                 \n",
      " 16                  -1  1    922240  ultralytics.nn.modules.conv.Conv             [320, 320, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  3   7174400  ultralytics.nn.modules.block.C2f             [960, 640, 3]                 \n",
      " 19                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n",
      " 22        [15, 18, 21]  1   8723746  ultralytics.nn.modules.head.Detect           [6, [320, 640, 640]]          \n",
      "Model summary: 209 layers, 68,158,386 parameters, 68,158,370 gradients, 258.1 GFLOPs\n",
      "\n",
      "Transferred 589/595 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir outputs/yolov8x_fold0', view at http://localhost:6006/\n",
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.3.conv.weight'\n",
      "Freezing layer 'model.3.bn.weight'\n",
      "Freezing layer 'model.3.bn.bias'\n",
      "Freezing layer 'model.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.3.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.3.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.3.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.3.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.3.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.3.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.5.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.5.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.5.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.5.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.5.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.5.cv2.bn.bias'\n",
      "Freezing layer 'model.5.conv.weight'\n",
      "Freezing layer 'model.5.bn.weight'\n",
      "Freezing layer 'model.5.bn.bias'\n",
      "Freezing layer 'model.6.cv1.conv.weight'\n",
      "Freezing layer 'model.6.cv1.bn.weight'\n",
      "Freezing layer 'model.6.cv1.bn.bias'\n",
      "Freezing layer 'model.6.cv2.conv.weight'\n",
      "Freezing layer 'model.6.cv2.bn.weight'\n",
      "Freezing layer 'model.6.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.3.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.3.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.3.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.3.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.3.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.3.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.4.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.4.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.4.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.4.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.4.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.4.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.5.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.5.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.5.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.5.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.5.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.5.cv2.bn.bias'\n",
      "Freezing layer 'model.7.conv.weight'\n",
      "Freezing layer 'model.7.bn.weight'\n",
      "Freezing layer 'model.7.bn.bias'\n",
      "Freezing layer 'model.8.cv1.conv.weight'\n",
      "Freezing layer 'model.8.cv1.bn.weight'\n",
      "Freezing layer 'model.8.cv1.bn.bias'\n",
      "Freezing layer 'model.8.cv2.conv.weight'\n",
      "Freezing layer 'model.8.cv2.bn.weight'\n",
      "Freezing layer 'model.8.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.9.cv1.conv.weight'\n",
      "Freezing layer 'model.9.cv1.bn.weight'\n",
      "Freezing layer 'model.9.cv1.bn.bias'\n",
      "Freezing layer 'model.9.cv2.conv.weight'\n",
      "Freezing layer 'model.9.cv2.bn.weight'\n",
      "Freezing layer 'model.9.cv2.bn.bias'\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.35M/5.35M [00:00<00:00, 95.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "Training error: \u001b[34m\u001b[1mtrain: \u001b[0mError loading data from /kaggle/working/yolo_data/fold_0/images/train\n",
      "See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n",
      "Ultralytics 8.3.94 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8x.pt, data=yolo_data/fold_1/data.yaml, epochs=30, time=None, patience=100, batch=2, imgsz=1024, save=True, save_period=-1, cache=True, device=None, workers=8, project=outputs, name=yolov8x_fold1, exist_ok=False, pretrained=True, optimizer=AdamW, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=True, close_mosaic=5, resume=False, amp=True, fraction=1.0, profile=False, freeze=10, multi_scale=True, overlap_mask=True, mask_ratio=4, dropout=0.1, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.001, momentum=0.937, weight_decay=0.0005, warmup_epochs=5, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.8, hsv_v=0.5, degrees=15, translate=0.2, scale=0.6, shear=0.0, perspective=0.0, flipud=0.2, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.3, copy_paste=0.3, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=outputs/yolov8x_fold1\n",
      "Overriding model.yaml nc=80 with nc=6\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      2320  ultralytics.nn.modules.conv.Conv             [3, 80, 3, 2]                 \n",
      "  1                  -1  1    115520  ultralytics.nn.modules.conv.Conv             [80, 160, 3, 2]               \n",
      "  2                  -1  3    436800  ultralytics.nn.modules.block.C2f             [160, 160, 3, True]           \n",
      "  3                  -1  1    461440  ultralytics.nn.modules.conv.Conv             [160, 320, 3, 2]              \n",
      "  4                  -1  6   3281920  ultralytics.nn.modules.block.C2f             [320, 320, 6, True]           \n",
      "  5                  -1  1   1844480  ultralytics.nn.modules.conv.Conv             [320, 640, 3, 2]              \n",
      "  6                  -1  6  13117440  ultralytics.nn.modules.block.C2f             [640, 640, 6, True]           \n",
      "  7                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n",
      "  8                  -1  3   6969600  ultralytics.nn.modules.block.C2f             [640, 640, 3, True]           \n",
      "  9                  -1  1   1025920  ultralytics.nn.modules.block.SPPF            [640, 640, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  3   1948800  ultralytics.nn.modules.block.C2f             [960, 320, 3]                 \n",
      " 16                  -1  1    922240  ultralytics.nn.modules.conv.Conv             [320, 320, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  3   7174400  ultralytics.nn.modules.block.C2f             [960, 640, 3]                 \n",
      " 19                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n",
      " 22        [15, 18, 21]  1   8723746  ultralytics.nn.modules.head.Detect           [6, [320, 640, 640]]          \n",
      "Model summary: 209 layers, 68,158,386 parameters, 68,158,370 gradients, 258.1 GFLOPs\n",
      "\n",
      "Transferred 589/595 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir outputs/yolov8x_fold1', view at http://localhost:6006/\n",
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.3.conv.weight'\n",
      "Freezing layer 'model.3.bn.weight'\n",
      "Freezing layer 'model.3.bn.bias'\n",
      "Freezing layer 'model.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.3.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.3.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.3.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.3.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.3.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.3.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.5.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.5.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.5.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.5.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.5.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.5.cv2.bn.bias'\n",
      "Freezing layer 'model.5.conv.weight'\n",
      "Freezing layer 'model.5.bn.weight'\n",
      "Freezing layer 'model.5.bn.bias'\n",
      "Freezing layer 'model.6.cv1.conv.weight'\n",
      "Freezing layer 'model.6.cv1.bn.weight'\n",
      "Freezing layer 'model.6.cv1.bn.bias'\n",
      "Freezing layer 'model.6.cv2.conv.weight'\n",
      "Freezing layer 'model.6.cv2.bn.weight'\n",
      "Freezing layer 'model.6.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.3.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.3.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.3.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.3.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.3.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.3.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.4.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.4.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.4.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.4.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.4.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.4.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.5.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.5.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.5.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.5.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.5.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.5.cv2.bn.bias'\n",
      "Freezing layer 'model.7.conv.weight'\n",
      "Freezing layer 'model.7.bn.weight'\n",
      "Freezing layer 'model.7.bn.bias'\n",
      "Freezing layer 'model.8.cv1.conv.weight'\n",
      "Freezing layer 'model.8.cv1.bn.weight'\n",
      "Freezing layer 'model.8.cv1.bn.bias'\n",
      "Freezing layer 'model.8.cv2.conv.weight'\n",
      "Freezing layer 'model.8.cv2.bn.weight'\n",
      "Freezing layer 'model.8.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.9.cv1.conv.weight'\n",
      "Freezing layer 'model.9.cv1.bn.weight'\n",
      "Freezing layer 'model.9.cv1.bn.bias'\n",
      "Freezing layer 'model.9.cv2.conv.weight'\n",
      "Freezing layer 'model.9.cv2.bn.weight'\n",
      "Freezing layer 'model.9.cv2.bn.bias'\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "Training error: \u001b[34m\u001b[1mtrain: \u001b[0mError loading data from /kaggle/working/yolo_data/fold_1/images/train\n",
      "See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n",
      "Ultralytics 8.3.94 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8x.pt, data=yolo_data/fold_2/data.yaml, epochs=30, time=None, patience=100, batch=2, imgsz=1024, save=True, save_period=-1, cache=True, device=None, workers=8, project=outputs, name=yolov8x_fold2, exist_ok=False, pretrained=True, optimizer=AdamW, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=True, close_mosaic=5, resume=False, amp=True, fraction=1.0, profile=False, freeze=10, multi_scale=True, overlap_mask=True, mask_ratio=4, dropout=0.1, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.001, momentum=0.937, weight_decay=0.0005, warmup_epochs=5, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.8, hsv_v=0.5, degrees=15, translate=0.2, scale=0.6, shear=0.0, perspective=0.0, flipud=0.2, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.3, copy_paste=0.3, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=outputs/yolov8x_fold2\n",
      "Overriding model.yaml nc=80 with nc=6\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      2320  ultralytics.nn.modules.conv.Conv             [3, 80, 3, 2]                 \n",
      "  1                  -1  1    115520  ultralytics.nn.modules.conv.Conv             [80, 160, 3, 2]               \n",
      "  2                  -1  3    436800  ultralytics.nn.modules.block.C2f             [160, 160, 3, True]           \n",
      "  3                  -1  1    461440  ultralytics.nn.modules.conv.Conv             [160, 320, 3, 2]              \n",
      "  4                  -1  6   3281920  ultralytics.nn.modules.block.C2f             [320, 320, 6, True]           \n",
      "  5                  -1  1   1844480  ultralytics.nn.modules.conv.Conv             [320, 640, 3, 2]              \n",
      "  6                  -1  6  13117440  ultralytics.nn.modules.block.C2f             [640, 640, 6, True]           \n",
      "  7                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n",
      "  8                  -1  3   6969600  ultralytics.nn.modules.block.C2f             [640, 640, 3, True]           \n",
      "  9                  -1  1   1025920  ultralytics.nn.modules.block.SPPF            [640, 640, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  3   1948800  ultralytics.nn.modules.block.C2f             [960, 320, 3]                 \n",
      " 16                  -1  1    922240  ultralytics.nn.modules.conv.Conv             [320, 320, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  3   7174400  ultralytics.nn.modules.block.C2f             [960, 640, 3]                 \n",
      " 19                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n",
      " 22        [15, 18, 21]  1   8723746  ultralytics.nn.modules.head.Detect           [6, [320, 640, 640]]          \n",
      "Model summary: 209 layers, 68,158,386 parameters, 68,158,370 gradients, 258.1 GFLOPs\n",
      "\n",
      "Transferred 589/595 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir outputs/yolov8x_fold2', view at http://localhost:6006/\n",
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.3.conv.weight'\n",
      "Freezing layer 'model.3.bn.weight'\n",
      "Freezing layer 'model.3.bn.bias'\n",
      "Freezing layer 'model.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.3.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.3.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.3.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.3.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.3.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.3.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.5.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.5.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.5.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.5.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.5.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.5.cv2.bn.bias'\n",
      "Freezing layer 'model.5.conv.weight'\n",
      "Freezing layer 'model.5.bn.weight'\n",
      "Freezing layer 'model.5.bn.bias'\n",
      "Freezing layer 'model.6.cv1.conv.weight'\n",
      "Freezing layer 'model.6.cv1.bn.weight'\n",
      "Freezing layer 'model.6.cv1.bn.bias'\n",
      "Freezing layer 'model.6.cv2.conv.weight'\n",
      "Freezing layer 'model.6.cv2.bn.weight'\n",
      "Freezing layer 'model.6.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.3.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.3.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.3.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.3.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.3.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.3.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.4.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.4.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.4.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.4.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.4.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.4.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.5.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.5.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.5.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.5.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.5.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.5.cv2.bn.bias'\n",
      "Freezing layer 'model.7.conv.weight'\n",
      "Freezing layer 'model.7.bn.weight'\n",
      "Freezing layer 'model.7.bn.bias'\n",
      "Freezing layer 'model.8.cv1.conv.weight'\n",
      "Freezing layer 'model.8.cv1.bn.weight'\n",
      "Freezing layer 'model.8.cv1.bn.bias'\n",
      "Freezing layer 'model.8.cv2.conv.weight'\n",
      "Freezing layer 'model.8.cv2.bn.weight'\n",
      "Freezing layer 'model.8.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.9.cv1.conv.weight'\n",
      "Freezing layer 'model.9.cv1.bn.weight'\n",
      "Freezing layer 'model.9.cv1.bn.bias'\n",
      "Freezing layer 'model.9.cv2.conv.weight'\n",
      "Freezing layer 'model.9.cv2.bn.weight'\n",
      "Freezing layer 'model.9.cv2.bn.bias'\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "Training error: \u001b[34m\u001b[1mtrain: \u001b[0mError loading data from /kaggle/working/yolo_data/fold_2/images/train\n",
      "See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8l.pt to 'yolov8l.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 83.7M/83.7M [00:00<00:00, 115MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.94 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8l.pt, data=yolo_data/fold_0/data.yaml, epochs=30, time=None, patience=100, batch=2, imgsz=1024, save=True, save_period=-1, cache=True, device=None, workers=8, project=outputs, name=yolov8l_fold0, exist_ok=False, pretrained=True, optimizer=AdamW, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=True, close_mosaic=5, resume=False, amp=True, fraction=1.0, profile=False, freeze=10, multi_scale=True, overlap_mask=True, mask_ratio=4, dropout=0.1, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.001, momentum=0.937, weight_decay=0.0005, warmup_epochs=5, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.8, hsv_v=0.5, degrees=15, translate=0.2, scale=0.6, shear=0.0, perspective=0.0, flipud=0.2, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.3, copy_paste=0.3, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=outputs/yolov8l_fold0\n",
      "Overriding model.yaml nc=80 with nc=6\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  3    279808  ultralytics.nn.modules.block.C2f             [128, 128, 3, True]           \n",
      "  3                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  4                  -1  6   2101248  ultralytics.nn.modules.block.C2f             [256, 256, 6, True]           \n",
      "  5                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  6                  -1  6   8396800  ultralytics.nn.modules.block.C2f             [512, 512, 6, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  3   4461568  ultralytics.nn.modules.block.C2f             [512, 512, 3, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  3   1247744  ultralytics.nn.modules.block.C2f             [768, 256, 3]                 \n",
      " 16                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  3   4592640  ultralytics.nn.modules.block.C2f             [768, 512, 3]                 \n",
      " 19                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
      " 22        [15, 18, 21]  1   5587426  ultralytics.nn.modules.head.Detect           [6, [256, 512, 512]]          \n",
      "Model summary: 209 layers, 43,634,466 parameters, 43,634,450 gradients, 165.4 GFLOPs\n",
      "\n",
      "Transferred 589/595 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir outputs/yolov8l_fold0', view at http://localhost:6006/\n",
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.3.conv.weight'\n",
      "Freezing layer 'model.3.bn.weight'\n",
      "Freezing layer 'model.3.bn.bias'\n",
      "Freezing layer 'model.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.3.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.3.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.3.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.3.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.3.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.3.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.5.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.5.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.5.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.5.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.5.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.5.cv2.bn.bias'\n",
      "Freezing layer 'model.5.conv.weight'\n",
      "Freezing layer 'model.5.bn.weight'\n",
      "Freezing layer 'model.5.bn.bias'\n",
      "Freezing layer 'model.6.cv1.conv.weight'\n",
      "Freezing layer 'model.6.cv1.bn.weight'\n",
      "Freezing layer 'model.6.cv1.bn.bias'\n",
      "Freezing layer 'model.6.cv2.conv.weight'\n",
      "Freezing layer 'model.6.cv2.bn.weight'\n",
      "Freezing layer 'model.6.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.3.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.3.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.3.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.3.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.3.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.3.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.4.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.4.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.4.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.4.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.4.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.4.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.5.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.5.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.5.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.5.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.5.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.5.cv2.bn.bias'\n",
      "Freezing layer 'model.7.conv.weight'\n",
      "Freezing layer 'model.7.bn.weight'\n",
      "Freezing layer 'model.7.bn.bias'\n",
      "Freezing layer 'model.8.cv1.conv.weight'\n",
      "Freezing layer 'model.8.cv1.bn.weight'\n",
      "Freezing layer 'model.8.cv1.bn.bias'\n",
      "Freezing layer 'model.8.cv2.conv.weight'\n",
      "Freezing layer 'model.8.cv2.bn.weight'\n",
      "Freezing layer 'model.8.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.9.cv1.conv.weight'\n",
      "Freezing layer 'model.9.cv1.bn.weight'\n",
      "Freezing layer 'model.9.cv1.bn.bias'\n",
      "Freezing layer 'model.9.cv2.conv.weight'\n",
      "Freezing layer 'model.9.cv2.bn.weight'\n",
      "Freezing layer 'model.9.cv2.bn.bias'\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "Training error: \u001b[34m\u001b[1mtrain: \u001b[0mError loading data from /kaggle/working/yolo_data/fold_0/images/train\n",
      "See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n",
      "Ultralytics 8.3.94 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8l.pt, data=yolo_data/fold_1/data.yaml, epochs=30, time=None, patience=100, batch=2, imgsz=1024, save=True, save_period=-1, cache=True, device=None, workers=8, project=outputs, name=yolov8l_fold1, exist_ok=False, pretrained=True, optimizer=AdamW, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=True, close_mosaic=5, resume=False, amp=True, fraction=1.0, profile=False, freeze=10, multi_scale=True, overlap_mask=True, mask_ratio=4, dropout=0.1, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.001, momentum=0.937, weight_decay=0.0005, warmup_epochs=5, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.8, hsv_v=0.5, degrees=15, translate=0.2, scale=0.6, shear=0.0, perspective=0.0, flipud=0.2, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.3, copy_paste=0.3, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=outputs/yolov8l_fold1\n",
      "Overriding model.yaml nc=80 with nc=6\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  3    279808  ultralytics.nn.modules.block.C2f             [128, 128, 3, True]           \n",
      "  3                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  4                  -1  6   2101248  ultralytics.nn.modules.block.C2f             [256, 256, 6, True]           \n",
      "  5                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  6                  -1  6   8396800  ultralytics.nn.modules.block.C2f             [512, 512, 6, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  3   4461568  ultralytics.nn.modules.block.C2f             [512, 512, 3, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  3   1247744  ultralytics.nn.modules.block.C2f             [768, 256, 3]                 \n",
      " 16                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  3   4592640  ultralytics.nn.modules.block.C2f             [768, 512, 3]                 \n",
      " 19                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
      " 22        [15, 18, 21]  1   5587426  ultralytics.nn.modules.head.Detect           [6, [256, 512, 512]]          \n",
      "Model summary: 209 layers, 43,634,466 parameters, 43,634,450 gradients, 165.4 GFLOPs\n",
      "\n",
      "Transferred 589/595 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir outputs/yolov8l_fold1', view at http://localhost:6006/\n",
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.3.conv.weight'\n",
      "Freezing layer 'model.3.bn.weight'\n",
      "Freezing layer 'model.3.bn.bias'\n",
      "Freezing layer 'model.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.3.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.3.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.3.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.3.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.3.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.3.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.5.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.5.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.5.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.5.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.5.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.5.cv2.bn.bias'\n",
      "Freezing layer 'model.5.conv.weight'\n",
      "Freezing layer 'model.5.bn.weight'\n",
      "Freezing layer 'model.5.bn.bias'\n",
      "Freezing layer 'model.6.cv1.conv.weight'\n",
      "Freezing layer 'model.6.cv1.bn.weight'\n",
      "Freezing layer 'model.6.cv1.bn.bias'\n",
      "Freezing layer 'model.6.cv2.conv.weight'\n",
      "Freezing layer 'model.6.cv2.bn.weight'\n",
      "Freezing layer 'model.6.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.3.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.3.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.3.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.3.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.3.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.3.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.4.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.4.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.4.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.4.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.4.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.4.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.5.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.5.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.5.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.5.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.5.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.5.cv2.bn.bias'\n",
      "Freezing layer 'model.7.conv.weight'\n",
      "Freezing layer 'model.7.bn.weight'\n",
      "Freezing layer 'model.7.bn.bias'\n",
      "Freezing layer 'model.8.cv1.conv.weight'\n",
      "Freezing layer 'model.8.cv1.bn.weight'\n",
      "Freezing layer 'model.8.cv1.bn.bias'\n",
      "Freezing layer 'model.8.cv2.conv.weight'\n",
      "Freezing layer 'model.8.cv2.bn.weight'\n",
      "Freezing layer 'model.8.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.9.cv1.conv.weight'\n",
      "Freezing layer 'model.9.cv1.bn.weight'\n",
      "Freezing layer 'model.9.cv1.bn.bias'\n",
      "Freezing layer 'model.9.cv2.conv.weight'\n",
      "Freezing layer 'model.9.cv2.bn.weight'\n",
      "Freezing layer 'model.9.cv2.bn.bias'\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "Training error: \u001b[34m\u001b[1mtrain: \u001b[0mError loading data from /kaggle/working/yolo_data/fold_1/images/train\n",
      "See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n",
      "Ultralytics 8.3.94 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8l.pt, data=yolo_data/fold_2/data.yaml, epochs=30, time=None, patience=100, batch=2, imgsz=1024, save=True, save_period=-1, cache=True, device=None, workers=8, project=outputs, name=yolov8l_fold2, exist_ok=False, pretrained=True, optimizer=AdamW, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=True, close_mosaic=5, resume=False, amp=True, fraction=1.0, profile=False, freeze=10, multi_scale=True, overlap_mask=True, mask_ratio=4, dropout=0.1, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.001, momentum=0.937, weight_decay=0.0005, warmup_epochs=5, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.8, hsv_v=0.5, degrees=15, translate=0.2, scale=0.6, shear=0.0, perspective=0.0, flipud=0.2, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.3, copy_paste=0.3, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=outputs/yolov8l_fold2\n",
      "Overriding model.yaml nc=80 with nc=6\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  3    279808  ultralytics.nn.modules.block.C2f             [128, 128, 3, True]           \n",
      "  3                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  4                  -1  6   2101248  ultralytics.nn.modules.block.C2f             [256, 256, 6, True]           \n",
      "  5                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  6                  -1  6   8396800  ultralytics.nn.modules.block.C2f             [512, 512, 6, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  3   4461568  ultralytics.nn.modules.block.C2f             [512, 512, 3, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  3   1247744  ultralytics.nn.modules.block.C2f             [768, 256, 3]                 \n",
      " 16                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  3   4592640  ultralytics.nn.modules.block.C2f             [768, 512, 3]                 \n",
      " 19                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
      " 22        [15, 18, 21]  1   5587426  ultralytics.nn.modules.head.Detect           [6, [256, 512, 512]]          \n",
      "Model summary: 209 layers, 43,634,466 parameters, 43,634,450 gradients, 165.4 GFLOPs\n",
      "\n",
      "Transferred 589/595 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir outputs/yolov8l_fold2', view at http://localhost:6006/\n",
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.3.conv.weight'\n",
      "Freezing layer 'model.3.bn.weight'\n",
      "Freezing layer 'model.3.bn.bias'\n",
      "Freezing layer 'model.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.3.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.3.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.3.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.3.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.3.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.3.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.5.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.5.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.5.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.5.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.5.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.5.cv2.bn.bias'\n",
      "Freezing layer 'model.5.conv.weight'\n",
      "Freezing layer 'model.5.bn.weight'\n",
      "Freezing layer 'model.5.bn.bias'\n",
      "Freezing layer 'model.6.cv1.conv.weight'\n",
      "Freezing layer 'model.6.cv1.bn.weight'\n",
      "Freezing layer 'model.6.cv1.bn.bias'\n",
      "Freezing layer 'model.6.cv2.conv.weight'\n",
      "Freezing layer 'model.6.cv2.bn.weight'\n",
      "Freezing layer 'model.6.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.3.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.3.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.3.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.3.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.3.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.3.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.4.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.4.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.4.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.4.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.4.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.4.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.5.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.5.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.5.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.5.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.5.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.5.cv2.bn.bias'\n",
      "Freezing layer 'model.7.conv.weight'\n",
      "Freezing layer 'model.7.bn.weight'\n",
      "Freezing layer 'model.7.bn.bias'\n",
      "Freezing layer 'model.8.cv1.conv.weight'\n",
      "Freezing layer 'model.8.cv1.bn.weight'\n",
      "Freezing layer 'model.8.cv1.bn.bias'\n",
      "Freezing layer 'model.8.cv2.conv.weight'\n",
      "Freezing layer 'model.8.cv2.bn.weight'\n",
      "Freezing layer 'model.8.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.9.cv1.conv.weight'\n",
      "Freezing layer 'model.9.cv1.bn.weight'\n",
      "Freezing layer 'model.9.cv1.bn.bias'\n",
      "Freezing layer 'model.9.cv2.conv.weight'\n",
      "Freezing layer 'model.9.cv2.bn.weight'\n",
      "Freezing layer 'model.9.cv2.bn.bias'\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "Training error: \u001b[34m\u001b[1mtrain: \u001b[0mError loading data from /kaggle/working/yolo_data/fold_2/images/train\n",
      "See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n",
      "No models loaded. Cannot generate predictions.\n",
      "Processing completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import yaml\n",
    "from ultralytics import YOLO\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "import gc\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from ultralytics.data.dataset import YOLODataset\n",
    "import torch\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "class YOLOWeightedDataset(YOLODataset):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the WeightedDataset with class balancing capabilities.\n",
    "        Additional parameters:\n",
    "            class_weights (dict, optional): Dictionary mapping class IDs to weights.\n",
    "                                           If not provided, will be calculated automatically.\n",
    "            weight_method (str): Method to use for weighting ('inverse', 'effective', 'manual').\n",
    "            effective_beta (float): Beta parameter for effective number of samples weighting.\n",
    "            aggregation (str): How to aggregate weights for multi-class images ('mean', 'max', 'sum').\n",
    "        \"\"\"\n",
    "        # Extract custom parameters before passing to parent\n",
    "        self.custom_class_weights = kwargs.pop('class_weights', None)\n",
    "        self.weight_method = kwargs.pop('weight_method', 'inverse')\n",
    "        self.effective_beta = kwargs.pop('effective_beta', 0.9)\n",
    "        self.aggregation_method = kwargs.pop('aggregation', 'mean')\n",
    "        \n",
    "        # Initialize parent class\n",
    "        super(YOLOWeightedDataset, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        # Set up aggregation function\n",
    "        if self.aggregation_method == 'mean':\n",
    "            self.agg_func = np.mean\n",
    "        elif self.aggregation_method == 'max':\n",
    "            self.agg_func = np.max\n",
    "        elif self.aggregation_method == 'sum':\n",
    "            self.agg_func = np.sum\n",
    "        else:\n",
    "            self.agg_func = np.mean\n",
    "            \n",
    "        # Count instances and calculate weights\n",
    "        self.train_mode = \"train\" in self.prefix\n",
    "        self.count_instances()\n",
    "        self.class_weights = self.calculate_class_weights()\n",
    "        self.weights = self.calculate_sample_weights()\n",
    "        self.probabilities = self.normalize_weights(self.weights)\n",
    "        \n",
    "    def count_instances(self):\n",
    "        \"\"\"Count instances of each class in the dataset\"\"\"\n",
    "        self.counts = np.zeros(len(self.data[\"names\"]))\n",
    "        self.img_with_class = [set() for _ in range(len(self.data[\"names\"]))]\n",
    "        \n",
    "        for i, label in enumerate(self.labels):\n",
    "            if label is None:\n",
    "                continue\n",
    "                \n",
    "            cls = label['cls'].reshape(-1).astype(int)\n",
    "            for class_id in cls:\n",
    "                if 0 <= class_id < len(self.counts):  # Ensure valid class id\n",
    "                    self.counts[class_id] += 1\n",
    "                    self.img_with_class[class_id].add(i)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        self.counts = np.where(self.counts == 0, 1, self.counts)\n",
    "        \n",
    "        # Calculate class distribution\n",
    "        self.class_dist = self.counts / np.sum(self.counts)\n",
    "        print(f\"Class counts: {self.counts}\")\n",
    "        print(f\"Class distribution: {self.class_dist}\")\n",
    "        \n",
    "    def calculate_class_weights(self):\n",
    "        \"\"\"Calculate weights for each class based on frequency\"\"\"\n",
    "        if self.custom_class_weights is not None:\n",
    "            # Use manually provided weights\n",
    "            weights = np.ones(len(self.data[\"names\"]))\n",
    "            for class_id, weight in self.custom_class_weights.items():\n",
    "                weights[class_id] = weight\n",
    "            return weights\n",
    "            \n",
    "        if self.weight_method == 'inverse':\n",
    "            # Inverse frequency weighting\n",
    "            weights = np.sum(self.counts) / (len(self.data[\"names\"]) * self.counts)\n",
    "        elif self.weight_method == 'effective':\n",
    "            # Effective number of samples weighting (Lin et al.)\n",
    "            # Formula: (1 - beta) / (1 - beta^n)\n",
    "            weights = (1 - self.effective_beta) / (1 - np.power(self.effective_beta, self.counts))\n",
    "        else:\n",
    "            # Balanced weighting (inverse of frequency)\n",
    "            weights = 1.0 / self.class_dist\n",
    "            \n",
    "        # Normalize weights to have mean of 1\n",
    "        weights = weights / np.mean(weights)\n",
    "        print(f\"Class weights: {weights}\")\n",
    "        return weights\n",
    "        \n",
    "    def calculate_sample_weights(self):\n",
    "        \"\"\"Calculate weight for each sample based on its class composition\"\"\"\n",
    "        weights = []\n",
    "        for i, label in enumerate(self.labels):\n",
    "            if label is None or 'cls' not in label:\n",
    "                # Give default weight to images without labels\n",
    "                weights.append(1.0)\n",
    "                continue\n",
    "                \n",
    "            cls = label['cls'].reshape(-1).astype(int)\n",
    "            if cls.size == 0:\n",
    "                weights.append(1.0)\n",
    "                continue\n",
    "                \n",
    "            # Filter out invalid class ids\n",
    "            valid_cls = cls[(cls >= 0) & (cls < len(self.class_weights))]\n",
    "            if len(valid_cls) == 0:\n",
    "                weights.append(1.0)\n",
    "                continue\n",
    "                \n",
    "            # Aggregate class weights according to selected method\n",
    "            class_weights_for_sample = self.class_weights[valid_cls]\n",
    "            weight = self.agg_func(class_weights_for_sample)\n",
    "            weights.append(weight)\n",
    "            \n",
    "        return weights\n",
    "        \n",
    "    def normalize_weights(self, weights):\n",
    "        \"\"\"Normalize weights to form a probability distribution\"\"\"\n",
    "        weights = np.array(weights)\n",
    "        if np.sum(weights) == 0:\n",
    "            return np.ones(len(weights)) / len(weights)\n",
    "        return weights / np.sum(weights)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return transformed label information based on the sampled index\"\"\"\n",
    "        if not self.train_mode:\n",
    "            # Use standard indexing for validation\n",
    "            return self.transforms(self.get_image_and_label(index))\n",
    "        else:\n",
    "            # Use weighted sampling for training\n",
    "            sampled_index = np.random.choice(len(self.labels), p=self.probabilities)\n",
    "            return self.transforms(self.get_image_and_label(sampled_index))\n",
    "\n",
    "\n",
    "def create_weighted_sampler(dataset):\n",
    "    \"\"\"Create a WeightedRandomSampler for use with DataLoader\"\"\"\n",
    "    return WeightedRandomSampler(\n",
    "        weights=dataset.weights,\n",
    "        num_samples=len(dataset),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "\n",
    "# Helper function to patch YOLOv8's build module\n",
    "def patch_yolo_dataloader():\n",
    "    import ultralytics.data.build as build\n",
    "    \n",
    "    # Store original build_yolo_dataset function\n",
    "    original_build_dataset = build.build_yolo_dataset\n",
    "    \n",
    "    # Define patched function\n",
    "    def patched_build_dataset(*args, **kwargs):\n",
    "        # Add weighted dataset parameter\n",
    "        use_weighted = kwargs.pop('weighted', False)\n",
    "        \n",
    "        # Call original function\n",
    "        dataset = original_build_dataset(*args, **kwargs)\n",
    "        \n",
    "        # Replace with weighted version if requested\n",
    "        if use_weighted and \"train\" in args[1]:\n",
    "            dataset = YOLOWeightedDataset(*args, **kwargs)\n",
    "            \n",
    "        return dataset\n",
    "    \n",
    "    # Replace original function\n",
    "    build.build_yolo_dataset = patched_build_dataset\n",
    "    \n",
    "    # Replace YOLODataset class\n",
    "    build.YOLODataset = YOLOWeightedDataset\n",
    "    \n",
    "    return \"YOLO dataloader patched with weighted sampling\"\n",
    "\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    # Data paths\n",
    "    data_dir = \"/kaggle/input/dlp-object-detection-week-10/final_dlp_data/final_dlp_data\"\n",
    "    train_images_dir = os.path.join(data_dir, \"train\", \"images\")\n",
    "    train_labels_dir = os.path.join(data_dir, \"train\", \"labels\")\n",
    "    test_images_dir = os.path.join(data_dir, \"test\", \"images\")\n",
    "    output_dir = \"outputs\"\n",
    "    yolo_data_dir = \"yolo_data\"\n",
    "    \n",
    "    # YOLO model configuration\n",
    "    yolo_models = [\"yolov8x.pt\", \"yolov8l.pt\"]\n",
    "    yolo_img_size = 1024\n",
    "    \n",
    "    # Training parameters\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    batch_size = 2\n",
    "    epochs = 30\n",
    "    folds = 3\n",
    "    \n",
    "    # Advanced training options\n",
    "    dropout = 0.1\n",
    "    freeze = 10\n",
    "    \n",
    "    # Learning rate configuration\n",
    "    lr0 = 0.01\n",
    "    lrf = 0.001\n",
    "    weight_decay = 0.0005\n",
    "    \n",
    "    # Inference parameters\n",
    "    confidence_threshold = 0.05\n",
    "    iou_threshold = 0.45\n",
    "    ensemble_conf_threshold = 0.25\n",
    "    \n",
    "    # Test-time augmentation\n",
    "    tta = True\n",
    "    tta_scales = [0.8, 1.0, 1.2]\n",
    "    tta_flips = [None, 'horizontal']\n",
    "    \n",
    "    # Class mapping - based on dataset analysis\n",
    "    class_names = {\n",
    "        0: \"aegypti\", 1: \"albopictus\", 2: \"anopheles\", \n",
    "        3: \"culex\", 4: \"culiseta\", 5: \"japonicus/koreicus\"\n",
    "    }\n",
    "\n",
    "    class_weights = {\n",
    "        0: 10.0,  # aegypti (very rare)\n",
    "        1: 1.0,  # albopictus (common)\n",
    "        2: 8.0,  # anopheles (very rare)\n",
    "        3: 1.0,  # culex (common)\n",
    "        4: 2.5,  # culiseta (uncommon)\n",
    "        5: 3.0  # japonicus/koreicus (uncommon)\n",
    "    }\n",
    "\n",
    "    # Class weighting parameters\n",
    "    use_weighted_dataset = True\n",
    "    weight_method = 'effective'  # 'inverse', 'effective', or 'manual'\n",
    "    effective_beta = 0.9  # For effective number of samples method\n",
    "    weight_aggregation = 'mean'  # 'mean', 'max', or 'sum'\n",
    "\n",
    "    # Additional augmentation for rare classes\n",
    "    augment_rare_classes = True\n",
    "    rare_class_ids = [0, 2]  # aegypti and anopheles\n",
    "    rare_augmentation_factor = 2.0  # Increase augmentation for rare classes\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(Config.output_dir, exist_ok=True)\n",
    "os.makedirs(Config.yolo_data_dir, exist_ok=True)\n",
    "\n",
    "def analyze_dataset():\n",
    "    \"\"\"Basic dataset analysis without logging\"\"\"\n",
    "    label_files = glob(os.path.join(Config.train_labels_dir, \"*.txt\"))\n",
    "    class_counts = {i: 0 for i in range(len(Config.class_names))}\n",
    "    \n",
    "    for label_file in tqdm(label_files, desc=\"Analyzing dataset\"):\n",
    "        img_file = os.path.join(\n",
    "            Config.train_images_dir, \n",
    "            os.path.basename(label_file).replace(\".txt\", \".jpg\")\n",
    "        )\n",
    "        \n",
    "        if os.path.exists(img_file):\n",
    "            # Class counts only\n",
    "            try:\n",
    "                with open(label_file, 'r') as f:\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) >= 5:\n",
    "                            class_id = int(parts[0])\n",
    "                            class_counts[class_id] += 1\n",
    "            except:\n",
    "                # Skip malformed label files\n",
    "                continue\n",
    "    \n",
    "    return class_counts\n",
    "\n",
    "def validate_label_file(label_path):\n",
    "    \"\"\"Validate and fix label file if needed\"\"\"\n",
    "    valid_lines = []\n",
    "    try:\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    try:\n",
    "                        class_id = int(parts[0])\n",
    "                        x_center = float(parts[1])\n",
    "                        y_center = float(parts[2])\n",
    "                        width = float(parts[3])\n",
    "                        height = float(parts[4])\n",
    "                        \n",
    "                        # Validate values are in range [0, 1]\n",
    "                        if (0 <= class_id < len(Config.class_names) and\n",
    "                            0 <= x_center <= 1 and 0 <= y_center <= 1 and\n",
    "                            0 <= width <= 1 and 0 <= height <= 1):\n",
    "                            valid_lines.append(line)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "        \n",
    "        # If file was modified, write back valid lines\n",
    "        if len(valid_lines) > 0:\n",
    "            with open(label_path, 'w') as f:\n",
    "                f.writelines(valid_lines)\n",
    "            return True\n",
    "        return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def prepare_stratified_folds():\n",
    "    \"\"\"Prepare dataset with stratified k-fold splitting based on class distribution\"\"\"\n",
    "    # Get all image and label files\n",
    "    label_files = glob(os.path.join(Config.train_labels_dir, \"*.txt\"))\n",
    "    \n",
    "    # Extract class distribution per image for stratification\n",
    "    image_data = []\n",
    "    \n",
    "    for label_file in tqdm(label_files, desc=\"Preparing stratification\"):\n",
    "        img_file = os.path.basename(label_file).replace(\".txt\", \".jpg\")\n",
    "        img_path = os.path.join(Config.train_images_dir, img_file)\n",
    "        \n",
    "        if os.path.exists(img_path):\n",
    "            # Validate and fix label file if needed\n",
    "            if not validate_label_file(label_file):\n",
    "                continue\n",
    "                \n",
    "            # Count classes in this image\n",
    "            class_counts = [0] * len(Config.class_names)\n",
    "            try:\n",
    "                with open(label_file, 'r') as f:\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) >= 5:\n",
    "                            class_id = int(parts[0])\n",
    "                            if 0 <= class_id < len(Config.class_names):\n",
    "                                class_counts[class_id] += 1\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            # Determine primary class (most frequent)\n",
    "            primary_class = np.argmax(class_counts) if sum(class_counts) > 0 else 0\n",
    "            image_data.append({\n",
    "                'image': img_file,\n",
    "                'label': os.path.basename(label_file),\n",
    "                'primary_class': primary_class,\n",
    "                'total_objects': sum(class_counts)\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame for easier manipulation\n",
    "    df = pd.DataFrame(image_data)\n",
    "    \n",
    "    # Debug: Check if DataFrame is properly created and has the expected columns\n",
    "    print(f\"DataFrame shape: {df.shape}\")\n",
    "    print(f\"DataFrame columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Check if primary_class column exists and has values\n",
    "    if 'primary_class' not in df.columns or df.empty:\n",
    "        print(\"Warning: 'primary_class' column missing or DataFrame is empty. Using basic fold splitting.\")\n",
    "        # Fallback: Create basic folds without stratification\n",
    "        total_images = len(image_data)\n",
    "        for fold in range(Config.folds):\n",
    "            # Create fold directories\n",
    "            fold_dir = os.path.join(Config.yolo_data_dir, f\"fold_{fold}\")\n",
    "            os.makedirs(os.path.join(fold_dir, \"images\", \"train\"), exist_ok=True)\n",
    "            os.makedirs(os.path.join(fold_dir, \"images\", \"val\"), exist_ok=True)\n",
    "            os.makedirs(os.path.join(fold_dir, \"labels\", \"train\"), exist_ok=True)\n",
    "            os.makedirs(os.path.join(fold_dir, \"labels\", \"val\"), exist_ok=True)\n",
    "            \n",
    "            # Simple split based on fold index\n",
    "            val_indices = np.arange(fold * (total_images // Config.folds), \n",
    "                                   (fold + 1) * (total_images // Config.folds))\n",
    "            \n",
    "            for i, item in enumerate(image_data):\n",
    "                is_val = i in val_indices\n",
    "                \n",
    "                # Determine destination folders\n",
    "                img_dest_dir = os.path.join(fold_dir, \"images\", \"val\" if is_val else \"train\")\n",
    "                label_dest_dir = os.path.join(fold_dir, \"labels\", \"val\" if is_val else \"train\")\n",
    "                \n",
    "                # Copy files\n",
    "                shutil.copy(\n",
    "                    os.path.join(Config.train_images_dir, item['image']),\n",
    "                    os.path.join(img_dest_dir, item['image'])\n",
    "                )\n",
    "                shutil.copy(\n",
    "                    os.path.join(Config.train_labels_dir, item['label']),\n",
    "                    os.path.join(label_dest_dir, item['label'])\n",
    "                )\n",
    "            \n",
    "            # Create YAML config\n",
    "            yaml_content = {\n",
    "                'path': os.path.abspath(fold_dir),\n",
    "                'train': 'images/train',\n",
    "                'val': 'images/val',\n",
    "                'names': Config.class_names,\n",
    "                'nc': len(Config.class_names)\n",
    "            }\n",
    "            \n",
    "            with open(os.path.join(fold_dir, 'data.yaml'), 'w') as f:\n",
    "                yaml.dump(yaml_content, f, default_flow_style=False)\n",
    "        \n",
    "        return Config.folds\n",
    "    \n",
    "    # Setup k-fold cross-validation with stratification by primary class\n",
    "    skf = StratifiedKFold(n_splits=Config.folds, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    # Create YAML config for each fold\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['primary_class'])):\n",
    "        # Create fold directories\n",
    "        fold_dir = os.path.join(Config.yolo_data_dir, f\"fold_{fold}\")\n",
    "        os.makedirs(os.path.join(fold_dir, \"images\", \"train\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(fold_dir, \"images\", \"val\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(fold_dir, \"labels\", \"train\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(fold_dir, \"labels\", \"val\"), exist_ok=True)\n",
    "        \n",
    "        # Split data\n",
    "        train_df = df.iloc[train_idx]\n",
    "        val_df = df.iloc[val_idx]\n",
    "        \n",
    "        # Copy files\n",
    "        for _, row in tqdm(train_df.iterrows(), desc=f\"Copying fold {fold} train files\", total=len(train_df)):\n",
    "            shutil.copy(\n",
    "                os.path.join(Config.train_images_dir, row['image']),\n",
    "                os.path.join(fold_dir, \"images\", \"train\", row['image'])\n",
    "            )\n",
    "            shutil.copy(\n",
    "                os.path.join(Config.train_labels_dir, row['label']),\n",
    "                os.path.join(fold_dir, \"labels\", \"train\", row['label'])\n",
    "            )\n",
    "        \n",
    "        for _, row in tqdm(val_df.iterrows(), desc=f\"Copying fold {fold} val files\", total=len(val_df)):\n",
    "            shutil.copy(\n",
    "                os.path.join(Config.train_images_dir, row['image']),\n",
    "                os.path.join(fold_dir, \"images\", \"val\", row['image'])\n",
    "            )\n",
    "            shutil.copy(\n",
    "                os.path.join(Config.train_labels_dir, row['label']),\n",
    "                os.path.join(fold_dir, \"labels\", \"val\", row['label'])\n",
    "            )\n",
    "        \n",
    "        # Create YAML config\n",
    "        yaml_content = {\n",
    "            'path': os.path.abspath(fold_dir),\n",
    "            'train': 'images/train',\n",
    "            'val': 'images/val',\n",
    "            'names': Config.class_names,\n",
    "            'nc': len(Config.class_names)\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(fold_dir, 'data.yaml'), 'w') as f:\n",
    "            yaml.dump(yaml_content, f, default_flow_style=False)\n",
    "    \n",
    "    return Config.folds\n",
    "\n",
    "\n",
    "# Function to apply the weighted dataset patch\n",
    "def setup_weighted_dataloading():\n",
    "    # Patch the YOLO dataloader to use our weighted dataset\n",
    "    try:\n",
    "        import ultralytics.data.build as build\n",
    "        from ultralytics.data.dataset import YOLODataset\n",
    "\n",
    "        # Save original build function\n",
    "        original_build_dataset = build.build_yolo_dataset\n",
    "\n",
    "        # Define the patched function\n",
    "        def patched_build_dataset(cfg, path, mode='train', batch=None, **kwargs):\n",
    "            # Get the usual dataset first\n",
    "            dataset = original_build_dataset(cfg, path, mode, batch, **kwargs)\n",
    "\n",
    "            # For training, replace with weighted dataset\n",
    "            if Config.use_weighted_dataset and 'train' in mode:\n",
    "                # Replace with our weighted version\n",
    "                dataset = YOLOWeightedDataset(\n",
    "                    cfg=cfg,\n",
    "                    path=path,\n",
    "                    mode=mode,\n",
    "                    batch=batch,\n",
    "                    class_weights=Config.class_weights,\n",
    "                    weight_method=Config.weight_method,\n",
    "                    effective_beta=Config.effective_beta,\n",
    "                    aggregation=Config.weight_aggregation,\n",
    "                    **kwargs\n",
    "                )\n",
    "                print(f\"Using weighted dataset for {mode} with {Config.weight_method} weighting\")\n",
    "\n",
    "            return dataset\n",
    "\n",
    "        # Replace the build function\n",
    "        build.build_yolo_dataset = patched_build_dataset\n",
    "\n",
    "        print(\"Successfully set up weighted dataset for training\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to set up weighted dataset: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def train_yolo_models():\n",
    "    \"\"\"Train YOLOv8 models with cross-validation and weighted sampling\"\"\"\n",
    "    # Apply weighted dataset patch\n",
    "    setup_weighted_dataloading()\n",
    "\n",
    "    models = []\n",
    "\n",
    "    # Loop through models and folds\n",
    "    for model_path in Config.yolo_models:\n",
    "        for fold in range(Config.folds):\n",
    "            fold_dir = os.path.join(Config.yolo_data_dir, f\"fold_{fold}\")\n",
    "            data_yaml_path = os.path.join(fold_dir, 'data.yaml')\n",
    "\n",
    "            # Load model\n",
    "            model = YOLO(model_path)\n",
    "\n",
    "            # Get class names from the YAML file\n",
    "            with open(data_yaml_path, 'r') as f:\n",
    "                data_yaml = yaml.safe_load(f)\n",
    "\n",
    "            # Train with custom hyperparameters\n",
    "            try:\n",
    "                # Remove the callbacks parameter since it's not supported\n",
    "                model.train(\n",
    "                    data=data_yaml_path,\n",
    "                    epochs=Config.epochs,\n",
    "                    imgsz=Config.yolo_img_size,\n",
    "                    batch=Config.batch_size,\n",
    "                    dropout=Config.dropout,\n",
    "                    freeze=Config.freeze,\n",
    "                    name=f\"{Path(model_path).stem}_fold{fold}\",\n",
    "                    project=Config.output_dir,\n",
    "                    pretrained=True,\n",
    "                    optimizer='AdamW',\n",
    "                    lr0=Config.lr0,\n",
    "                    lrf=Config.lrf,\n",
    "                    weight_decay=Config.weight_decay,\n",
    "                    warmup_epochs=5,\n",
    "                    warmup_momentum=0.8,\n",
    "                    box=7.5,\n",
    "                    cls=0.5,\n",
    "                    dfl=1.5,\n",
    "                    hsv_h=0.015,\n",
    "                    hsv_s=0.8,\n",
    "                    hsv_v=0.5,\n",
    "                    degrees=15,\n",
    "                    translate=0.2,\n",
    "                    scale=0.6,\n",
    "                    fliplr=0.5,\n",
    "                    flipud=0.2,\n",
    "                    mosaic=1.0,\n",
    "                    mixup=0.3,\n",
    "                    copy_paste=0.3,\n",
    "                    auto_augment='randaugment',\n",
    "                    cache=True,\n",
    "                    rect=False,\n",
    "                    cos_lr=True,\n",
    "                    close_mosaic=5,\n",
    "                    amp=True,\n",
    "                    overlap_mask=True,\n",
    "                    multi_scale=True\n",
    "                    # Removed callbacks parameter\n",
    "                )\n",
    "                \n",
    "                # After training, save a separate copy of the model for rare class optimization\n",
    "                # This is a workaround since we can't use custom callbacks\n",
    "                if Config.rare_class_ids:\n",
    "                    best_model_path = os.path.join(\n",
    "                        Config.output_dir,\n",
    "                        f\"{Path(model_path).stem}_fold{fold}\",\n",
    "                        \"weights\",\n",
    "                        \"best.pt\"\n",
    "                    )\n",
    "                    if os.path.exists(best_model_path):\n",
    "                        rare_model_path = os.path.join(\n",
    "                            Config.output_dir,\n",
    "                            f\"{Path(model_path).stem}_fold{fold}\",\n",
    "                            \"weights\",\n",
    "                            \"best_rare_classes.pt\"\n",
    "                        )\n",
    "                        shutil.copy(best_model_path, rare_model_path)\n",
    "                        print(f\"Saved copy of best model for rare class optimization at {rare_model_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Training error: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Get best models for ensemble\n",
    "            best_model_path = os.path.join(\n",
    "                Config.output_dir,\n",
    "                f\"{Path(model_path).stem}_fold{fold}\",\n",
    "                \"weights\",\n",
    "                \"best.pt\"\n",
    "            )\n",
    "\n",
    "            best_rare_path = os.path.join(\n",
    "                Config.output_dir,\n",
    "                f\"{Path(model_path).stem}_fold{fold}\",\n",
    "                \"weights\",\n",
    "                \"best_rare_classes.pt\"\n",
    "            )\n",
    "\n",
    "            if os.path.exists(best_model_path):\n",
    "                models.append(best_model_path)\n",
    "\n",
    "            if os.path.exists(best_rare_path):\n",
    "                models.append(best_rare_path)\n",
    "\n",
    "            # Clean up GPU memory\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "def ensemble_predictions(predictions, img_shape):\n",
    "    \"\"\"Ensemble multiple predictions with weighted box fusion and rare class preference\"\"\"\n",
    "    try:\n",
    "        from ensemble_boxes import weighted_boxes_fusion\n",
    "    except ImportError:\n",
    "        print(\"ensemble_boxes not installed. Installing...\")\n",
    "        import pip\n",
    "        pip.main(['install', 'ensemble-boxes'])\n",
    "        from ensemble_boxes import weighted_boxes_fusion\n",
    "\n",
    "    boxes_list = []\n",
    "    scores_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    h, w = img_shape[:2]\n",
    "\n",
    "    # Extract boxes, scores, and labels from each prediction\n",
    "    for pred in predictions:\n",
    "        if len(pred.boxes) > 0:\n",
    "            # Get normalized xyxy boxes\n",
    "            boxes = pred.boxes.xyxyn.cpu().numpy()\n",
    "            scores = pred.boxes.conf.cpu().numpy()\n",
    "            labels = pred.boxes.cls.cpu().numpy().astype(int)\n",
    "\n",
    "            # Apply score boosting for rare classes\n",
    "            for i, label in enumerate(labels):\n",
    "                if label in Config.rare_class_ids:\n",
    "                    # Boost the confidence of rare classes\n",
    "                    scores[i] = min(1.0, scores[i] * 1.1)  # 10% boost but cap at 1.0\n",
    "\n",
    "            boxes_list.append(boxes)\n",
    "            scores_list.append(scores)\n",
    "            labels_list.append(labels)\n",
    "        else:\n",
    "            # Add empty lists if no detections\n",
    "            boxes_list.append([])\n",
    "            scores_list.append([])\n",
    "            labels_list.append([])\n",
    "\n",
    "    # Skip fusion if no boxes\n",
    "    if not any(boxes_list):\n",
    "        return [], [], []\n",
    "\n",
    "    # Apply weighted box fusion\n",
    "    try:\n",
    "        # Use higher weights for predictions from models optimized for rare classes\n",
    "        weights = None\n",
    "        if len(boxes_list) > 1:\n",
    "            weights = [1.0] * len(boxes_list)\n",
    "            for i, model_path in enumerate(model_paths):\n",
    "                if \"best_rare_classes\" in model_path:\n",
    "                    weights[i] = 1.5  # Higher weight for rare-class-optimized models\n",
    "\n",
    "        boxes, scores, labels = weighted_boxes_fusion(\n",
    "            boxes_list, scores_list, labels_list,\n",
    "            weights=weights,\n",
    "            iou_thr=Config.iou_threshold,\n",
    "            skip_box_thr=Config.confidence_threshold\n",
    "        )\n",
    "\n",
    "        # Denormalize boxes to get pixel coordinates\n",
    "        boxes[:, [0, 2]] *= w\n",
    "        boxes[:, [1, 3]] *= h\n",
    "\n",
    "        return boxes, scores, labels\n",
    "    except Exception as e:\n",
    "        print(f\"Box fusion error: {e}\")\n",
    "        # Fallback to first prediction if fusion fails\n",
    "        if boxes_list and boxes_list[0]:\n",
    "            return (boxes_list[0] * np.array([[w, h, w, h]])), scores_list[0], labels_list[0]\n",
    "        return [], [], []\n",
    "\n",
    "def apply_tta(model, img, scales, flips):\n",
    "    \"\"\"Apply test-time augmentation with various scales and flips\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    # Handle PIL decompression bomb warning by resizing large images\n",
    "    h, w = img.shape[:2]\n",
    "    max_pixels = 89000000  # Just under the PIL limit\n",
    "    \n",
    "    if h * w > max_pixels:\n",
    "        scale_factor = np.sqrt(max_pixels / (h * w))\n",
    "        h_new, w_new = int(h * scale_factor), int(w * scale_factor)\n",
    "        img = cv2.resize(img, (w_new, h_new))\n",
    "        h, w = h_new, w_new\n",
    "    \n",
    "    for scale in scales:\n",
    "        # Scale the image\n",
    "        new_h, new_w = int(h * scale), int(w * scale)\n",
    "        scaled_img = cv2.resize(img, (new_w, new_h))\n",
    "        \n",
    "        for flip in flips:\n",
    "            test_img = scaled_img.copy()\n",
    "            \n",
    "            # Apply flip if needed\n",
    "            if flip == 'horizontal':\n",
    "                test_img = cv2.flip(test_img, 1)\n",
    "            \n",
    "            # Run prediction\n",
    "            try:\n",
    "                pred = model.predict(test_img, conf=Config.confidence_threshold, iou=Config.iou_threshold, verbose=False)[0]\n",
    "                \n",
    "                # If horizontal flip, adjust the predictions\n",
    "                if flip == 'horizontal' and len(pred.boxes) > 0:\n",
    "                    # Get xyxy format (non-normalized)\n",
    "                    boxes = pred.boxes.xyxy.cpu().numpy()\n",
    "                    for box in boxes:\n",
    "                        # Flip x-coordinates\n",
    "                        box[0], box[2] = new_w - box[2], new_w - box[0]\n",
    "                \n",
    "                # Store prediction\n",
    "                predictions.append(pred)\n",
    "            except Exception as e:\n",
    "                print(f\"Prediction error: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def generate_predictions(model_paths, test_dir):\n",
    "    \"\"\"Generate predictions using ensemble of models with TTA\"\"\"\n",
    "    # Load models\n",
    "    models = []\n",
    "    for path in model_paths:\n",
    "        try:\n",
    "            models.append(YOLO(path))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {path}: {e}\")\n",
    "    \n",
    "    if not models:\n",
    "        print(\"No models loaded. Cannot generate predictions.\")\n",
    "        return\n",
    "    \n",
    "    # Get test images\n",
    "    test_images = glob(os.path.join(test_dir, \"*.jpg\"))\n",
    "    \n",
    "    all_predictions = []\n",
    "    \n",
    "    for img_path in tqdm(test_images, desc=\"Generating predictions\"):\n",
    "        img_id = os.path.basename(img_path)\n",
    "        \n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                raise ValueError(f\"Failed to load image: {img_path}\")\n",
    "            \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            h, w = img.shape[:2]\n",
    "            \n",
    "            all_model_predictions = []\n",
    "            \n",
    "            # Process each model\n",
    "            for model in models:\n",
    "                if Config.tta:\n",
    "                    # Apply test-time augmentation\n",
    "                    preds = apply_tta(model, img, Config.tta_scales, Config.tta_flips)\n",
    "                    all_model_predictions.extend(preds)\n",
    "                else:\n",
    "                    # Regular prediction\n",
    "                    try:\n",
    "                        pred = model.predict(img, conf=Config.confidence_threshold, iou=Config.iou_threshold, verbose=False)[0]\n",
    "                        all_model_predictions.append(pred)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Prediction error on {img_id}: {e}\")\n",
    "            \n",
    "            # Ensemble predictions if multiple\n",
    "            if len(all_model_predictions) > 1:\n",
    "                boxes, scores, labels = ensemble_predictions(all_model_predictions, img.shape)\n",
    "            elif len(all_model_predictions) == 1:\n",
    "                # Just one prediction, use it directly\n",
    "                pred = all_model_predictions[0]\n",
    "                if len(pred.boxes) > 0:\n",
    "                    boxes = pred.boxes.xyxy.cpu().numpy()\n",
    "                    scores = pred.boxes.conf.cpu().numpy()\n",
    "                    labels = pred.boxes.cls.cpu().numpy().astype(int)\n",
    "                    \n",
    "                    # Filter by final confidence threshold\n",
    "                    mask = scores >= Config.ensemble_conf_threshold\n",
    "                    boxes = boxes[mask]\n",
    "                    scores = scores[mask]\n",
    "                    labels = labels[mask]\n",
    "                else:\n",
    "                    boxes, scores, labels = [], [], []\n",
    "            else:\n",
    "                boxes, scores, labels = [], [], []\n",
    "            \n",
    "            # Convert to submission format\n",
    "            if len(boxes) > 0:\n",
    "                for box, score, label in zip(boxes, scores, labels):\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    \n",
    "                    # Convert to center, width, height and normalize\n",
    "                    x_center = ((x1 + x2) / 2) / w\n",
    "                    y_center = ((y1 + y2) / 2) / h\n",
    "                    width = (x2 - x1) / w\n",
    "                    height = (y2 - y1) / h\n",
    "                    \n",
    "                    # Ensure values are between 0 and 1\n",
    "                    x_center = max(0, min(1, x_center))\n",
    "                    y_center = max(0, min(1, y_center))\n",
    "                    width = max(0, min(1, width))\n",
    "                    height = max(0, min(1, height))\n",
    "                    \n",
    "                    # Ensure label is valid\n",
    "                    if label >= 0 and label < len(Config.class_names):\n",
    "                        class_name = Config.class_names.get(label, \"Unknown\")\n",
    "                        \n",
    "                        all_predictions.append({\n",
    "                            'ImageID': img_id,\n",
    "                            'LabelName': class_name,\n",
    "                            'Conf': float(score),\n",
    "                            'xcenter': float(x_center),\n",
    "                            'ycenter': float(y_center),\n",
    "                            'bbx_width': float(width),\n",
    "                            'bbx_height': float(height)\n",
    "                        })\n",
    "            else:\n",
    "                # Default predictions for empty images - based on class distribution\n",
    "                # Add one prediction for each of the two most common classes\n",
    "                common_classes = [\"albopictus\", \"culex\"]\n",
    "                \n",
    "                for idx, class_name in enumerate(common_classes):\n",
    "                    # Create different positions for the default boxes\n",
    "                    x_center = 0.4 + (idx * 0.2)\n",
    "                    y_center = 0.4 + (idx * 0.2)\n",
    "                    \n",
    "                    all_predictions.append({\n",
    "                        'ImageID': img_id,\n",
    "                        'LabelName': class_name,\n",
    "                        'Conf': float(Config.confidence_threshold),\n",
    "                        'xcenter': x_center,\n",
    "                        'ycenter': y_center,\n",
    "                        'bbx_width': 0.3,\n",
    "                        'bbx_height': 0.3\n",
    "                    })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_id}: {e}\")\n",
    "            # Add fallback prediction\n",
    "            all_predictions.append({\n",
    "                'ImageID': img_id,\n",
    "                'LabelName': \"albopictus\",  # Most common class\n",
    "                'Conf': float(Config.confidence_threshold),\n",
    "                'xcenter': 0.5,\n",
    "                'ycenter': 0.5,\n",
    "                'bbx_width': 0.3,\n",
    "                'bbx_height': 0.3\n",
    "            })\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission_df = pd.DataFrame(all_predictions)\n",
    "    submission_df.insert(0, 'id', range(len(submission_df)))\n",
    "    \n",
    "    # Save submission file\n",
    "    submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "def main():\n",
    "    # Analyze dataset - basic analysis only, no logging\n",
    "    analyze_dataset()\n",
    "    \n",
    "    # Prepare stratified folds\n",
    "    prepare_stratified_folds()\n",
    "    \n",
    "    # Train models\n",
    "    model_paths = train_yolo_models()\n",
    "    \n",
    "    # Generate ensemble predictions\n",
    "    generate_predictions(model_paths, Config.test_images_dir)\n",
    "    \n",
    "    print(\"Processing completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11404671,
     "sourceId": 96038,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 62.812565,
   "end_time": "2025-03-23T18:45:03.055918",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-23T18:44:00.243353",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "008c2ef38cda4113b55e436d5e66e9ab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_594c52fc12b148e7bd491ea5e9d499af",
        "IPY_MODEL_16788dc1466e4331bd74a3b40aed23fc",
        "IPY_MODEL_f9c7d425dc1345259b389154e37101fa"
       ],
       "layout": "IPY_MODEL_c597ea924bf4476f8c76e2df8b2e7f17",
       "tabbable": null,
       "tooltip": null
      }
     },
     "16788dc1466e4331bd74a3b40aed23fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_abc23aff997741f2afbdfe35e25a4454",
       "max": 7500.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f1e8527c13d649e9978f719a15cace65",
       "tabbable": null,
       "tooltip": null,
       "value": 7500.0
      }
     },
     "17ed0a7e6f8648c3880ba05a360792b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "1951195eeb044e09811ce632de4ad606": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1e3b52b3b14e405e8c5cec6e2442345e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2988c09102bc4acfa235f9d259b5e0f4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5e65c2dd5a9d48fa9f78f5c2ff772852",
        "IPY_MODEL_b6e74d3e4b674d4cabff1e096f03dafd",
        "IPY_MODEL_91f63f2533c941e3bbab30054241d227"
       ],
       "layout": "IPY_MODEL_a68f2021514d4348a7ba1542dc04b889",
       "tabbable": null,
       "tooltip": null
      }
     },
     "2df3fd533e094c448df4389df1298ea4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3ccdfa2d086d4d428facdad1c491029c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4362d10e1bb34404b4c368c4898cc8e5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "594c52fc12b148e7bd491ea5e9d499af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1951195eeb044e09811ce632de4ad606",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_1e3b52b3b14e405e8c5cec6e2442345e",
       "tabbable": null,
       "tooltip": null,
       "value": "Analyzingâ€‡dataset:â€‡100%"
      }
     },
     "5e65c2dd5a9d48fa9f78f5c2ff772852": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fec4a2328bcc4ad3ab21fd8eaf8d630d",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_6c35977eab9941bb91554608d23d4390",
       "tabbable": null,
       "tooltip": null,
       "value": "Preparingâ€‡stratification:â€‡100%"
      }
     },
     "6c35977eab9941bb91554608d23d4390": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "91f63f2533c941e3bbab30054241d227": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2df3fd533e094c448df4389df1298ea4",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_93b5ff88981042bd808f197db688605e",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡7500/7500â€‡[00:00&lt;00:00,â€‡120753.60it/s]"
      }
     },
     "93b5ff88981042bd808f197db688605e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a68f2021514d4348a7ba1542dc04b889": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "abc23aff997741f2afbdfe35e25a4454": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b6e74d3e4b674d4cabff1e096f03dafd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3ccdfa2d086d4d428facdad1c491029c",
       "max": 7500.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_17ed0a7e6f8648c3880ba05a360792b0",
       "tabbable": null,
       "tooltip": null,
       "value": 7500.0
      }
     },
     "bdab882fb73542a28d4687ce4064b71b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c597ea924bf4476f8c76e2df8b2e7f17": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f1e8527c13d649e9978f719a15cace65": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f9c7d425dc1345259b389154e37101fa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_bdab882fb73542a28d4687ce4064b71b",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_4362d10e1bb34404b4c368c4898cc8e5",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡7500/7500â€‡[00:05&lt;00:00,â€‡1330.50it/s]"
      }
     },
     "fec4a2328bcc4ad3ab21fd8eaf8d630d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
