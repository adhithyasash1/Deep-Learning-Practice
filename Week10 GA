{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00eed81f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T18:39:23.222077Z",
     "iopub.status.busy": "2025-03-23T18:39:23.221728Z",
     "iopub.status.idle": "2025-03-23T18:39:23.230107Z",
     "shell.execute_reply": "2025-03-23T18:39:23.229393Z"
    },
    "papermill": {
     "duration": 0.014764,
     "end_time": "2025-03-23T18:39:23.231206",
     "exception": false,
     "start_time": "2025-03-23T18:39:23.216442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nimport numpy as np\\nimport pandas as pd\\nfrom PIL import Image\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom collections import defaultdict\\n\\n# Configuration\\ndata_dir = \"/kaggle/input/dlp-object-detection-week-10/final_dlp_data/final_dlp_data\"\\ntrain_images_dir = os.path.join(data_dir, \"train\", \"images\")\\ntrain_labels_dir = os.path.join(data_dir, \"train\", \"labels\")\\ntest_images_dir = os.path.join(data_dir, \"test\", \"images\")\\n\\nclass_names = {\\n    0: \"aegypti\",\\n    1: \"albopictus\",\\n    2: \"anopheles\",\\n    3: \"culex\",\\n    4: \"culiseta\",\\n    5: \"japonicus/koreicus\"\\n}\\n\\n# 1. Class Distribution Analysis\\nclass_counts = defaultdict(int)\\nlabel_files = [f for f in os.listdir(train_labels_dir) if f.endswith(\\'.txt\\')]\\n\\nfor lbl_file in label_files:\\n    with open(os.path.join(train_labels_dir, lbl_file), \\'r\\') as f:\\n        lines = f.readlines()\\n        for line in lines:\\n            class_id = int(line.strip().split()[0])\\n            class_counts[class_id] += 1\\n\\n# Plot class distribution\\nplt.figure(figsize=(12, 6))\\nsns.barplot(x=[class_names[k] for k in class_counts.keys()], y=list(class_counts.values()))\\nplt.title(\\'Class Distribution\\')\\nplt.xlabel(\\'Mosquito Species\\')\\nplt.ylabel(\\'Count\\')\\nplt.xticks(rotation=45)\\nplt.show()\\n\\n# 2. Image Size Analysis\\nimage_sizes = []\\naspect_ratios = []\\n\\nfor img_file in os.listdir(train_images_dir)[:1000]:  # Sample 1000 images\\n    with Image.open(os.path.join(train_images_dir, img_file)) as img:\\n        width, height = img.size\\n        image_sizes.append((width, height))\\n        aspect_ratios.append(width / height)\\n\\n# Plot image dimensions\\nplt.figure(figsize=(12, 6))\\nplt.scatter([s[0] for s in image_sizes], [s[1] for s in image_sizes], alpha=0.5)\\nplt.title(\\'Image Dimensions Scatter Plot\\')\\nplt.xlabel(\\'Width\\')\\nplt.ylabel(\\'Height\\')\\nplt.show()\\n\\n# Plot aspect ratio distribution\\nplt.figure(figsize=(12, 6))\\nsns.histplot(aspect_ratios, bins=30, kde=True)\\nplt.title(\\'Aspect Ratio Distribution\\')\\nplt.xlabel(\\'Width/Height Ratio\\')\\nplt.show()\\n\\n# 3. Bounding Box Analysis\\nbbox_sizes = []\\nbbox_positions = []\\nper_class_bbox = defaultdict(list)\\n\\nfor lbl_file in label_files[:1000]:  # Sample 1000 labels\\n    with open(os.path.join(train_labels_dir, lbl_file), \\'r\\') as f:\\n        lines = f.readlines()\\n        for line in lines:\\n            parts = line.strip().split()\\n            class_id = int(parts[0])\\n            x_center = float(parts[1])\\n            y_center = float(parts[2])\\n            width = float(parts[3])\\n            height = float(parts[4])\\n            \\n            # Store normalized dimensions\\n            bbox_sizes.append((width, height))\\n            bbox_positions.append((x_center, y_center))\\n            per_class_bbox[class_id].append((width, height))\\n\\n# Plot bounding box sizes\\nplt.figure(figsize=(12, 6))\\nplt.scatter([s[0] for s in bbox_sizes], [s[1] for s in bbox_sizes], alpha=0.3)\\nplt.title(\\'Normalized Bounding Box Sizes\\')\\nplt.xlabel(\\'Width (normalized)\\')\\nplt.ylabel(\\'Height (normalized)\\')\\nplt.show()\\n\\n# 4. Sample Visualization with Bounding Boxes\\ndef plot_sample(image_path, label_path):\\n    img = Image.open(image_path)\\n    width, height = img.size\\n    \\n    with open(label_path, \\'r\\') as f:\\n        boxes = []\\n        for line in f.readlines():\\n            class_id, xc, yc, w, h = map(float, line.strip().split())\\n            \\n            # Convert from normalized to pixel coordinates\\n            x = (xc - w/2) * width\\n            y = (yc - h/2) * height\\n            box_w = w * width\\n            box_h = h * height\\n            \\n            boxes.append({\\n                \\'class\\': class_names[int(class_id)],\\n                \\'box\\': [x, y, box_w, box_h]\\n            })\\n    \\n    plt.figure(figsize=(12, 8))\\n    plt.imshow(img)\\n    for box in boxes:\\n        x, y, w, h = box[\\'box\\']\\n        plt.gca().add_patch(plt.Rectangle(\\n            (x, y), w, h, \\n            linewidth=2, \\n            edgecolor=\\'r\\', \\n            facecolor=\\'none\\'\\n        ))\\n        plt.text(x, y-5, box[\\'class\\'], color=\\'r\\', fontsize=10, backgroundcolor=\\'white\\')\\n    plt.axis(\\'off\\')\\n    plt.show()\\n\\n# Plot 5 random samples\\nfor _ in range(5):\\n    random_img = np.random.choice(os.listdir(train_images_dir))\\n    img_path = os.path.join(train_images_dir, random_img)\\n    lbl_path = os.path.join(train_labels_dir, os.path.splitext(random_img)[0] + \\'.txt\\')\\n    if os.path.exists(lbl_path):\\n        plot_sample(img_path, lbl_path)\\n\\n# 5. Data Quality Checks\\n# Check for missing labels\\nmissing_labels = 0\\nfor img_file in os.listdir(train_images_dir):\\n    lbl_file = os.path.splitext(img_file)[0] + \\'.txt\\'\\n    if not os.path.exists(os.path.join(train_labels_dir, lbl_file)):\\n        missing_labels += 1\\n\\nprint(f\"Number of images without labels: {missing_labels}\")\\n\\n# Check for empty labels\\nempty_labels = 0\\nfor lbl_file in os.listdir(train_labels_dir):\\n    with open(os.path.join(train_labels_dir, lbl_file), \\'r\\') as f:\\n        if len(f.readlines()) == 0:\\n            empty_labels += 1\\n\\nprint(f\"Number of empty label files: {empty_labels}\")\\n\\n# 6. Image Channel Analysis\\ndef analyze_channels(image_dir):\\n    channels = defaultdict(int)\\n    for img_file in os.listdir(image_dir)[:1000]:\\n        with Image.open(os.path.join(image_dir, img_file)) as img:\\n            channels[len(img.getbands())] += 1\\n    return channels\\n\\nprint(\"\\nTraining image channels:\")\\nprint(analyze_channels(train_images_dir))\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "# Configuration\n",
    "data_dir = \"/kaggle/input/dlp-object-detection-week-10/final_dlp_data/final_dlp_data\"\n",
    "train_images_dir = os.path.join(data_dir, \"train\", \"images\")\n",
    "train_labels_dir = os.path.join(data_dir, \"train\", \"labels\")\n",
    "test_images_dir = os.path.join(data_dir, \"test\", \"images\")\n",
    "\n",
    "class_names = {\n",
    "    0: \"aegypti\",\n",
    "    1: \"albopictus\",\n",
    "    2: \"anopheles\",\n",
    "    3: \"culex\",\n",
    "    4: \"culiseta\",\n",
    "    5: \"japonicus/koreicus\"\n",
    "}\n",
    "\n",
    "# 1. Class Distribution Analysis\n",
    "class_counts = defaultdict(int)\n",
    "label_files = [f for f in os.listdir(train_labels_dir) if f.endswith('.txt')]\n",
    "\n",
    "for lbl_file in label_files:\n",
    "    with open(os.path.join(train_labels_dir, lbl_file), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            class_id = int(line.strip().split()[0])\n",
    "            class_counts[class_id] += 1\n",
    "\n",
    "# Plot class distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=[class_names[k] for k in class_counts.keys()], y=list(class_counts.values()))\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Mosquito Species')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# 2. Image Size Analysis\n",
    "image_sizes = []\n",
    "aspect_ratios = []\n",
    "\n",
    "for img_file in os.listdir(train_images_dir)[:1000]:  # Sample 1000 images\n",
    "    with Image.open(os.path.join(train_images_dir, img_file)) as img:\n",
    "        width, height = img.size\n",
    "        image_sizes.append((width, height))\n",
    "        aspect_ratios.append(width / height)\n",
    "\n",
    "# Plot image dimensions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter([s[0] for s in image_sizes], [s[1] for s in image_sizes], alpha=0.5)\n",
    "plt.title('Image Dimensions Scatter Plot')\n",
    "plt.xlabel('Width')\n",
    "plt.ylabel('Height')\n",
    "plt.show()\n",
    "\n",
    "# Plot aspect ratio distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(aspect_ratios, bins=30, kde=True)\n",
    "plt.title('Aspect Ratio Distribution')\n",
    "plt.xlabel('Width/Height Ratio')\n",
    "plt.show()\n",
    "\n",
    "# 3. Bounding Box Analysis\n",
    "bbox_sizes = []\n",
    "bbox_positions = []\n",
    "per_class_bbox = defaultdict(list)\n",
    "\n",
    "for lbl_file in label_files[:1000]:  # Sample 1000 labels\n",
    "    with open(os.path.join(train_labels_dir, lbl_file), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            class_id = int(parts[0])\n",
    "            x_center = float(parts[1])\n",
    "            y_center = float(parts[2])\n",
    "            width = float(parts[3])\n",
    "            height = float(parts[4])\n",
    "            \n",
    "            # Store normalized dimensions\n",
    "            bbox_sizes.append((width, height))\n",
    "            bbox_positions.append((x_center, y_center))\n",
    "            per_class_bbox[class_id].append((width, height))\n",
    "\n",
    "# Plot bounding box sizes\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter([s[0] for s in bbox_sizes], [s[1] for s in bbox_sizes], alpha=0.3)\n",
    "plt.title('Normalized Bounding Box Sizes')\n",
    "plt.xlabel('Width (normalized)')\n",
    "plt.ylabel('Height (normalized)')\n",
    "plt.show()\n",
    "\n",
    "# 4. Sample Visualization with Bounding Boxes\n",
    "def plot_sample(image_path, label_path):\n",
    "    img = Image.open(image_path)\n",
    "    width, height = img.size\n",
    "    \n",
    "    with open(label_path, 'r') as f:\n",
    "        boxes = []\n",
    "        for line in f.readlines():\n",
    "            class_id, xc, yc, w, h = map(float, line.strip().split())\n",
    "            \n",
    "            # Convert from normalized to pixel coordinates\n",
    "            x = (xc - w/2) * width\n",
    "            y = (yc - h/2) * height\n",
    "            box_w = w * width\n",
    "            box_h = h * height\n",
    "            \n",
    "            boxes.append({\n",
    "                'class': class_names[int(class_id)],\n",
    "                'box': [x, y, box_w, box_h]\n",
    "            })\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(img)\n",
    "    for box in boxes:\n",
    "        x, y, w, h = box['box']\n",
    "        plt.gca().add_patch(plt.Rectangle(\n",
    "            (x, y), w, h, \n",
    "            linewidth=2, \n",
    "            edgecolor='r', \n",
    "            facecolor='none'\n",
    "        ))\n",
    "        plt.text(x, y-5, box['class'], color='r', fontsize=10, backgroundcolor='white')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Plot 5 random samples\n",
    "for _ in range(5):\n",
    "    random_img = np.random.choice(os.listdir(train_images_dir))\n",
    "    img_path = os.path.join(train_images_dir, random_img)\n",
    "    lbl_path = os.path.join(train_labels_dir, os.path.splitext(random_img)[0] + '.txt')\n",
    "    if os.path.exists(lbl_path):\n",
    "        plot_sample(img_path, lbl_path)\n",
    "\n",
    "# 5. Data Quality Checks\n",
    "# Check for missing labels\n",
    "missing_labels = 0\n",
    "for img_file in os.listdir(train_images_dir):\n",
    "    lbl_file = os.path.splitext(img_file)[0] + '.txt'\n",
    "    if not os.path.exists(os.path.join(train_labels_dir, lbl_file)):\n",
    "        missing_labels += 1\n",
    "\n",
    "print(f\"Number of images without labels: {missing_labels}\")\n",
    "\n",
    "# Check for empty labels\n",
    "empty_labels = 0\n",
    "for lbl_file in os.listdir(train_labels_dir):\n",
    "    with open(os.path.join(train_labels_dir, lbl_file), 'r') as f:\n",
    "        if len(f.readlines()) == 0:\n",
    "            empty_labels += 1\n",
    "\n",
    "print(f\"Number of empty label files: {empty_labels}\")\n",
    "\n",
    "# 6. Image Channel Analysis\n",
    "def analyze_channels(image_dir):\n",
    "    channels = defaultdict(int)\n",
    "    for img_file in os.listdir(image_dir)[:1000]:\n",
    "        with Image.open(os.path.join(image_dir, img_file)) as img:\n",
    "            channels[len(img.getbands())] += 1\n",
    "    return channels\n",
    "\n",
    "print(\"\\nTraining image channels:\")\n",
    "print(analyze_channels(train_images_dir))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4514c81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T18:39:23.239754Z",
     "iopub.status.busy": "2025-03-23T18:39:23.239526Z",
     "iopub.status.idle": "2025-03-23T18:39:23.245254Z",
     "shell.execute_reply": "2025-03-23T18:39:23.244546Z"
    },
    "papermill": {
     "duration": 0.011376,
     "end_time": "2025-03-23T18:39:23.246635",
     "exception": false,
     "start_time": "2025-03-23T18:39:23.235259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nimport numpy as np\\nfrom PIL import Image\\nfrom collections import defaultdict\\n\\n# Configuration\\ndata_dir = \"/kaggle/input/dlp-object-detection-week-10/final_dlp_data/final_dlp_data\"\\ntrain_images_dir = os.path.join(data_dir, \"train\", \"images\")\\ntrain_labels_dir = os.path.join(data_dir, \"train\", \"labels\")\\n\\nclass_names = {\\n    0: \"aegypti\",\\n    1: \"albopictus\",\\n    2: \"anopheles\",\\n    3: \"culex\",\\n    4: \"culiseta\",\\n    5: \"japonicus/koreicus\"\\n}\\n\\ndef analyze_dataset():\\n    # 1. Class Distribution Analysis\\n    class_counts = defaultdict(int)\\n    label_files = [f for f in os.listdir(train_labels_dir) if f.endswith(\\'.txt\\')]\\n    \\n    for lbl_file in label_files:\\n        with open(os.path.join(train_labels_dir, lbl_file), \\'r\\') as f:\\n            for line in f.readlines():\\n                class_id = int(line.strip().split()[0])\\n                class_counts[class_id] += 1\\n\\n    print(\"=== Class Distribution ===\")\\n    for cls_id, count in sorted(class_counts.items()):\\n        print(f\"{class_names[cls_id]:<20}: {count} samples ({count/len(label_files):.2f} per image)\")\\n    print(\"\\n\")\\n\\n    # 2. Image Size Statistics\\n    image_sizes = []\\n    for img_file in os.listdir(train_images_dir)[:1000]:  # Sample 1000 images\\n        with Image.open(os.path.join(train_images_dir, img_file)) as img:\\n            image_sizes.append(img.size)\\n    \\n    widths, heights = zip(*image_sizes)\\n    print(\"=== Image Dimensions ===\")\\n    print(f\"Average width: {np.mean(widths):.1f} ± {np.std(widths):.1f} px\")\\n    print(f\"Average height: {np.mean(heights):.1f} ± {np.std(heights):.1f} px\")\\n    print(f\"Min dimensions: {min(widths)}x{min(heights)}\")\\n    print(f\"Max dimensions: {max(widths)}x{max(heights)}\")\\n    print(\"\\n\")\\n\\n    # 3. Aspect Ratio Analysis\\n    aspect_ratios = [w/h for w, h in image_sizes]\\n    print(\"=== Aspect Ratios ===\")\\n    print(f\"Mean aspect ratio: {np.mean(aspect_ratios):.2f} ± {np.std(aspect_ratios):.2f}\")\\n    print(f\"Most common ratio: {max(set(aspect_ratios), key=aspect_ratios.count):.2f}\")\\n    print(\"\\n\")\\n\\n    # 4. Bounding Box Statistics\\n    bbox_sizes = []\\n    per_class_bbox = defaultdict(list)\\n    \\n    for lbl_file in label_files[:1000]:  # Sample 1000 labels\\n        with open(os.path.join(train_labels_dir, lbl_file), \\'r\\') as f:\\n            for line in f.readlines():\\n                parts = line.strip().split()\\n                cls_id = int(parts[0])\\n                width = float(parts[3])\\n                height = float(parts[4])\\n                bbox_sizes.append((width, height))\\n                per_class_bbox[cls_id].append((width, height))\\n\\n    bbox_areas = [w*h for w, h in bbox_sizes]\\n    print(\"=== Bounding Boxes ===\")\\n    print(f\"Average box size: {np.mean(bbox_areas):.4f} ± {np.std(bbox_areas):.4f} (normalized area)\")\\n    print(f\"Average boxes per image: {len(bbox_sizes)/1000:.2f}\")\\n    print(\"\\nPer-class box sizes (normalized width/height):\")\\n    for cls_id in sorted(per_class_bbox.keys()):\\n        sizes = np.array(per_class_bbox[cls_id])\\n        print(f\"{class_names[cls_id]:<20}: W={np.mean(sizes[:,0]):.3f} ± {np.std(sizes[:,0]):.3f}, \"\\n              f\"H={np.mean(sizes[:,1]):.3f} ± {np.std(sizes[:,1]):.3f}\")\\n    print(\"\\n\")\\n\\n    # 5. Data Quality Report\\n    missing_labels = 0\\n    for img_file in os.listdir(train_images_dir):\\n        lbl_file = os.path.splitext(img_file)[0] + \\'.txt\\'\\n        if not os.path.exists(os.path.join(train_labels_dir, lbl_file)):\\n            missing_labels += 1\\n\\n    empty_labels = 0\\n    for lbl_file in os.listdir(train_labels_dir):\\n        if os.path.getsize(os.path.join(train_labels_dir, lbl_file)) == 0:\\n            empty_labels += 1\\n\\n    print(\"=== Data Quality ===\")\\n    print(f\"Total training images: {len(os.listdir(train_images_dir))}\")\\n    print(f\"Total label files: {len(label_files)}\")\\n    print(f\"Missing labels: {missing_labels}\")\\n    print(f\"Empty label files: {empty_labels}\")\\n    print(\"\\n\")\\n\\n    # 6. Channel Analysis\\n    channels = defaultdict(int)\\n    for img_file in os.listdir(train_images_dir)[:1000]:\\n        with Image.open(os.path.join(train_images_dir, img_file)) as img:\\n            channels[len(img.getbands())] += 1\\n\\n    print(\"=== Color Channels ===\")\\n    for ch, count in channels.items():\\n        print(f\"{ch} channels: {count} images\")\\n\\ndef show_sample_labels(n=3):\\n    print(\"\\n=== Sample Labels ===\")\\n    for img_file in os.listdir(train_images_dir)[:n]:\\n        lbl_file = os.path.splitext(img_file)[0] + \\'.txt\\'\\n        lbl_path = os.path.join(train_labels_dir, lbl_file)\\n        if os.path.exists(lbl_path):\\n            with open(lbl_path, \\'r\\') as f:\\n                lines = f.readlines()\\n                print(f\"\\nImage: {img_file}\")\\n                for line in lines:\\n                    parts = line.strip().split()\\n                    cls_id = int(parts[0])\\n                    print(f\"- {class_names[cls_id]}: \"\\n                          f\"x={float(parts[1]):.3f}, y={float(parts[2]):.3f}, \"\\n                          f\"w={float(parts[3]):.3f}, h={float(parts[4]):.3f}\")\\n\\n# Execute analysis\\nanalyze_dataset()\\nshow_sample_labels(3)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "\n",
    "# Configuration\n",
    "data_dir = \"/kaggle/input/dlp-object-detection-week-10/final_dlp_data/final_dlp_data\"\n",
    "train_images_dir = os.path.join(data_dir, \"train\", \"images\")\n",
    "train_labels_dir = os.path.join(data_dir, \"train\", \"labels\")\n",
    "\n",
    "class_names = {\n",
    "    0: \"aegypti\",\n",
    "    1: \"albopictus\",\n",
    "    2: \"anopheles\",\n",
    "    3: \"culex\",\n",
    "    4: \"culiseta\",\n",
    "    5: \"japonicus/koreicus\"\n",
    "}\n",
    "\n",
    "def analyze_dataset():\n",
    "    # 1. Class Distribution Analysis\n",
    "    class_counts = defaultdict(int)\n",
    "    label_files = [f for f in os.listdir(train_labels_dir) if f.endswith('.txt')]\n",
    "    \n",
    "    for lbl_file in label_files:\n",
    "        with open(os.path.join(train_labels_dir, lbl_file), 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                class_id = int(line.strip().split()[0])\n",
    "                class_counts[class_id] += 1\n",
    "\n",
    "    print(\"=== Class Distribution ===\")\n",
    "    for cls_id, count in sorted(class_counts.items()):\n",
    "        print(f\"{class_names[cls_id]:<20}: {count} samples ({count/len(label_files):.2f} per image)\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # 2. Image Size Statistics\n",
    "    image_sizes = []\n",
    "    for img_file in os.listdir(train_images_dir)[:1000]:  # Sample 1000 images\n",
    "        with Image.open(os.path.join(train_images_dir, img_file)) as img:\n",
    "            image_sizes.append(img.size)\n",
    "    \n",
    "    widths, heights = zip(*image_sizes)\n",
    "    print(\"=== Image Dimensions ===\")\n",
    "    print(f\"Average width: {np.mean(widths):.1f} ± {np.std(widths):.1f} px\")\n",
    "    print(f\"Average height: {np.mean(heights):.1f} ± {np.std(heights):.1f} px\")\n",
    "    print(f\"Min dimensions: {min(widths)}x{min(heights)}\")\n",
    "    print(f\"Max dimensions: {max(widths)}x{max(heights)}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # 3. Aspect Ratio Analysis\n",
    "    aspect_ratios = [w/h for w, h in image_sizes]\n",
    "    print(\"=== Aspect Ratios ===\")\n",
    "    print(f\"Mean aspect ratio: {np.mean(aspect_ratios):.2f} ± {np.std(aspect_ratios):.2f}\")\n",
    "    print(f\"Most common ratio: {max(set(aspect_ratios), key=aspect_ratios.count):.2f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # 4. Bounding Box Statistics\n",
    "    bbox_sizes = []\n",
    "    per_class_bbox = defaultdict(list)\n",
    "    \n",
    "    for lbl_file in label_files[:1000]:  # Sample 1000 labels\n",
    "        with open(os.path.join(train_labels_dir, lbl_file), 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                parts = line.strip().split()\n",
    "                cls_id = int(parts[0])\n",
    "                width = float(parts[3])\n",
    "                height = float(parts[4])\n",
    "                bbox_sizes.append((width, height))\n",
    "                per_class_bbox[cls_id].append((width, height))\n",
    "\n",
    "    bbox_areas = [w*h for w, h in bbox_sizes]\n",
    "    print(\"=== Bounding Boxes ===\")\n",
    "    print(f\"Average box size: {np.mean(bbox_areas):.4f} ± {np.std(bbox_areas):.4f} (normalized area)\")\n",
    "    print(f\"Average boxes per image: {len(bbox_sizes)/1000:.2f}\")\n",
    "    print(\"\\nPer-class box sizes (normalized width/height):\")\n",
    "    for cls_id in sorted(per_class_bbox.keys()):\n",
    "        sizes = np.array(per_class_bbox[cls_id])\n",
    "        print(f\"{class_names[cls_id]:<20}: W={np.mean(sizes[:,0]):.3f} ± {np.std(sizes[:,0]):.3f}, \"\n",
    "              f\"H={np.mean(sizes[:,1]):.3f} ± {np.std(sizes[:,1]):.3f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # 5. Data Quality Report\n",
    "    missing_labels = 0\n",
    "    for img_file in os.listdir(train_images_dir):\n",
    "        lbl_file = os.path.splitext(img_file)[0] + '.txt'\n",
    "        if not os.path.exists(os.path.join(train_labels_dir, lbl_file)):\n",
    "            missing_labels += 1\n",
    "\n",
    "    empty_labels = 0\n",
    "    for lbl_file in os.listdir(train_labels_dir):\n",
    "        if os.path.getsize(os.path.join(train_labels_dir, lbl_file)) == 0:\n",
    "            empty_labels += 1\n",
    "\n",
    "    print(\"=== Data Quality ===\")\n",
    "    print(f\"Total training images: {len(os.listdir(train_images_dir))}\")\n",
    "    print(f\"Total label files: {len(label_files)}\")\n",
    "    print(f\"Missing labels: {missing_labels}\")\n",
    "    print(f\"Empty label files: {empty_labels}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # 6. Channel Analysis\n",
    "    channels = defaultdict(int)\n",
    "    for img_file in os.listdir(train_images_dir)[:1000]:\n",
    "        with Image.open(os.path.join(train_images_dir, img_file)) as img:\n",
    "            channels[len(img.getbands())] += 1\n",
    "\n",
    "    print(\"=== Color Channels ===\")\n",
    "    for ch, count in channels.items():\n",
    "        print(f\"{ch} channels: {count} images\")\n",
    "\n",
    "def show_sample_labels(n=3):\n",
    "    print(\"\\n=== Sample Labels ===\")\n",
    "    for img_file in os.listdir(train_images_dir)[:n]:\n",
    "        lbl_file = os.path.splitext(img_file)[0] + '.txt'\n",
    "        lbl_path = os.path.join(train_labels_dir, lbl_file)\n",
    "        if os.path.exists(lbl_path):\n",
    "            with open(lbl_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                print(f\"\\nImage: {img_file}\")\n",
    "                for line in lines:\n",
    "                    parts = line.strip().split()\n",
    "                    cls_id = int(parts[0])\n",
    "                    print(f\"- {class_names[cls_id]}: \"\n",
    "                          f\"x={float(parts[1]):.3f}, y={float(parts[2]):.3f}, \"\n",
    "                          f\"w={float(parts[3]):.3f}, h={float(parts[4]):.3f}\")\n",
    "\n",
    "# Execute analysis\n",
    "analyze_dataset()\n",
    "show_sample_labels(3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "570b3785",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T18:39:23.254668Z",
     "iopub.status.busy": "2025-03-23T18:39:23.254473Z",
     "iopub.status.idle": "2025-03-23T18:39:29.851873Z",
     "shell.execute_reply": "2025-03-23T18:39:29.851029Z"
    },
    "papermill": {
     "duration": 6.603139,
     "end_time": "2025-03-23T18:39:29.853501",
     "exception": false,
     "start_time": "2025-03-23T18:39:23.250362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ultralytics\r\n",
      "  Downloading ultralytics-8.3.94-py3-none-any.whl.metadata (35 kB)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\r\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.5)\r\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\r\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (6.0.2)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\r\n",
      "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\r\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\r\n",
      "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\r\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\r\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\r\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\r\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\r\n",
      "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.12.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\r\n",
      "Downloading ultralytics-8.3.94-py3-none-any.whl (949 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m949.8/949.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\r\n",
      "Installing collected packages: ultralytics-thop, ultralytics\r\n",
      "Successfully installed ultralytics-8.3.94 ultralytics-thop-2.0.14\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics numpy pandas matplotlib pillow opencv-python pyyaml tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7625c76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T18:39:29.864611Z",
     "iopub.status.busy": "2025-03-23T18:39:29.864364Z",
     "iopub.status.idle": "2025-03-23T18:39:29.875077Z",
     "shell.execute_reply": "2025-03-23T18:39:29.874327Z"
    },
    "papermill": {
     "duration": 0.017426,
     "end_time": "2025-03-23T18:39:29.876190",
     "exception": false,
     "start_time": "2025-03-23T18:39:29.858764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Import necessary libraries\\nimport os\\nimport pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom PIL import Image, ImageDraw\\nfrom tqdm.notebook import tqdm\\nfrom ultralytics import YOLO\\nimport locale\\nimport csv\\nfrom pathlib import Path\\nimport yaml\\nimport torch\\n\\n# Set UTF-8 encoding\\ndef getpreferredencoding(do_setlocale = True):\\n    return \"UTF-8\"\\nlocale.getpreferredencoding = getpreferredencoding\\n\\n# Define paths\\nBASE_DIR = \\'/kaggle/input/dlp-object-detection-week-10/final_dlp_data/final_dlp_data\\'\\nTRAIN_IMAGES_DIR = os.path.join(BASE_DIR, \\'train/images\\')\\nTRAIN_LABELS_DIR = os.path.join(BASE_DIR, \\'train/labels\\')\\nTEST_IMAGES_DIR = os.path.join(BASE_DIR, \\'test/images\\')\\nOUTPUT_DIR = \"/kaggle/working/outputs\"\\n\\n# Create necessary directories\\nos.makedirs(OUTPUT_DIR, exist_ok=True)\\n\\n# Define class names and mapping\\nclass_names = {\\n    0: \"aegypti\",\\n    1: \"albopictus\",\\n    2: \"anopheles\", \\n    3: \"culex\",\\n    4: \"culiseta\",\\n    5: \"japonicus/koreicus\"\\n}\\n\\n# Function to analyze dataset\\ndef analyze_dataset():\\n    print(\"Analyzing dataset...\")\\n    \\n    # Count images\\n    train_images = len(os.listdir(TRAIN_IMAGES_DIR))\\n    test_images = len(os.listdir(TEST_IMAGES_DIR))\\n    \\n    print(f\"Training images: {train_images}\")\\n    print(f\"Testing images: {test_images}\")\\n    \\n    # Count labels per class\\n    class_counts = {name: 0 for name in class_names.values()}\\n    \\n    for label_file in os.listdir(TRAIN_LABELS_DIR):\\n        with open(os.path.join(TRAIN_LABELS_DIR, label_file), \\'r\\') as f:\\n            lines = f.readlines()\\n            for line in lines:\\n                if line.strip():\\n                    class_id = int(line.split()[0])\\n                    class_counts[class_names[class_id]] += 1\\n    \\n    print(\"\\nClass distribution:\")\\n    for class_name, count in class_counts.items():\\n        print(f\"{class_name}: {count} samples ({count/train_images:.2f} per image)\")\\n\\n# Function to prepare dataset with YAML file\\ndef prepare_dataset():\\n    print(\"Preparing dataset structure...\")\\n    \\n    # Create a YAML file to define the dataset structure\\n    dataset_content = {\\n        \\'train\\': TRAIN_IMAGES_DIR,\\n        \\'val\\': TRAIN_IMAGES_DIR,  # Using same for validation in this case\\n        \\'nc\\': 6,\\n        \\'names\\': class_names\\n    }\\n    \\n    # Create YAML file\\n    yaml_path = os.path.join(OUTPUT_DIR, \\'dataset.yaml\\')\\n    with open(yaml_path, \\'w\\') as f:\\n        yaml.dump(dataset_content, f, default_flow_style=False)\\n    \\n    print(f\"Created dataset configuration file at {yaml_path}\")\\n    \\n    return yaml_path\\n\\n# Function to train model\\ndef train_model(epochs=50, img_size=640, batch_size=16, patience=10):\\n    print(f\"Training YOLOv8 model for {epochs} epochs...\")\\n    \\n    # Get dataset configuration file\\n    yaml_path = prepare_dataset()\\n    \\n    # Load a pre-trained YOLOv8 model\\n    model = YOLO(\\'yolov8n.pt\\')  # Using YOLOv8 nano version for faster training\\n    \\n    # Train the model using the YAML file\\n    results = model.train(\\n        data=yaml_path,  # Use YAML file path instead of dictionary\\n        epochs=epochs,\\n        imgsz=img_size,\\n        batch=batch_size,\\n        patience=patience,\\n        verbose=True,\\n        device=\\'0\\' if torch.cuda.is_available() else \\'cpu\\',  # Use GPU if available\\n        project=OUTPUT_DIR,\\n        name=\\'train\\',\\n        pretrained=True,\\n        optimizer=\\'AdamW\\',\\n        lr0=1e-3,\\n        lrf=1e-4,\\n        momentum=0.937,\\n        weight_decay=5e-4,\\n        warmup_epochs=3.0,\\n        warmup_momentum=0.8,\\n        warmup_bias_lr=0.1,\\n        box=7.5,\\n        cls=0.5,\\n        dfl=1.5,\\n        label_smoothing=0.0,\\n        nbs=64,\\n        hsv_h=0.015,\\n        hsv_s=0.7,\\n        hsv_v=0.4,\\n        translate=0.1,\\n        scale=0.5,\\n        fliplr=0.5,\\n        mosaic=1.0,\\n        augment=True,\\n        close_mosaic=10,\\n        perspective=0.0,\\n        rect=False,\\n        mixup=0.0,\\n        copy_paste=0.0\\n    )\\n    \\n    # Save the trained weights\\n    best_model_path = os.path.join(OUTPUT_DIR, \\'train\\', \\'weights\\', \\'best.pt\\')\\n    print(f\"Best model saved at: {best_model_path}\")\\n    \\n    # Plot training results\\n    try:\\n        plt.figure(figsize=(12, 8))\\n        \\n        # Plot training loss\\n        plt.subplot(2, 2, 1)\\n        plt.plot(results.results_dict[\\'train/box_loss\\'], label=\\'train\\')\\n        plt.plot(results.results_dict[\\'val/box_loss\\'], label=\\'val\\')\\n        plt.title(\\'Box Loss\\')\\n        plt.xlabel(\\'Epoch\\')\\n        plt.ylabel(\\'Loss\\')\\n        plt.legend()\\n        \\n        # Plot classification loss\\n        plt.subplot(2, 2, 2)\\n        plt.plot(results.results_dict[\\'train/cls_loss\\'], label=\\'train\\')\\n        plt.plot(results.results_dict[\\'val/cls_loss\\'], label=\\'val\\')\\n        plt.title(\\'Classification Loss\\')\\n        plt.xlabel(\\'Epoch\\')\\n        plt.ylabel(\\'Loss\\')\\n        plt.legend()\\n        \\n        # Plot mAP@50\\n        plt.subplot(2, 2, 3)\\n        plt.plot(results.results_dict[\\'metrics/mAP50(B)\\'], label=\\'mAP@0.5\\')\\n        plt.title(\\'mAP@0.5\\')\\n        plt.xlabel(\\'Epoch\\')\\n        plt.ylabel(\\'mAP\\')\\n        plt.legend()\\n        \\n        # Plot mAP@50-95\\n        plt.subplot(2, 2, 4)\\n        plt.plot(results.results_dict[\\'metrics/mAP50-95(B)\\'], label=\\'mAP@0.5:0.95\\')\\n        plt.title(\\'mAP@0.5:0.95\\')\\n        plt.xlabel(\\'Epoch\\')\\n        plt.ylabel(\\'mAP\\')\\n        plt.legend()\\n        \\n        plt.tight_layout()\\n        plt.savefig(os.path.join(OUTPUT_DIR, \\'training_results.png\\'))\\n        plt.show()\\n    except Exception as e:\\n        print(f\"Error plotting results: {e}\")\\n    \\n    return best_model_path\\n\\n# Function to generate predictions and create submission file\\ndef generate_predictions(model_path):\\n    print(\"Generating predictions on test set...\")\\n    \\n    # Load the trained model\\n    model = YOLO(model_path)\\n    \\n    # Create a list to store predictions\\n    predictions = []\\n    \\n    # Process each test image\\n    test_images = sorted(os.listdir(TEST_IMAGES_DIR))\\n    for idx, img_name in enumerate(tqdm(test_images)):\\n        # Use the index directly as the ID (starting from 0)\\n        current_id = idx\\n        \\n        img_path = os.path.join(TEST_IMAGES_DIR, img_name)\\n        \\n        # Run prediction\\n        results = model.predict(source=img_path, conf=0.25, iou=0.45, save=False, verbose=False)\\n        \\n        # Check if any detections were found\\n        if len(results[0].boxes) > 0:\\n            # Get the highest confidence detection\\n            conf_values = results[0].boxes.conf.cpu().numpy()\\n            best_idx = np.argmax(conf_values)\\n            \\n            box = results[0].boxes.xyxy[best_idx]\\n            conf = results[0].boxes.conf[best_idx]\\n            cls = results[0].boxes.cls[best_idx]\\n            \\n            x1, y1, x2, y2 = box.cpu().numpy()\\n            cls_id = int(cls.item())\\n            conf_val = float(conf.item())\\n            \\n            # Calculate center, width, height in normalized coordinates\\n            img = Image.open(img_path)\\n            img_width, img_height = img.size\\n            \\n            xcenter = ((x1 + x2) / 2) / img_width\\n            ycenter = ((y1 + y2) / 2) / img_height\\n            width = (x2 - x1) / img_width\\n            height = (y2 - y1) / img_height\\n            \\n            # Add prediction to list with the current ID\\n            predictions.append({\\n                \\'id\\': current_id,\\n                \\'ImageID\\': img_name,\\n                \\'LabelName\\': class_names[cls_id],\\n                \\'Conf\\': conf_val,\\n                \\'xcenter\\': xcenter,\\n                \\'ycenter\\': ycenter,\\n                \\'bbx_width\\': width,\\n                \\'bbx_height\\': height\\n            })\\n        else:\\n            # If no detection, use default prediction with the current ID\\n            predictions.append({\\n                \\'id\\': current_id,\\n                \\'ImageID\\': img_name,\\n                \\'LabelName\\': \\'albopictus\\',  # Default class based on distribution\\n                \\'Conf\\': 0.5,  # Default confidence\\n                \\'xcenter\\': 0.5,  # Center of image\\n                \\'ycenter\\': 0.5,\\n                \\'bbx_width\\': 0.3,  # Average box dimensions\\n                \\'bbx_height\\': 0.3\\n            })\\n    \\n    # Create submission dataframe and save to CSV\\n    submission_df = pd.DataFrame(predictions)\\n    submission_path = os.path.join(OUTPUT_DIR, \\'submission.csv\\')\\n    submission_df.to_csv(submission_path, index=False)\\n    \\n    print(f\"Submission file saved to {submission_path}\")\\n    print(f\"Number of test images: {len(test_images)}\")\\n    print(f\"Number of predictions: {len(predictions)}\")\\n    return submission_path\\n\\n\\n# Function to visualize sample predictions\\ndef visualize_predictions(model_path, num_samples=5):\\n    print(f\"Visualizing {num_samples} sample predictions...\")\\n    \\n    # Load the trained model\\n    model = YOLO(model_path)\\n    \\n    # Sample random test images\\n    test_images = os.listdir(TEST_IMAGES_DIR)\\n    sample_images = np.random.choice(test_images, min(num_samples, len(test_images)), replace=False)\\n    \\n    # Display predictions\\n    plt.figure(figsize=(15, 20))\\n    for i, img_name in enumerate(sample_images):\\n        img_path = os.path.join(TEST_IMAGES_DIR, img_name)\\n        \\n        # Run prediction\\n        results = model.predict(source=img_path, conf=0.25, save=False, verbose=False)\\n        \\n        # Load the image\\n        img = Image.open(img_path)\\n        draw = ImageDraw.Draw(img)\\n        \\n        # Draw bounding boxes\\n        if len(results[0].boxes) > 0:\\n            for box, conf, cls in zip(results[0].boxes.xyxy, results[0].boxes.conf, results[0].boxes.cls):\\n                x1, y1, x2, y2 = box.cpu().numpy()\\n                cls_id = int(cls.item())\\n                conf_val = float(conf.item())\\n                \\n                # Draw rectangle\\n                draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=3)\\n                \\n                # Add label\\n                label = f\"{class_names[cls_id]} ({conf_val:.2f})\"\\n                draw.text((x1, y1 - 10), label, fill=\"red\")\\n        \\n        # Display the image\\n        plt.subplot(num_samples, 1, i + 1)\\n        plt.imshow(np.array(img))\\n        plt.title(f\"Image: {img_name}\")\\n        plt.axis(\\'off\\')\\n    \\n    plt.tight_layout()\\n    plt.savefig(os.path.join(OUTPUT_DIR, \\'sample_predictions.png\\'))\\n    plt.show()\\n\\n# Function to improve model for low light conditions\\ndef enhance_model_for_low_light(epochs=20, img_size=640):\\n    \"\"\"\\n    Enhance model specifically for low light conditions:\\n    1. Apply data augmentation tailored for low light\\n    2. Fine-tune a pre-trained model with focus on low light performance\\n    \"\"\"\\n    print(\"Enhancing model for low light conditions...\")\\n    \\n    # Load a pre-trained YOLOv8 model\\n    model = YOLO(\\'yolov8n.pt\\')\\n    \\n    # Create enhanced dataset configuration YAML\\n    enhanced_dataset_config = {\\n        \\'train\\': TRAIN_IMAGES_DIR,\\n        \\'val\\': TRAIN_IMAGES_DIR,\\n        \\'nc\\': 6,\\n        \\'names\\': class_names\\n    }\\n    \\n    # Create YAML file for enhanced model\\n    enhanced_yaml_path = os.path.join(OUTPUT_DIR, \\'enhanced_dataset.yaml\\')\\n    with open(enhanced_yaml_path, \\'w\\') as f:\\n        yaml.dump(enhanced_dataset_config, f, default_flow_style=False)\\n    \\n    # Enhanced training with focus on low light conditions\\n    results = model.train(\\n        data=enhanced_yaml_path,  # Use YAML file path\\n        epochs=epochs,\\n        imgsz=img_size,\\n        batch=16,\\n        patience=5,\\n        verbose=True,\\n        device=\\'0\\' if torch.cuda.is_available() else \\'cpu\\',  # Use GPU if available\\n        project=OUTPUT_DIR,\\n        name=\\'lowlight_train\\',\\n        pretrained=True,\\n        optimizer=\\'AdamW\\',\\n        lr0=5e-4,  # Lower learning rate for fine-tuning\\n        lrf=1e-5,\\n        momentum=0.937,\\n        weight_decay=5e-4,\\n        warmup_epochs=3.0,\\n        warmup_momentum=0.8,\\n        warmup_bias_lr=0.1,\\n        box=7.5,\\n        cls=0.5,\\n        dfl=1.5,\\n        label_smoothing=0.0,\\n        nbs=64,\\n        # Enhanced data augmentation for low light conditions\\n        hsv_h=0.015,\\n        hsv_s=0.7,  # Higher saturation augmentation\\n        hsv_v=0.8,  # Higher brightness augmentation\\n        translate=0.1,\\n        scale=0.5,\\n        fliplr=0.5,\\n        mosaic=1.0,\\n        augment=True,\\n        close_mosaic=10,\\n        perspective=0.0,\\n        rect=False,\\n        mixup=0.1,  # Add some mixup\\n        copy_paste=0.1  # Add some copy-paste augmentation\\n    )\\n    \\n    # Save the enhanced model\\n    enhanced_model_path = os.path.join(OUTPUT_DIR, \\'lowlight_train\\', \\'weights\\', \\'best.pt\\')\\n    print(f\"Enhanced low-light model saved at: {enhanced_model_path}\")\\n    \\n    return enhanced_model_path\\n\\n# Function to apply low-light preprocessing to images\\ndef apply_low_light_preprocessing(image_path):\\n    \"\"\"Apply preprocessing techniques to enhance low-light images\"\"\"\\n    try:\\n        from PIL import ImageEnhance\\n        \\n        # Open the image\\n        img = Image.open(image_path)\\n        \\n        # Apply brightness enhancement\\n        enhancer = ImageEnhance.Brightness(img)\\n        img_enhanced = enhancer.enhance(1.5)  # Increase brightness by 50%\\n        \\n        # Apply contrast enhancement\\n        enhancer = ImageEnhance.Contrast(img_enhanced)\\n        img_enhanced = enhancer.enhance(1.3)  # Increase contrast by 30%\\n        \\n        # Return enhanced image\\n        return img_enhanced\\n    except Exception as e:\\n        print(f\"Error preprocessing image: {e}\")\\n        return Image.open(image_path)  # Return original if enhancement fails\\n\\n# Function to create an ensemble model for better performance\\ndef create_ensemble_model(base_model_path, enhanced_model_path):\\n    \"\"\"Create an ensemble prediction function that combines outputs from both models\"\"\"\\n    print(\"Creating ensemble prediction model...\")\\n    \\n    # Define ensemble prediction function\\n    def ensemble_predict(image_path, conf_threshold=0.25, iou_threshold=0.45):\\n        # Load both models\\n        base_model = YOLO(base_model_path)\\n        enhanced_model = YOLO(enhanced_model_path)\\n        \\n        # Get predictions from base model\\n        base_results = base_model.predict(source=image_path, conf=conf_threshold, iou=iou_threshold, save=False, verbose=False)\\n        \\n        # Apply low-light preprocessing and get predictions from enhanced model\\n        enhanced_image = apply_low_light_preprocessing(image_path)\\n        # Save the enhanced image to a temporary file\\n        temp_path = os.path.join(os.path.dirname(image_path), \"temp_enhanced.jpg\")\\n        enhanced_image.save(temp_path)\\n        enhanced_results = enhanced_model.predict(source=temp_path, conf=conf_threshold, iou=iou_threshold, save=False, verbose=False)\\n        \\n        # Remove temporary file\\n        if os.path.exists(temp_path):\\n            os.remove(temp_path)\\n        \\n        # Combine results (simple approach: take all detections and apply NMS)\\n        all_boxes = []\\n        all_scores = []\\n        all_classes = []\\n        \\n        # Process base model results\\n        if len(base_results[0].boxes) > 0:\\n            for box, conf, cls in zip(base_results[0].boxes.xyxy, base_results[0].boxes.conf, base_results[0].boxes.cls):\\n                all_boxes.append(box.cpu().numpy())\\n                all_scores.append(float(conf.item()))\\n                all_classes.append(int(cls.item()))\\n        \\n        # Process enhanced model results\\n        if len(enhanced_results[0].boxes) > 0:\\n            for box, conf, cls in zip(enhanced_results[0].boxes.xyxy, enhanced_results[0].boxes.conf, enhanced_results[0].boxes.cls):\\n                all_boxes.append(box.cpu().numpy())\\n                all_scores.append(float(conf.item()))\\n                all_classes.append(int(cls.item()))\\n        \\n        # Apply NMS to combined results (if there are any detections)\\n        if len(all_boxes) > 0:\\n            import torch\\n            from torchvision.ops import nms\\n            \\n            # Convert to tensors\\n            boxes_tensor = torch.tensor(np.array(all_boxes))\\n            scores_tensor = torch.tensor(np.array(all_scores))\\n            \\n            # Apply NMS\\n            keep_indices = nms(boxes_tensor, scores_tensor, iou_threshold)\\n            \\n            # Filter results\\n            filtered_boxes = [all_boxes[i.item()] for i in keep_indices]\\n            filtered_scores = [all_scores[i.item()] for i in keep_indices]\\n            filtered_classes = [all_classes[i.item()] for i in keep_indices]\\n            \\n            return filtered_boxes, filtered_scores, filtered_classes\\n        else:\\n            return [], [], []\\n    \\n    # Return the ensemble prediction function\\n    return ensemble_predict\\n\\n# Main execution function\\ndef main():\\n    # Step 1: Analyze the dataset\\n    analyze_dataset()\\n    \\n    # Step 2: Train the initial model with fewer epochs for testing\\n    model_path = train_model(epochs=5)  # Reduced for testing, change back to 50 for full training\\n    \\n    # Step 3: Enhance model for low light conditions\\n    enhanced_model_path = enhance_model_for_low_light(epochs=5)  # Reduced for testing\\n    \\n    # Step 4: Generate predictions and create submission file using the enhanced model\\n    submission_path = generate_predictions(enhanced_model_path)\\n    \\n    # Step 5: Visualize sample predictions\\n    visualize_predictions(enhanced_model_path, num_samples=3)  # Reduced sample count for faster execution\\n    \\n    print(f\"Completed! Final submission file: {submission_path}\")\\n\\n# Run the main function\\nif __name__ == \"__main__\":\\n    main()\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "from tqdm.notebook import tqdm\n",
    "from ultralytics import YOLO\n",
    "import locale\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import torch\n",
    "\n",
    "# Set UTF-8 encoding\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "locale.getpreferredencoding = getpreferredencoding\n",
    "\n",
    "# Define paths\n",
    "BASE_DIR = '/kaggle/input/dlp-object-detection-week-10/final_dlp_data/final_dlp_data'\n",
    "TRAIN_IMAGES_DIR = os.path.join(BASE_DIR, 'train/images')\n",
    "TRAIN_LABELS_DIR = os.path.join(BASE_DIR, 'train/labels')\n",
    "TEST_IMAGES_DIR = os.path.join(BASE_DIR, 'test/images')\n",
    "OUTPUT_DIR = \"/kaggle/working/outputs\"\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Define class names and mapping\n",
    "class_names = {\n",
    "    0: \"aegypti\",\n",
    "    1: \"albopictus\",\n",
    "    2: \"anopheles\", \n",
    "    3: \"culex\",\n",
    "    4: \"culiseta\",\n",
    "    5: \"japonicus/koreicus\"\n",
    "}\n",
    "\n",
    "# Function to analyze dataset\n",
    "def analyze_dataset():\n",
    "    print(\"Analyzing dataset...\")\n",
    "    \n",
    "    # Count images\n",
    "    train_images = len(os.listdir(TRAIN_IMAGES_DIR))\n",
    "    test_images = len(os.listdir(TEST_IMAGES_DIR))\n",
    "    \n",
    "    print(f\"Training images: {train_images}\")\n",
    "    print(f\"Testing images: {test_images}\")\n",
    "    \n",
    "    # Count labels per class\n",
    "    class_counts = {name: 0 for name in class_names.values()}\n",
    "    \n",
    "    for label_file in os.listdir(TRAIN_LABELS_DIR):\n",
    "        with open(os.path.join(TRAIN_LABELS_DIR, label_file), 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                if line.strip():\n",
    "                    class_id = int(line.split()[0])\n",
    "                    class_counts[class_names[class_id]] += 1\n",
    "    \n",
    "    print(\"\\nClass distribution:\")\n",
    "    for class_name, count in class_counts.items():\n",
    "        print(f\"{class_name}: {count} samples ({count/train_images:.2f} per image)\")\n",
    "\n",
    "# Function to prepare dataset with YAML file\n",
    "def prepare_dataset():\n",
    "    print(\"Preparing dataset structure...\")\n",
    "    \n",
    "    # Create a YAML file to define the dataset structure\n",
    "    dataset_content = {\n",
    "        'train': TRAIN_IMAGES_DIR,\n",
    "        'val': TRAIN_IMAGES_DIR,  # Using same for validation in this case\n",
    "        'nc': 6,\n",
    "        'names': class_names\n",
    "    }\n",
    "    \n",
    "    # Create YAML file\n",
    "    yaml_path = os.path.join(OUTPUT_DIR, 'dataset.yaml')\n",
    "    with open(yaml_path, 'w') as f:\n",
    "        yaml.dump(dataset_content, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"Created dataset configuration file at {yaml_path}\")\n",
    "    \n",
    "    return yaml_path\n",
    "\n",
    "# Function to train model\n",
    "def train_model(epochs=50, img_size=640, batch_size=16, patience=10):\n",
    "    print(f\"Training YOLOv8 model for {epochs} epochs...\")\n",
    "    \n",
    "    # Get dataset configuration file\n",
    "    yaml_path = prepare_dataset()\n",
    "    \n",
    "    # Load a pre-trained YOLOv8 model\n",
    "    model = YOLO('yolov8n.pt')  # Using YOLOv8 nano version for faster training\n",
    "    \n",
    "    # Train the model using the YAML file\n",
    "    results = model.train(\n",
    "        data=yaml_path,  # Use YAML file path instead of dictionary\n",
    "        epochs=epochs,\n",
    "        imgsz=img_size,\n",
    "        batch=batch_size,\n",
    "        patience=patience,\n",
    "        verbose=True,\n",
    "        device='0' if torch.cuda.is_available() else 'cpu',  # Use GPU if available\n",
    "        project=OUTPUT_DIR,\n",
    "        name='train',\n",
    "        pretrained=True,\n",
    "        optimizer='AdamW',\n",
    "        lr0=1e-3,\n",
    "        lrf=1e-4,\n",
    "        momentum=0.937,\n",
    "        weight_decay=5e-4,\n",
    "        warmup_epochs=3.0,\n",
    "        warmup_momentum=0.8,\n",
    "        warmup_bias_lr=0.1,\n",
    "        box=7.5,\n",
    "        cls=0.5,\n",
    "        dfl=1.5,\n",
    "        label_smoothing=0.0,\n",
    "        nbs=64,\n",
    "        hsv_h=0.015,\n",
    "        hsv_s=0.7,\n",
    "        hsv_v=0.4,\n",
    "        translate=0.1,\n",
    "        scale=0.5,\n",
    "        fliplr=0.5,\n",
    "        mosaic=1.0,\n",
    "        augment=True,\n",
    "        close_mosaic=10,\n",
    "        perspective=0.0,\n",
    "        rect=False,\n",
    "        mixup=0.0,\n",
    "        copy_paste=0.0\n",
    "    )\n",
    "    \n",
    "    # Save the trained weights\n",
    "    best_model_path = os.path.join(OUTPUT_DIR, 'train', 'weights', 'best.pt')\n",
    "    print(f\"Best model saved at: {best_model_path}\")\n",
    "    \n",
    "    # Plot training results\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot training loss\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(results.results_dict['train/box_loss'], label='train')\n",
    "        plt.plot(results.results_dict['val/box_loss'], label='val')\n",
    "        plt.title('Box Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot classification loss\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(results.results_dict['train/cls_loss'], label='train')\n",
    "        plt.plot(results.results_dict['val/cls_loss'], label='val')\n",
    "        plt.title('Classification Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot mAP@50\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(results.results_dict['metrics/mAP50(B)'], label='mAP@0.5')\n",
    "        plt.title('mAP@0.5')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('mAP')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot mAP@50-95\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(results.results_dict['metrics/mAP50-95(B)'], label='mAP@0.5:0.95')\n",
    "        plt.title('mAP@0.5:0.95')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('mAP')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, 'training_results.png'))\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting results: {e}\")\n",
    "    \n",
    "    return best_model_path\n",
    "\n",
    "# Function to generate predictions and create submission file\n",
    "def generate_predictions(model_path):\n",
    "    print(\"Generating predictions on test set...\")\n",
    "    \n",
    "    # Load the trained model\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    # Create a list to store predictions\n",
    "    predictions = []\n",
    "    \n",
    "    # Process each test image\n",
    "    test_images = sorted(os.listdir(TEST_IMAGES_DIR))\n",
    "    for idx, img_name in enumerate(tqdm(test_images)):\n",
    "        # Use the index directly as the ID (starting from 0)\n",
    "        current_id = idx\n",
    "        \n",
    "        img_path = os.path.join(TEST_IMAGES_DIR, img_name)\n",
    "        \n",
    "        # Run prediction\n",
    "        results = model.predict(source=img_path, conf=0.25, iou=0.45, save=False, verbose=False)\n",
    "        \n",
    "        # Check if any detections were found\n",
    "        if len(results[0].boxes) > 0:\n",
    "            # Get the highest confidence detection\n",
    "            conf_values = results[0].boxes.conf.cpu().numpy()\n",
    "            best_idx = np.argmax(conf_values)\n",
    "            \n",
    "            box = results[0].boxes.xyxy[best_idx]\n",
    "            conf = results[0].boxes.conf[best_idx]\n",
    "            cls = results[0].boxes.cls[best_idx]\n",
    "            \n",
    "            x1, y1, x2, y2 = box.cpu().numpy()\n",
    "            cls_id = int(cls.item())\n",
    "            conf_val = float(conf.item())\n",
    "            \n",
    "            # Calculate center, width, height in normalized coordinates\n",
    "            img = Image.open(img_path)\n",
    "            img_width, img_height = img.size\n",
    "            \n",
    "            xcenter = ((x1 + x2) / 2) / img_width\n",
    "            ycenter = ((y1 + y2) / 2) / img_height\n",
    "            width = (x2 - x1) / img_width\n",
    "            height = (y2 - y1) / img_height\n",
    "            \n",
    "            # Add prediction to list with the current ID\n",
    "            predictions.append({\n",
    "                'id': current_id,\n",
    "                'ImageID': img_name,\n",
    "                'LabelName': class_names[cls_id],\n",
    "                'Conf': conf_val,\n",
    "                'xcenter': xcenter,\n",
    "                'ycenter': ycenter,\n",
    "                'bbx_width': width,\n",
    "                'bbx_height': height\n",
    "            })\n",
    "        else:\n",
    "            # If no detection, use default prediction with the current ID\n",
    "            predictions.append({\n",
    "                'id': current_id,\n",
    "                'ImageID': img_name,\n",
    "                'LabelName': 'albopictus',  # Default class based on distribution\n",
    "                'Conf': 0.5,  # Default confidence\n",
    "                'xcenter': 0.5,  # Center of image\n",
    "                'ycenter': 0.5,\n",
    "                'bbx_width': 0.3,  # Average box dimensions\n",
    "                'bbx_height': 0.3\n",
    "            })\n",
    "    \n",
    "    # Create submission dataframe and save to CSV\n",
    "    submission_df = pd.DataFrame(predictions)\n",
    "    submission_path = os.path.join(OUTPUT_DIR, 'submission.csv')\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    \n",
    "    print(f\"Submission file saved to {submission_path}\")\n",
    "    print(f\"Number of test images: {len(test_images)}\")\n",
    "    print(f\"Number of predictions: {len(predictions)}\")\n",
    "    return submission_path\n",
    "\n",
    "\n",
    "# Function to visualize sample predictions\n",
    "def visualize_predictions(model_path, num_samples=5):\n",
    "    print(f\"Visualizing {num_samples} sample predictions...\")\n",
    "    \n",
    "    # Load the trained model\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    # Sample random test images\n",
    "    test_images = os.listdir(TEST_IMAGES_DIR)\n",
    "    sample_images = np.random.choice(test_images, min(num_samples, len(test_images)), replace=False)\n",
    "    \n",
    "    # Display predictions\n",
    "    plt.figure(figsize=(15, 20))\n",
    "    for i, img_name in enumerate(sample_images):\n",
    "        img_path = os.path.join(TEST_IMAGES_DIR, img_name)\n",
    "        \n",
    "        # Run prediction\n",
    "        results = model.predict(source=img_path, conf=0.25, save=False, verbose=False)\n",
    "        \n",
    "        # Load the image\n",
    "        img = Image.open(img_path)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        \n",
    "        # Draw bounding boxes\n",
    "        if len(results[0].boxes) > 0:\n",
    "            for box, conf, cls in zip(results[0].boxes.xyxy, results[0].boxes.conf, results[0].boxes.cls):\n",
    "                x1, y1, x2, y2 = box.cpu().numpy()\n",
    "                cls_id = int(cls.item())\n",
    "                conf_val = float(conf.item())\n",
    "                \n",
    "                # Draw rectangle\n",
    "                draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=3)\n",
    "                \n",
    "                # Add label\n",
    "                label = f\"{class_names[cls_id]} ({conf_val:.2f})\"\n",
    "                draw.text((x1, y1 - 10), label, fill=\"red\")\n",
    "        \n",
    "        # Display the image\n",
    "        plt.subplot(num_samples, 1, i + 1)\n",
    "        plt.imshow(np.array(img))\n",
    "        plt.title(f\"Image: {img_name}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'sample_predictions.png'))\n",
    "    plt.show()\n",
    "\n",
    "# Function to improve model for low light conditions\n",
    "def enhance_model_for_low_light(epochs=20, img_size=640):\n",
    "    \"\"\"\n",
    "    Enhance model specifically for low light conditions:\n",
    "    1. Apply data augmentation tailored for low light\n",
    "    2. Fine-tune a pre-trained model with focus on low light performance\n",
    "    \"\"\"\n",
    "    print(\"Enhancing model for low light conditions...\")\n",
    "    \n",
    "    # Load a pre-trained YOLOv8 model\n",
    "    model = YOLO('yolov8n.pt')\n",
    "    \n",
    "    # Create enhanced dataset configuration YAML\n",
    "    enhanced_dataset_config = {\n",
    "        'train': TRAIN_IMAGES_DIR,\n",
    "        'val': TRAIN_IMAGES_DIR,\n",
    "        'nc': 6,\n",
    "        'names': class_names\n",
    "    }\n",
    "    \n",
    "    # Create YAML file for enhanced model\n",
    "    enhanced_yaml_path = os.path.join(OUTPUT_DIR, 'enhanced_dataset.yaml')\n",
    "    with open(enhanced_yaml_path, 'w') as f:\n",
    "        yaml.dump(enhanced_dataset_config, f, default_flow_style=False)\n",
    "    \n",
    "    # Enhanced training with focus on low light conditions\n",
    "    results = model.train(\n",
    "        data=enhanced_yaml_path,  # Use YAML file path\n",
    "        epochs=epochs,\n",
    "        imgsz=img_size,\n",
    "        batch=16,\n",
    "        patience=5,\n",
    "        verbose=True,\n",
    "        device='0' if torch.cuda.is_available() else 'cpu',  # Use GPU if available\n",
    "        project=OUTPUT_DIR,\n",
    "        name='lowlight_train',\n",
    "        pretrained=True,\n",
    "        optimizer='AdamW',\n",
    "        lr0=5e-4,  # Lower learning rate for fine-tuning\n",
    "        lrf=1e-5,\n",
    "        momentum=0.937,\n",
    "        weight_decay=5e-4,\n",
    "        warmup_epochs=3.0,\n",
    "        warmup_momentum=0.8,\n",
    "        warmup_bias_lr=0.1,\n",
    "        box=7.5,\n",
    "        cls=0.5,\n",
    "        dfl=1.5,\n",
    "        label_smoothing=0.0,\n",
    "        nbs=64,\n",
    "        # Enhanced data augmentation for low light conditions\n",
    "        hsv_h=0.015,\n",
    "        hsv_s=0.7,  # Higher saturation augmentation\n",
    "        hsv_v=0.8,  # Higher brightness augmentation\n",
    "        translate=0.1,\n",
    "        scale=0.5,\n",
    "        fliplr=0.5,\n",
    "        mosaic=1.0,\n",
    "        augment=True,\n",
    "        close_mosaic=10,\n",
    "        perspective=0.0,\n",
    "        rect=False,\n",
    "        mixup=0.1,  # Add some mixup\n",
    "        copy_paste=0.1  # Add some copy-paste augmentation\n",
    "    )\n",
    "    \n",
    "    # Save the enhanced model\n",
    "    enhanced_model_path = os.path.join(OUTPUT_DIR, 'lowlight_train', 'weights', 'best.pt')\n",
    "    print(f\"Enhanced low-light model saved at: {enhanced_model_path}\")\n",
    "    \n",
    "    return enhanced_model_path\n",
    "\n",
    "# Function to apply low-light preprocessing to images\n",
    "def apply_low_light_preprocessing(image_path):\n",
    "    \"\"\"Apply preprocessing techniques to enhance low-light images\"\"\"\n",
    "    try:\n",
    "        from PIL import ImageEnhance\n",
    "        \n",
    "        # Open the image\n",
    "        img = Image.open(image_path)\n",
    "        \n",
    "        # Apply brightness enhancement\n",
    "        enhancer = ImageEnhance.Brightness(img)\n",
    "        img_enhanced = enhancer.enhance(1.5)  # Increase brightness by 50%\n",
    "        \n",
    "        # Apply contrast enhancement\n",
    "        enhancer = ImageEnhance.Contrast(img_enhanced)\n",
    "        img_enhanced = enhancer.enhance(1.3)  # Increase contrast by 30%\n",
    "        \n",
    "        # Return enhanced image\n",
    "        return img_enhanced\n",
    "    except Exception as e:\n",
    "        print(f\"Error preprocessing image: {e}\")\n",
    "        return Image.open(image_path)  # Return original if enhancement fails\n",
    "\n",
    "# Function to create an ensemble model for better performance\n",
    "def create_ensemble_model(base_model_path, enhanced_model_path):\n",
    "    \"\"\"Create an ensemble prediction function that combines outputs from both models\"\"\"\n",
    "    print(\"Creating ensemble prediction model...\")\n",
    "    \n",
    "    # Define ensemble prediction function\n",
    "    def ensemble_predict(image_path, conf_threshold=0.25, iou_threshold=0.45):\n",
    "        # Load both models\n",
    "        base_model = YOLO(base_model_path)\n",
    "        enhanced_model = YOLO(enhanced_model_path)\n",
    "        \n",
    "        # Get predictions from base model\n",
    "        base_results = base_model.predict(source=image_path, conf=conf_threshold, iou=iou_threshold, save=False, verbose=False)\n",
    "        \n",
    "        # Apply low-light preprocessing and get predictions from enhanced model\n",
    "        enhanced_image = apply_low_light_preprocessing(image_path)\n",
    "        # Save the enhanced image to a temporary file\n",
    "        temp_path = os.path.join(os.path.dirname(image_path), \"temp_enhanced.jpg\")\n",
    "        enhanced_image.save(temp_path)\n",
    "        enhanced_results = enhanced_model.predict(source=temp_path, conf=conf_threshold, iou=iou_threshold, save=False, verbose=False)\n",
    "        \n",
    "        # Remove temporary file\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "        \n",
    "        # Combine results (simple approach: take all detections and apply NMS)\n",
    "        all_boxes = []\n",
    "        all_scores = []\n",
    "        all_classes = []\n",
    "        \n",
    "        # Process base model results\n",
    "        if len(base_results[0].boxes) > 0:\n",
    "            for box, conf, cls in zip(base_results[0].boxes.xyxy, base_results[0].boxes.conf, base_results[0].boxes.cls):\n",
    "                all_boxes.append(box.cpu().numpy())\n",
    "                all_scores.append(float(conf.item()))\n",
    "                all_classes.append(int(cls.item()))\n",
    "        \n",
    "        # Process enhanced model results\n",
    "        if len(enhanced_results[0].boxes) > 0:\n",
    "            for box, conf, cls in zip(enhanced_results[0].boxes.xyxy, enhanced_results[0].boxes.conf, enhanced_results[0].boxes.cls):\n",
    "                all_boxes.append(box.cpu().numpy())\n",
    "                all_scores.append(float(conf.item()))\n",
    "                all_classes.append(int(cls.item()))\n",
    "        \n",
    "        # Apply NMS to combined results (if there are any detections)\n",
    "        if len(all_boxes) > 0:\n",
    "            import torch\n",
    "            from torchvision.ops import nms\n",
    "            \n",
    "            # Convert to tensors\n",
    "            boxes_tensor = torch.tensor(np.array(all_boxes))\n",
    "            scores_tensor = torch.tensor(np.array(all_scores))\n",
    "            \n",
    "            # Apply NMS\n",
    "            keep_indices = nms(boxes_tensor, scores_tensor, iou_threshold)\n",
    "            \n",
    "            # Filter results\n",
    "            filtered_boxes = [all_boxes[i.item()] for i in keep_indices]\n",
    "            filtered_scores = [all_scores[i.item()] for i in keep_indices]\n",
    "            filtered_classes = [all_classes[i.item()] for i in keep_indices]\n",
    "            \n",
    "            return filtered_boxes, filtered_scores, filtered_classes\n",
    "        else:\n",
    "            return [], [], []\n",
    "    \n",
    "    # Return the ensemble prediction function\n",
    "    return ensemble_predict\n",
    "\n",
    "# Main execution function\n",
    "def main():\n",
    "    # Step 1: Analyze the dataset\n",
    "    analyze_dataset()\n",
    "    \n",
    "    # Step 2: Train the initial model with fewer epochs for testing\n",
    "    model_path = train_model(epochs=5)  # Reduced for testing, change back to 50 for full training\n",
    "    \n",
    "    # Step 3: Enhance model for low light conditions\n",
    "    enhanced_model_path = enhance_model_for_low_light(epochs=5)  # Reduced for testing\n",
    "    \n",
    "    # Step 4: Generate predictions and create submission file using the enhanced model\n",
    "    submission_path = generate_predictions(enhanced_model_path)\n",
    "    \n",
    "    # Step 5: Visualize sample predictions\n",
    "    visualize_predictions(enhanced_model_path, num_samples=3)  # Reduced sample count for faster execution\n",
    "    \n",
    "    print(f\"Completed! Final submission file: {submission_path}\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "618b0393",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T18:39:29.887037Z",
     "iopub.status.busy": "2025-03-23T18:39:29.886825Z",
     "iopub.status.idle": "2025-03-23T18:39:29.898538Z",
     "shell.execute_reply": "2025-03-23T18:39:29.897804Z"
    },
    "papermill": {
     "duration": 0.018657,
     "end_time": "2025-03-23T18:39:29.899695",
     "exception": false,
     "start_time": "2025-03-23T18:39:29.881038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nimport pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom PIL import Image, ImageDraw\\nimport random\\nfrom pathlib import Path\\nimport shutil\\nimport cv2\\n\\n# Define paths (modify these to match your environment)\\nBASE_DIR = \\'/kaggle/input/dlp-object-detection-week-10/final_dlp_data/final_dlp_data\\'\\nTRAIN_IMAGES_DIR = os.path.join(BASE_DIR, \\'train/images\\')\\nTRAIN_LABELS_DIR = os.path.join(BASE_DIR, \\'train/labels\\')\\nTEST_IMAGES_DIR = os.path.join(BASE_DIR, \\'test/images\\')\\nOUTPUT_DIR = \"/kaggle/working/analysis\"\\n\\n# Create output directory\\nos.makedirs(OUTPUT_DIR, exist_ok=True)\\n\\n# Define class names\\nclass_names = {\\n    0: \"aegypti\",\\n    1: \"albopictus\",\\n    2: \"anopheles\", \\n    3: \"culex\",\\n    4: \"culiseta\",\\n    5: \"japonicus/koreicus\"\\n}\\n\\ndef analyze_dataset_statistics():\\n    \"\"\"Analyze and visualize dataset statistics\"\"\"\\n    print(\"Analyzing dataset statistics...\")\\n    \\n    # Count images\\n    train_images = len(os.listdir(TRAIN_IMAGES_DIR))\\n    test_images = len(os.listdir(TEST_IMAGES_DIR))\\n    \\n    print(f\"Training images: {train_images}\")\\n    print(f\"Testing images: {test_images}\")\\n    \\n    # Count labels per class\\n    class_counts = {name: 0 for name in class_names.values()}\\n    image_sizes = []\\n    box_sizes = []\\n    class_box_sizes = {name: [] for name in class_names.values()}\\n    brightness_values = []\\n    contrast_values = []\\n    \\n    # Sample images for visualization\\n    sample_indices = random.sample(range(train_images), min(50, train_images))\\n    sample_images = []\\n    \\n    # Process each label file\\n    all_files = os.listdir(TRAIN_LABELS_DIR)\\n    for i, label_file in enumerate(all_files):\\n        img_file = label_file.replace(\\'.txt\\', \\'.jpeg\\')\\n        img_path = os.path.join(TRAIN_IMAGES_DIR, img_file)\\n        \\n        # Check if image exists\\n        if not os.path.exists(img_path):\\n            img_file = label_file.replace(\\'.txt\\', \\'.jpg\\')\\n            img_path = os.path.join(TRAIN_IMAGES_DIR, img_file)\\n            if not os.path.exists(img_path):\\n                print(f\"Warning: Image file not found for {label_file}\")\\n                continue\\n        \\n        # Analyze image properties\\n        try:\\n            img = Image.open(img_path)\\n            img_width, img_height = img.size\\n            image_sizes.append((img_width, img_height))\\n            \\n            # Convert to numpy array for brightness/contrast analysis\\n            img_np = np.array(img)\\n            if len(img_np.shape) == 3:  # Color image\\n                # Convert to grayscale for brightness analysis\\n                gray_img = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\\n                brightness_values.append(np.mean(gray_img))\\n                contrast_values.append(np.std(gray_img))\\n        except Exception as e:\\n            print(f\"Error processing image {img_path}: {e}\")\\n            continue\\n        \\n        # Read label file\\n        try:\\n            with open(os.path.join(TRAIN_LABELS_DIR, label_file), \\'r\\') as f:\\n                lines = f.readlines()\\n                \\n                for line in lines:\\n                    if line.strip():\\n                        parts = line.strip().split()\\n                        if len(parts) >= 5:\\n                            class_id = int(parts[0])\\n                            x_center = float(parts[1])\\n                            y_center = float(parts[2])\\n                            width = float(parts[3])\\n                            height = float(parts[4])\\n                            \\n                            class_name = class_names[class_id]\\n                            class_counts[class_name] += 1\\n                            \\n                            # Calculate box area (normalized)\\n                            box_area = width * height\\n                            box_sizes.append(box_area)\\n                            class_box_sizes[class_name].append((width, height))\\n                            \\n                            # If this is a sample image, store info for visualization\\n                            if i in sample_indices:\\n                                # Create visualization of the bounding box\\n                                sample_images.append({\\n                                    \\'img_path\\': img_path,\\n                                    \\'class_name\\': class_name,\\n                                    \\'box\\': (x_center, y_center, width, height)\\n                                })\\n        except Exception as e:\\n            print(f\"Error processing label {label_file}: {e}\")\\n            continue\\n    \\n    # Calculate statistics\\n    image_widths, image_heights = zip(*image_sizes)\\n    avg_width = sum(image_widths) / len(image_widths)\\n    avg_height = sum(image_heights) / len(image_heights)\\n    std_width = np.std(image_widths)\\n    std_height = np.std(image_heights)\\n    \\n    avg_box_size = sum(box_sizes) / len(box_sizes) if box_sizes else 0\\n    std_box_size = np.std(box_sizes) if box_sizes else 0\\n    \\n    # Calculate per-class box sizes\\n    class_box_stats = {}\\n    for class_name, boxes in class_box_sizes.items():\\n        if boxes:\\n            widths, heights = zip(*boxes)\\n            class_box_stats[class_name] = {\\n                \\'avg_width\\': sum(widths) / len(widths),\\n                \\'std_width\\': np.std(widths),\\n                \\'avg_height\\': sum(heights) / len(heights),\\n                \\'std_height\\': np.std(heights)\\n            }\\n    \\n    # Print statistics\\n    print(\"\\n=== Dataset Statistics ===\")\\n    print(f\"Training images: {train_images}\")\\n    print(f\"Test images: {test_images}\")\\n    \\n    print(\"\\n=== Class Distribution ===\")\\n    total_boxes = sum(class_counts.values())\\n    for class_name, count in class_counts.items():\\n        print(f\"{class_name}: {count} samples ({count/train_images:.2f} per image, {count/total_boxes*100:.1f}% of total)\")\\n    \\n    print(\"\\n=== Image Dimensions ===\")\\n    print(f\"Average width: {avg_width:.1f} ± {std_width:.1f} px\")\\n    print(f\"Average height: {avg_height:.1f} ± {std_height:.1f} px\")\\n    print(f\"Min width: {min(image_widths)} px\")\\n    print(f\"Max width: {max(image_widths)} px\")\\n    print(f\"Min height: {min(image_heights)} px\")\\n    print(f\"Max height: {max(image_heights)} px\")\\n    \\n    print(\"\\n=== Bounding Boxes ===\")\\n    print(f\"Average box size: {avg_box_size:.4f} ± {std_box_size:.4f} (normalized area)\")\\n    print(\"\\nPer-class box sizes (normalized width/height):\")\\n    for class_name, stats in class_box_stats.items():\\n        print(f\"{class_name}: W={stats[\\'avg_width\\']:.3f} ± {stats[\\'std_width\\']:.3f}, H={stats[\\'avg_height\\']:.3f} ± {stats[\\'std_height\\']:.3f}\")\\n    \\n    print(\"\\n=== Image Quality ===\")\\n    print(f\"Average brightness: {sum(brightness_values)/len(brightness_values):.1f} ± {np.std(brightness_values):.1f} (0-255)\")\\n    print(f\"Average contrast: {sum(contrast_values)/len(contrast_values):.1f} ± {np.std(contrast_values):.1f}\")\\n    \\n    # Plot class distribution\\n    plt.figure(figsize=(10, 6))\\n    plt.bar(class_counts.keys(), class_counts.values())\\n    plt.xlabel(\\'Class\\')\\n    plt.ylabel(\\'Number of Instances\\')\\n    plt.title(\\'Class Distribution\\')\\n    plt.xticks(rotation=45)\\n    plt.tight_layout()\\n    plt.savefig(os.path.join(OUTPUT_DIR, \\'class_distribution.png\\'))\\n    \\n    # Plot image size distribution\\n    plt.figure(figsize=(12, 5))\\n    plt.subplot(1, 2, 1)\\n    plt.hist(image_widths, bins=20)\\n    plt.xlabel(\\'Width (px)\\')\\n    plt.ylabel(\\'Count\\')\\n    plt.title(\\'Image Width Distribution\\')\\n    \\n    plt.subplot(1, 2, 2)\\n    plt.hist(image_heights, bins=20)\\n    plt.xlabel(\\'Height (px)\\')\\n    plt.ylabel(\\'Count\\')\\n    plt.title(\\'Image Height Distribution\\')\\n    plt.tight_layout()\\n    plt.savefig(os.path.join(OUTPUT_DIR, \\'image_size_distribution.png\\'))\\n    \\n    # Plot brightness distribution\\n    plt.figure(figsize=(12, 5))\\n    plt.subplot(1, 2, 1)\\n    plt.hist(brightness_values, bins=50)\\n    plt.xlabel(\\'Brightness\\')\\n    plt.ylabel(\\'Count\\')\\n    plt.title(\\'Image Brightness Distribution\\')\\n    \\n    plt.subplot(1, 2, 2)\\n    plt.hist(contrast_values, bins=50)\\n    plt.xlabel(\\'Contrast\\')\\n    plt.ylabel(\\'Count\\')\\n    plt.title(\\'Image Contrast Distribution\\')\\n    plt.tight_layout()\\n    plt.savefig(os.path.join(OUTPUT_DIR, \\'image_quality_distribution.png\\'))\\n    \\n    # Visualize sample images with bounding boxes\\n    visualize_sample_images(sample_images)\\n    \\n    return class_counts, class_box_stats, brightness_values\\n\\ndef visualize_sample_images(sample_images, num_samples=10):\\n    \"\"\"Visualize sample images with bounding boxes\"\"\"\\n    if not sample_images:\\n        return\\n    \\n    # Select random samples if we have more than we need\\n    if len(sample_images) > num_samples:\\n        sample_images = random.sample(sample_images, num_samples)\\n    \\n    # Create a figure with subplots\\n    fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(14, 20))\\n    axes = axes.flatten()\\n    \\n    for i, sample in enumerate(sample_images):\\n        if i >= len(axes):\\n            break\\n            \\n        # Load the image\\n        img_path = sample[\\'img_path\\']\\n        img = Image.open(img_path)\\n        img_width, img_height = img.size\\n        \\n        # Create a copy for drawing\\n        img_draw = img.copy()\\n        draw = ImageDraw.Draw(img_draw)\\n        \\n        # Get bounding box coordinates\\n        x_center, y_center, width, height = sample[\\'box\\']\\n        # Convert normalized coordinates to pixel coordinates\\n        x1 = int((x_center - width/2) * img_width)\\n        y1 = int((y_center - height/2) * img_height)\\n        x2 = int((x_center + width/2) * img_width)\\n        y2 = int((y_center + height/2) * img_height)\\n        \\n        # Draw the bounding box\\n        draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=3)\\n        \\n        # Add label\\n        draw.text((x1, y1-15), sample[\\'class_name\\'], fill=\"red\")\\n        \\n        # Display the image\\n        axes[i].imshow(img_draw)\\n        axes[i].set_title(f\"{os.path.basename(img_path)} - {sample[\\'class_name\\']}\")\\n        axes[i].axis(\\'off\\')\\n    \\n    # Hide empty subplots\\n    for i in range(len(sample_images), len(axes)):\\n        axes[i].axis(\\'off\\')\\n    \\n    plt.tight_layout()\\n    plt.savefig(os.path.join(OUTPUT_DIR, \\'sample_images.png\\'))\\n    plt.close()\\n\\ndef analyze_test_dataset():\\n    \"\"\"Analyze the test dataset for comparison with training set\"\"\"\\n    print(\"\\nAnalyzing test dataset...\")\\n    \\n    test_images = os.listdir(TEST_IMAGES_DIR)\\n    test_image_sizes = []\\n    test_brightness_values = []\\n    test_contrast_values = []\\n    \\n    # Process each test image\\n    for img_file in test_images:\\n        img_path = os.path.join(TEST_IMAGES_DIR, img_file)\\n        \\n        try:\\n            img = Image.open(img_path)\\n            img_width, img_height = img.size\\n            test_image_sizes.append((img_width, img_height))\\n            \\n            # Convert to numpy array for brightness/contrast analysis\\n            img_np = np.array(img)\\n            if len(img_np.shape) == 3:  # Color image\\n                # Convert to grayscale for brightness analysis\\n                gray_img = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\\n                test_brightness_values.append(np.mean(gray_img))\\n                test_contrast_values.append(np.std(gray_img))\\n        except Exception as e:\\n            print(f\"Error processing test image {img_path}: {e}\")\\n            continue\\n    \\n    # Calculate statistics\\n    test_widths, test_heights = zip(*test_image_sizes)\\n    avg_test_width = sum(test_widths) / len(test_widths)\\n    avg_test_height = sum(test_heights) / len(test_heights)\\n    std_test_width = np.std(test_widths)\\n    std_test_height = np.std(test_heights)\\n    \\n    # Print statistics\\n    print(\"\\n=== Test Dataset Statistics ===\")\\n    print(f\"Test images: {len(test_images)}\")\\n    \\n    print(\"\\n=== Test Image Dimensions ===\")\\n    print(f\"Average width: {avg_test_width:.1f} ± {std_test_width:.1f} px\")\\n    print(f\"Average height: {avg_test_height:.1f} ± {std_test_height:.1f} px\")\\n    print(f\"Min width: {min(test_widths)} px\")\\n    print(f\"Max width: {max(test_widths)} px\")\\n    print(f\"Min height: {min(test_heights)} px\")\\n    print(f\"Max height: {max(test_heights)} px\")\\n    \\n    print(\"\\n=== Test Image Quality ===\")\\n    print(f\"Average brightness: {sum(test_brightness_values)/len(test_brightness_values):.1f} ± {np.std(test_brightness_values):.1f} (0-255)\")\\n    print(f\"Average contrast: {sum(test_contrast_values)/len(test_contrast_values):.1f} ± {np.std(test_contrast_values):.1f}\")\\n    \\n    # Plot test image size distribution\\n    plt.figure(figsize=(12, 5))\\n    plt.subplot(1, 2, 1)\\n    plt.hist(test_widths, bins=20)\\n    plt.xlabel(\\'Width (px)\\')\\n    plt.ylabel(\\'Count\\')\\n    plt.title(\\'Test Image Width Distribution\\')\\n    \\n    plt.subplot(1, 2, 2)\\n    plt.hist(test_heights, bins=20)\\n    plt.xlabel(\\'Height (px)\\')\\n    plt.ylabel(\\'Count\\')\\n    plt.title(\\'Test Image Height Distribution\\')\\n    plt.tight_layout()\\n    plt.savefig(os.path.join(OUTPUT_DIR, \\'test_image_size_distribution.png\\'))\\n    \\n    # Plot test brightness distribution\\n    plt.figure(figsize=(12, 5))\\n    plt.subplot(1, 2, 1)\\n    plt.hist(test_brightness_values, bins=50)\\n    plt.xlabel(\\'Brightness\\')\\n    plt.ylabel(\\'Count\\')\\n    plt.title(\\'Test Image Brightness Distribution\\')\\n    \\n    plt.subplot(1, 2, 2)\\n    plt.hist(test_contrast_values, bins=50)\\n    plt.xlabel(\\'Contrast\\')\\n    plt.ylabel(\\'Count\\')\\n    plt.title(\\'Test Image Contrast Distribution\\')\\n    plt.tight_layout()\\n    plt.savefig(os.path.join(OUTPUT_DIR, \\'test_image_quality_distribution.png\\'))\\n    \\n    # Compare train and test distributions\\n    plt.figure(figsize=(12, 10))\\n    \\n    plt.subplot(2, 2, 1)\\n    plt.hist(image_widths, bins=20, alpha=0.5, label=\\'Train\\')\\n    plt.hist(test_widths, bins=20, alpha=0.5, label=\\'Test\\')\\n    plt.xlabel(\\'Width (px)\\')\\n    plt.ylabel(\\'Count\\')\\n    plt.title(\\'Image Width Comparison\\')\\n    plt.legend()\\n    \\n    plt.subplot(2, 2, 2)\\n    plt.hist(image_heights, bins=20, alpha=0.5, label=\\'Train\\')\\n    plt.hist(test_heights, bins=20, alpha=0.5, label=\\'Test\\')\\n    plt.xlabel(\\'Height (px)\\')\\n    plt.ylabel(\\'Count\\')\\n    plt.title(\\'Image Height Comparison\\')\\n    plt.legend()\\n    \\n    plt.subplot(2, 2, 3)\\n    plt.hist(brightness_values, bins=50, alpha=0.5, label=\\'Train\\')\\n    plt.hist(test_brightness_values, bins=50, alpha=0.5, label=\\'Test\\')\\n    plt.xlabel(\\'Brightness\\')\\n    plt.ylabel(\\'Count\\')\\n    plt.title(\\'Image Brightness Comparison\\')\\n    plt.legend()\\n    \\n    plt.subplot(2, 2, 4)\\n    plt.hist(contrast_values, bins=50, alpha=0.5, label=\\'Train\\')\\n    plt.hist(test_contrast_values, bins=50, alpha=0.5, label=\\'Test\\')\\n    plt.xlabel(\\'Contrast\\')\\n    plt.ylabel(\\'Count\\')\\n    plt.title(\\'Image Contrast Comparison\\')\\n    plt.legend()\\n    \\n    plt.tight_layout()\\n    plt.savefig(os.path.join(OUTPUT_DIR, \\'train_test_comparison.png\\'))\\n    plt.close()\\n    \\n    # Visualize sample test images\\n    visualize_test_samples(test_images)\\n    \\n    return test_brightness_values, test_contrast_values\\n\\ndef visualize_test_samples(test_images, num_samples=10):\\n    \"\"\"Visualize sample test images\"\"\"\\n    if not test_images:\\n        return\\n    \\n    # Select random samples\\n    sample_files = random.sample(test_images, min(num_samples, len(test_images)))\\n    \\n    # Create a figure with subplots\\n    fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(14, 20))\\n    axes = axes.flatten()\\n    \\n    for i, img_file in enumerate(sample_files):\\n        if i >= len(axes):\\n            break\\n            \\n        # Load the image\\n        img_path = os.path.join(TEST_IMAGES_DIR, img_file)\\n        img = Image.open(img_path)\\n        \\n        # Display the image\\n        axes[i].imshow(img)\\n        axes[i].set_title(f\"{img_file}\")\\n        axes[i].axis(\\'off\\')\\n    \\n    # Hide empty subplots\\n    for i in range(len(sample_files), len(axes)):\\n        axes[i].axis(\\'off\\')\\n    \\n    plt.tight_layout()\\n    plt.savefig(os.path.join(OUTPUT_DIR, \\'test_sample_images.png\\'))\\n    plt.close()\\n\\ndef check_data_quality_issues():\\n    \"\"\"Check for common data quality issues\"\"\"\\n    print(\"\\nChecking for data quality issues...\")\\n    \\n    # Check for mismatches between images and labels\\n    train_images = set(os.listdir(TRAIN_IMAGES_DIR))\\n    train_labels = set([f.replace(\\'.txt\\', \\'.jpeg\\') for f in os.listdir(TRAIN_LABELS_DIR)])\\n    train_labels.update([f.replace(\\'.txt\\', \\'.jpg\\') for f in os.listdir(TRAIN_LABELS_DIR)])\\n    \\n    missing_labels = [img for img in train_images if img.replace(\\'.jpeg\\', \\'.txt\\').replace(\\'.jpg\\', \\'.txt\\') not in os.listdir(TRAIN_LABELS_DIR)]\\n    missing_images = [label.replace(\\'.txt\\', \\'.jpeg\\') for label in os.listdir(TRAIN_LABELS_DIR) if label.replace(\\'.txt\\', \\'.jpeg\\') not in train_images and label.replace(\\'.txt\\', \\'.jpg\\') not in train_images]\\n    \\n    print(f\"Images without labels: {len(missing_labels)}\")\\n    print(f\"Labels without images: {len(missing_images)}\")\\n    \\n    # Check for corrupted images\\n    corrupted_images = []\\n    for img_file in train_images:\\n        img_path = os.path.join(TRAIN_IMAGES_DIR, img_file)\\n        try:\\n            img = Image.open(img_path)\\n            img.verify()  # Verify the image\\n        except:\\n            corrupted_images.append(img_file)\\n    \\n    print(f\"Corrupted images: {len(corrupted_images)}\")\\n    if corrupted_images:\\n        print(\"Sample corrupted images:\", corrupted_images[:5])\\n    \\n    # Check for empty label files\\n    empty_labels = []\\n    for label_file in os.listdir(TRAIN_LABELS_DIR):\\n        label_path = os.path.join(TRAIN_LABELS_DIR, label_file)\\n        try:\\n            with open(label_path, \\'r\\') as f:\\n                content = f.read().strip()\\n                if not content:\\n                    empty_labels.append(label_file)\\n        except:\\n            print(f\"Error reading label file: {label_file}\")\\n    \\n    print(f\"Empty label files: {len(empty_labels)}\")\\n    \\n    # Check for inconsistent label formats\\n    malformed_labels = []\\n    for label_file in os.listdir(TRAIN_LABELS_DIR):\\n        label_path = os.path.join(TRAIN_LABELS_DIR, label_file)\\n        try:\\n            with open(label_path, \\'r\\') as f:\\n                lines = f.readlines()\\n                for line in lines:\\n                    parts = line.strip().split()\\n                    if len(parts) != 5:\\n                        malformed_labels.append(label_file)\\n                        break\\n                    \\n                    # Check if class id is valid\\n                    class_id = int(parts[0])\\n                    if class_id not in class_names:\\n                        malformed_labels.append(label_file)\\n                        break\\n                    \\n                    # Check if bounding box coordinates are valid\\n                    x_center, y_center, width, height = map(float, parts[1:5])\\n                    if not (0 <= x_center <= 1 and 0 <= y_center <= 1 and 0 <= width <= 1 and 0 <= height <= 1):\\n                        malformed_labels.append(label_file)\\n                        break\\n        except:\\n            malformed_labels.append(label_file)\\n    \\n    print(f\"Malformed label files: {len(malformed_labels)}\")\\n    if malformed_labels:\\n        print(\"Sample malformed labels:\", malformed_labels[:5])\\n    \\n    return corrupted_images, malformed_labels\\n\\ndef analyze_low_light_images(threshold=100):\\n    \"\"\"Identify and analyze low-light images\"\"\"\\n    print(f\"\\nAnalyzing low-light images (brightness < {threshold})...\")\\n    \\n    low_light_train = []\\n    low_light_test = []\\n    \\n    # Process training images\\n    for img_file in os.listdir(TRAIN_IMAGES_DIR):\\n        img_path = os.path.join(TRAIN_IMAGES_DIR, img_file)\\n        try:\\n            img = Image.open(img_path)\\n            img_np = np.array(img)\\n            if len(img_np.shape) == 3:  # Color image\\n                gray_img = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\\n                brightness = np.mean(gray_img)\\n                if brightness < threshold:\\n                    low_light_train.append((img_file, brightness))\\n        except:\\n            continue\\n    \\n    # Process test images\\n    for img_file in os.listdir(TEST_IMAGES_DIR):\\n        img_path = os.path.join(TEST_IMAGES_DIR, img_file)\\n        try:\\n            img = Image.open(img_path)\\n            img_np = np.array(img)\\n            if len(img_np.shape) == 3:  # Color image\\n                gray_img = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\\n                brightness = np.mean(gray_img)\\n                if brightness < threshold:\\n                    low_light_test.append((img_file, brightness))\\n        except:\\n            continue\\n    \\n    print(f\"Low-light training images: {len(low_light_train)} ({len(low_light_train)/len(os.listdir(TRAIN_IMAGES_DIR))*100:.1f}%)\")\\n    print(f\"Low-light test images: {len(low_light_test)} ({len(low_light_test)/len(os.listdir(TEST_IMAGES_DIR))*100:.1f}%)\")\\n    \\n    # Save sample low-light images\\n    output_folder = os.path.join(OUTPUT_DIR, \\'low_light_samples\\')\\n    os.makedirs(output_folder, exist_ok=True)\\n    \\n    # Sample and save some low-light images\\n    if low_light_train:\\n        samples = sorted(low_light_train, key=lambda x: x[1])[:10]  # Get the 10 darkest images\\n        for i, (img_file, brightness) in enumerate(samples):\\n            src_path = os.path.join(TRAIN_IMAGES_DIR, img_file)\\n            dst_path = os.path.join(output_folder, f\"train_low_light_{i}_{brightness:.1f}_{img_file}\")\\n            shutil.copy(src_path, dst_path)\\n    \\n    if low_light_test:\\n        samples = sorted(low_light_test, key=lambda x: x[1])[:10]  # Get the 10 darkest images\\n        for i, (img_file, brightness) in enumerate(samples):\\n            src_path = os.path.join(TEST_IMAGES_DIR, img_file)\\n            dst_path = os.path.join(output_folder, f\"test_low_light_{i}_{brightness:.1f}_{img_file}\")\\n            shutil.copy(src_path, dst_path)\\n    \\n    return low_light_train, low_light_test\\n\\n# Global variable to store brightness values\\nimage_widths = []\\nimage_heights = []\\nbrightness_values = []\\ncontrast_values = []\\n\\n# Main function to run all analyses\\ndef main():\\n    global brightness_values\\n    \\n    print(\"Starting dataset analysis...\")\\n    \\n    # Analyze training dataset\\n    class_counts, class_box_stats, brightness_values = analyze_dataset_statistics()\\n    \\n    # Analyze test dataset\\n    test_brightness_values, test_contrast_values = analyze_test_dataset()\\n    \\n    # Check for data quality issues\\n    corrupted_images, malformed_labels = check_data_quality_issues()\\n    \\n    # Analyze low-light images\\n    low_light_train, low_light_test = analyze_low_light_images(threshold=100)\\n    \\n    print(\"\\nAnalysis complete! Results saved to:\", OUTPUT_DIR)\\n\\nif __name__ == \"__main__\":\\n    main()\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "import random\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import cv2\n",
    "\n",
    "# Define paths (modify these to match your environment)\n",
    "BASE_DIR = '/kaggle/input/dlp-object-detection-week-10/final_dlp_data/final_dlp_data'\n",
    "TRAIN_IMAGES_DIR = os.path.join(BASE_DIR, 'train/images')\n",
    "TRAIN_LABELS_DIR = os.path.join(BASE_DIR, 'train/labels')\n",
    "TEST_IMAGES_DIR = os.path.join(BASE_DIR, 'test/images')\n",
    "OUTPUT_DIR = \"/kaggle/working/analysis\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Define class names\n",
    "class_names = {\n",
    "    0: \"aegypti\",\n",
    "    1: \"albopictus\",\n",
    "    2: \"anopheles\", \n",
    "    3: \"culex\",\n",
    "    4: \"culiseta\",\n",
    "    5: \"japonicus/koreicus\"\n",
    "}\n",
    "\n",
    "def analyze_dataset_statistics():\n",
    "    \"\"\"Analyze and visualize dataset statistics\"\"\"\n",
    "    print(\"Analyzing dataset statistics...\")\n",
    "    \n",
    "    # Count images\n",
    "    train_images = len(os.listdir(TRAIN_IMAGES_DIR))\n",
    "    test_images = len(os.listdir(TEST_IMAGES_DIR))\n",
    "    \n",
    "    print(f\"Training images: {train_images}\")\n",
    "    print(f\"Testing images: {test_images}\")\n",
    "    \n",
    "    # Count labels per class\n",
    "    class_counts = {name: 0 for name in class_names.values()}\n",
    "    image_sizes = []\n",
    "    box_sizes = []\n",
    "    class_box_sizes = {name: [] for name in class_names.values()}\n",
    "    brightness_values = []\n",
    "    contrast_values = []\n",
    "    \n",
    "    # Sample images for visualization\n",
    "    sample_indices = random.sample(range(train_images), min(50, train_images))\n",
    "    sample_images = []\n",
    "    \n",
    "    # Process each label file\n",
    "    all_files = os.listdir(TRAIN_LABELS_DIR)\n",
    "    for i, label_file in enumerate(all_files):\n",
    "        img_file = label_file.replace('.txt', '.jpeg')\n",
    "        img_path = os.path.join(TRAIN_IMAGES_DIR, img_file)\n",
    "        \n",
    "        # Check if image exists\n",
    "        if not os.path.exists(img_path):\n",
    "            img_file = label_file.replace('.txt', '.jpg')\n",
    "            img_path = os.path.join(TRAIN_IMAGES_DIR, img_file)\n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"Warning: Image file not found for {label_file}\")\n",
    "                continue\n",
    "        \n",
    "        # Analyze image properties\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "            img_width, img_height = img.size\n",
    "            image_sizes.append((img_width, img_height))\n",
    "            \n",
    "            # Convert to numpy array for brightness/contrast analysis\n",
    "            img_np = np.array(img)\n",
    "            if len(img_np.shape) == 3:  # Color image\n",
    "                # Convert to grayscale for brightness analysis\n",
    "                gray_img = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n",
    "                brightness_values.append(np.mean(gray_img))\n",
    "                contrast_values.append(np.std(gray_img))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Read label file\n",
    "        try:\n",
    "            with open(os.path.join(TRAIN_LABELS_DIR, label_file), 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                \n",
    "                for line in lines:\n",
    "                    if line.strip():\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) >= 5:\n",
    "                            class_id = int(parts[0])\n",
    "                            x_center = float(parts[1])\n",
    "                            y_center = float(parts[2])\n",
    "                            width = float(parts[3])\n",
    "                            height = float(parts[4])\n",
    "                            \n",
    "                            class_name = class_names[class_id]\n",
    "                            class_counts[class_name] += 1\n",
    "                            \n",
    "                            # Calculate box area (normalized)\n",
    "                            box_area = width * height\n",
    "                            box_sizes.append(box_area)\n",
    "                            class_box_sizes[class_name].append((width, height))\n",
    "                            \n",
    "                            # If this is a sample image, store info for visualization\n",
    "                            if i in sample_indices:\n",
    "                                # Create visualization of the bounding box\n",
    "                                sample_images.append({\n",
    "                                    'img_path': img_path,\n",
    "                                    'class_name': class_name,\n",
    "                                    'box': (x_center, y_center, width, height)\n",
    "                                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing label {label_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Calculate statistics\n",
    "    image_widths, image_heights = zip(*image_sizes)\n",
    "    avg_width = sum(image_widths) / len(image_widths)\n",
    "    avg_height = sum(image_heights) / len(image_heights)\n",
    "    std_width = np.std(image_widths)\n",
    "    std_height = np.std(image_heights)\n",
    "    \n",
    "    avg_box_size = sum(box_sizes) / len(box_sizes) if box_sizes else 0\n",
    "    std_box_size = np.std(box_sizes) if box_sizes else 0\n",
    "    \n",
    "    # Calculate per-class box sizes\n",
    "    class_box_stats = {}\n",
    "    for class_name, boxes in class_box_sizes.items():\n",
    "        if boxes:\n",
    "            widths, heights = zip(*boxes)\n",
    "            class_box_stats[class_name] = {\n",
    "                'avg_width': sum(widths) / len(widths),\n",
    "                'std_width': np.std(widths),\n",
    "                'avg_height': sum(heights) / len(heights),\n",
    "                'std_height': np.std(heights)\n",
    "            }\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\n=== Dataset Statistics ===\")\n",
    "    print(f\"Training images: {train_images}\")\n",
    "    print(f\"Test images: {test_images}\")\n",
    "    \n",
    "    print(\"\\n=== Class Distribution ===\")\n",
    "    total_boxes = sum(class_counts.values())\n",
    "    for class_name, count in class_counts.items():\n",
    "        print(f\"{class_name}: {count} samples ({count/train_images:.2f} per image, {count/total_boxes*100:.1f}% of total)\")\n",
    "    \n",
    "    print(\"\\n=== Image Dimensions ===\")\n",
    "    print(f\"Average width: {avg_width:.1f} ± {std_width:.1f} px\")\n",
    "    print(f\"Average height: {avg_height:.1f} ± {std_height:.1f} px\")\n",
    "    print(f\"Min width: {min(image_widths)} px\")\n",
    "    print(f\"Max width: {max(image_widths)} px\")\n",
    "    print(f\"Min height: {min(image_heights)} px\")\n",
    "    print(f\"Max height: {max(image_heights)} px\")\n",
    "    \n",
    "    print(\"\\n=== Bounding Boxes ===\")\n",
    "    print(f\"Average box size: {avg_box_size:.4f} ± {std_box_size:.4f} (normalized area)\")\n",
    "    print(\"\\nPer-class box sizes (normalized width/height):\")\n",
    "    for class_name, stats in class_box_stats.items():\n",
    "        print(f\"{class_name}: W={stats['avg_width']:.3f} ± {stats['std_width']:.3f}, H={stats['avg_height']:.3f} ± {stats['std_height']:.3f}\")\n",
    "    \n",
    "    print(\"\\n=== Image Quality ===\")\n",
    "    print(f\"Average brightness: {sum(brightness_values)/len(brightness_values):.1f} ± {np.std(brightness_values):.1f} (0-255)\")\n",
    "    print(f\"Average contrast: {sum(contrast_values)/len(contrast_values):.1f} ± {np.std(contrast_values):.1f}\")\n",
    "    \n",
    "    # Plot class distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(class_counts.keys(), class_counts.values())\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Number of Instances')\n",
    "    plt.title('Class Distribution')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'class_distribution.png'))\n",
    "    \n",
    "    # Plot image size distribution\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(image_widths, bins=20)\n",
    "    plt.xlabel('Width (px)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Image Width Distribution')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(image_heights, bins=20)\n",
    "    plt.xlabel('Height (px)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Image Height Distribution')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'image_size_distribution.png'))\n",
    "    \n",
    "    # Plot brightness distribution\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(brightness_values, bins=50)\n",
    "    plt.xlabel('Brightness')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Image Brightness Distribution')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(contrast_values, bins=50)\n",
    "    plt.xlabel('Contrast')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Image Contrast Distribution')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'image_quality_distribution.png'))\n",
    "    \n",
    "    # Visualize sample images with bounding boxes\n",
    "    visualize_sample_images(sample_images)\n",
    "    \n",
    "    return class_counts, class_box_stats, brightness_values\n",
    "\n",
    "def visualize_sample_images(sample_images, num_samples=10):\n",
    "    \"\"\"Visualize sample images with bounding boxes\"\"\"\n",
    "    if not sample_images:\n",
    "        return\n",
    "    \n",
    "    # Select random samples if we have more than we need\n",
    "    if len(sample_images) > num_samples:\n",
    "        sample_images = random.sample(sample_images, num_samples)\n",
    "    \n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(14, 20))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, sample in enumerate(sample_images):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "            \n",
    "        # Load the image\n",
    "        img_path = sample['img_path']\n",
    "        img = Image.open(img_path)\n",
    "        img_width, img_height = img.size\n",
    "        \n",
    "        # Create a copy for drawing\n",
    "        img_draw = img.copy()\n",
    "        draw = ImageDraw.Draw(img_draw)\n",
    "        \n",
    "        # Get bounding box coordinates\n",
    "        x_center, y_center, width, height = sample['box']\n",
    "        # Convert normalized coordinates to pixel coordinates\n",
    "        x1 = int((x_center - width/2) * img_width)\n",
    "        y1 = int((y_center - height/2) * img_height)\n",
    "        x2 = int((x_center + width/2) * img_width)\n",
    "        y2 = int((y_center + height/2) * img_height)\n",
    "        \n",
    "        # Draw the bounding box\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=3)\n",
    "        \n",
    "        # Add label\n",
    "        draw.text((x1, y1-15), sample['class_name'], fill=\"red\")\n",
    "        \n",
    "        # Display the image\n",
    "        axes[i].imshow(img_draw)\n",
    "        axes[i].set_title(f\"{os.path.basename(img_path)} - {sample['class_name']}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(sample_images), len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'sample_images.png'))\n",
    "    plt.close()\n",
    "\n",
    "def analyze_test_dataset():\n",
    "    \"\"\"Analyze the test dataset for comparison with training set\"\"\"\n",
    "    print(\"\\nAnalyzing test dataset...\")\n",
    "    \n",
    "    test_images = os.listdir(TEST_IMAGES_DIR)\n",
    "    test_image_sizes = []\n",
    "    test_brightness_values = []\n",
    "    test_contrast_values = []\n",
    "    \n",
    "    # Process each test image\n",
    "    for img_file in test_images:\n",
    "        img_path = os.path.join(TEST_IMAGES_DIR, img_file)\n",
    "        \n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "            img_width, img_height = img.size\n",
    "            test_image_sizes.append((img_width, img_height))\n",
    "            \n",
    "            # Convert to numpy array for brightness/contrast analysis\n",
    "            img_np = np.array(img)\n",
    "            if len(img_np.shape) == 3:  # Color image\n",
    "                # Convert to grayscale for brightness analysis\n",
    "                gray_img = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n",
    "                test_brightness_values.append(np.mean(gray_img))\n",
    "                test_contrast_values.append(np.std(gray_img))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing test image {img_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Calculate statistics\n",
    "    test_widths, test_heights = zip(*test_image_sizes)\n",
    "    avg_test_width = sum(test_widths) / len(test_widths)\n",
    "    avg_test_height = sum(test_heights) / len(test_heights)\n",
    "    std_test_width = np.std(test_widths)\n",
    "    std_test_height = np.std(test_heights)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\n=== Test Dataset Statistics ===\")\n",
    "    print(f\"Test images: {len(test_images)}\")\n",
    "    \n",
    "    print(\"\\n=== Test Image Dimensions ===\")\n",
    "    print(f\"Average width: {avg_test_width:.1f} ± {std_test_width:.1f} px\")\n",
    "    print(f\"Average height: {avg_test_height:.1f} ± {std_test_height:.1f} px\")\n",
    "    print(f\"Min width: {min(test_widths)} px\")\n",
    "    print(f\"Max width: {max(test_widths)} px\")\n",
    "    print(f\"Min height: {min(test_heights)} px\")\n",
    "    print(f\"Max height: {max(test_heights)} px\")\n",
    "    \n",
    "    print(\"\\n=== Test Image Quality ===\")\n",
    "    print(f\"Average brightness: {sum(test_brightness_values)/len(test_brightness_values):.1f} ± {np.std(test_brightness_values):.1f} (0-255)\")\n",
    "    print(f\"Average contrast: {sum(test_contrast_values)/len(test_contrast_values):.1f} ± {np.std(test_contrast_values):.1f}\")\n",
    "    \n",
    "    # Plot test image size distribution\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(test_widths, bins=20)\n",
    "    plt.xlabel('Width (px)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Test Image Width Distribution')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(test_heights, bins=20)\n",
    "    plt.xlabel('Height (px)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Test Image Height Distribution')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'test_image_size_distribution.png'))\n",
    "    \n",
    "    # Plot test brightness distribution\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(test_brightness_values, bins=50)\n",
    "    plt.xlabel('Brightness')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Test Image Brightness Distribution')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(test_contrast_values, bins=50)\n",
    "    plt.xlabel('Contrast')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Test Image Contrast Distribution')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'test_image_quality_distribution.png'))\n",
    "    \n",
    "    # Compare train and test distributions\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.hist(image_widths, bins=20, alpha=0.5, label='Train')\n",
    "    plt.hist(test_widths, bins=20, alpha=0.5, label='Test')\n",
    "    plt.xlabel('Width (px)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Image Width Comparison')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.hist(image_heights, bins=20, alpha=0.5, label='Train')\n",
    "    plt.hist(test_heights, bins=20, alpha=0.5, label='Test')\n",
    "    plt.xlabel('Height (px)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Image Height Comparison')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.hist(brightness_values, bins=50, alpha=0.5, label='Train')\n",
    "    plt.hist(test_brightness_values, bins=50, alpha=0.5, label='Test')\n",
    "    plt.xlabel('Brightness')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Image Brightness Comparison')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.hist(contrast_values, bins=50, alpha=0.5, label='Train')\n",
    "    plt.hist(test_contrast_values, bins=50, alpha=0.5, label='Test')\n",
    "    plt.xlabel('Contrast')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Image Contrast Comparison')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'train_test_comparison.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Visualize sample test images\n",
    "    visualize_test_samples(test_images)\n",
    "    \n",
    "    return test_brightness_values, test_contrast_values\n",
    "\n",
    "def visualize_test_samples(test_images, num_samples=10):\n",
    "    \"\"\"Visualize sample test images\"\"\"\n",
    "    if not test_images:\n",
    "        return\n",
    "    \n",
    "    # Select random samples\n",
    "    sample_files = random.sample(test_images, min(num_samples, len(test_images)))\n",
    "    \n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(14, 20))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, img_file in enumerate(sample_files):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "            \n",
    "        # Load the image\n",
    "        img_path = os.path.join(TEST_IMAGES_DIR, img_file)\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        # Display the image\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"{img_file}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(sample_files), len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'test_sample_images.png'))\n",
    "    plt.close()\n",
    "\n",
    "def check_data_quality_issues():\n",
    "    \"\"\"Check for common data quality issues\"\"\"\n",
    "    print(\"\\nChecking for data quality issues...\")\n",
    "    \n",
    "    # Check for mismatches between images and labels\n",
    "    train_images = set(os.listdir(TRAIN_IMAGES_DIR))\n",
    "    train_labels = set([f.replace('.txt', '.jpeg') for f in os.listdir(TRAIN_LABELS_DIR)])\n",
    "    train_labels.update([f.replace('.txt', '.jpg') for f in os.listdir(TRAIN_LABELS_DIR)])\n",
    "    \n",
    "    missing_labels = [img for img in train_images if img.replace('.jpeg', '.txt').replace('.jpg', '.txt') not in os.listdir(TRAIN_LABELS_DIR)]\n",
    "    missing_images = [label.replace('.txt', '.jpeg') for label in os.listdir(TRAIN_LABELS_DIR) if label.replace('.txt', '.jpeg') not in train_images and label.replace('.txt', '.jpg') not in train_images]\n",
    "    \n",
    "    print(f\"Images without labels: {len(missing_labels)}\")\n",
    "    print(f\"Labels without images: {len(missing_images)}\")\n",
    "    \n",
    "    # Check for corrupted images\n",
    "    corrupted_images = []\n",
    "    for img_file in train_images:\n",
    "        img_path = os.path.join(TRAIN_IMAGES_DIR, img_file)\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "            img.verify()  # Verify the image\n",
    "        except:\n",
    "            corrupted_images.append(img_file)\n",
    "    \n",
    "    print(f\"Corrupted images: {len(corrupted_images)}\")\n",
    "    if corrupted_images:\n",
    "        print(\"Sample corrupted images:\", corrupted_images[:5])\n",
    "    \n",
    "    # Check for empty label files\n",
    "    empty_labels = []\n",
    "    for label_file in os.listdir(TRAIN_LABELS_DIR):\n",
    "        label_path = os.path.join(TRAIN_LABELS_DIR, label_file)\n",
    "        try:\n",
    "            with open(label_path, 'r') as f:\n",
    "                content = f.read().strip()\n",
    "                if not content:\n",
    "                    empty_labels.append(label_file)\n",
    "        except:\n",
    "            print(f\"Error reading label file: {label_file}\")\n",
    "    \n",
    "    print(f\"Empty label files: {len(empty_labels)}\")\n",
    "    \n",
    "    # Check for inconsistent label formats\n",
    "    malformed_labels = []\n",
    "    for label_file in os.listdir(TRAIN_LABELS_DIR):\n",
    "        label_path = os.path.join(TRAIN_LABELS_DIR, label_file)\n",
    "        try:\n",
    "            with open(label_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) != 5:\n",
    "                        malformed_labels.append(label_file)\n",
    "                        break\n",
    "                    \n",
    "                    # Check if class id is valid\n",
    "                    class_id = int(parts[0])\n",
    "                    if class_id not in class_names:\n",
    "                        malformed_labels.append(label_file)\n",
    "                        break\n",
    "                    \n",
    "                    # Check if bounding box coordinates are valid\n",
    "                    x_center, y_center, width, height = map(float, parts[1:5])\n",
    "                    if not (0 <= x_center <= 1 and 0 <= y_center <= 1 and 0 <= width <= 1 and 0 <= height <= 1):\n",
    "                        malformed_labels.append(label_file)\n",
    "                        break\n",
    "        except:\n",
    "            malformed_labels.append(label_file)\n",
    "    \n",
    "    print(f\"Malformed label files: {len(malformed_labels)}\")\n",
    "    if malformed_labels:\n",
    "        print(\"Sample malformed labels:\", malformed_labels[:5])\n",
    "    \n",
    "    return corrupted_images, malformed_labels\n",
    "\n",
    "def analyze_low_light_images(threshold=100):\n",
    "    \"\"\"Identify and analyze low-light images\"\"\"\n",
    "    print(f\"\\nAnalyzing low-light images (brightness < {threshold})...\")\n",
    "    \n",
    "    low_light_train = []\n",
    "    low_light_test = []\n",
    "    \n",
    "    # Process training images\n",
    "    for img_file in os.listdir(TRAIN_IMAGES_DIR):\n",
    "        img_path = os.path.join(TRAIN_IMAGES_DIR, img_file)\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "            img_np = np.array(img)\n",
    "            if len(img_np.shape) == 3:  # Color image\n",
    "                gray_img = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n",
    "                brightness = np.mean(gray_img)\n",
    "                if brightness < threshold:\n",
    "                    low_light_train.append((img_file, brightness))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Process test images\n",
    "    for img_file in os.listdir(TEST_IMAGES_DIR):\n",
    "        img_path = os.path.join(TEST_IMAGES_DIR, img_file)\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "            img_np = np.array(img)\n",
    "            if len(img_np.shape) == 3:  # Color image\n",
    "                gray_img = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n",
    "                brightness = np.mean(gray_img)\n",
    "                if brightness < threshold:\n",
    "                    low_light_test.append((img_file, brightness))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Low-light training images: {len(low_light_train)} ({len(low_light_train)/len(os.listdir(TRAIN_IMAGES_DIR))*100:.1f}%)\")\n",
    "    print(f\"Low-light test images: {len(low_light_test)} ({len(low_light_test)/len(os.listdir(TEST_IMAGES_DIR))*100:.1f}%)\")\n",
    "    \n",
    "    # Save sample low-light images\n",
    "    output_folder = os.path.join(OUTPUT_DIR, 'low_light_samples')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Sample and save some low-light images\n",
    "    if low_light_train:\n",
    "        samples = sorted(low_light_train, key=lambda x: x[1])[:10]  # Get the 10 darkest images\n",
    "        for i, (img_file, brightness) in enumerate(samples):\n",
    "            src_path = os.path.join(TRAIN_IMAGES_DIR, img_file)\n",
    "            dst_path = os.path.join(output_folder, f\"train_low_light_{i}_{brightness:.1f}_{img_file}\")\n",
    "            shutil.copy(src_path, dst_path)\n",
    "    \n",
    "    if low_light_test:\n",
    "        samples = sorted(low_light_test, key=lambda x: x[1])[:10]  # Get the 10 darkest images\n",
    "        for i, (img_file, brightness) in enumerate(samples):\n",
    "            src_path = os.path.join(TEST_IMAGES_DIR, img_file)\n",
    "            dst_path = os.path.join(output_folder, f\"test_low_light_{i}_{brightness:.1f}_{img_file}\")\n",
    "            shutil.copy(src_path, dst_path)\n",
    "    \n",
    "    return low_light_train, low_light_test\n",
    "\n",
    "# Global variable to store brightness values\n",
    "image_widths = []\n",
    "image_heights = []\n",
    "brightness_values = []\n",
    "contrast_values = []\n",
    "\n",
    "# Main function to run all analyses\n",
    "def main():\n",
    "    global brightness_values\n",
    "    \n",
    "    print(\"Starting dataset analysis...\")\n",
    "    \n",
    "    # Analyze training dataset\n",
    "    class_counts, class_box_stats, brightness_values = analyze_dataset_statistics()\n",
    "    \n",
    "    # Analyze test dataset\n",
    "    test_brightness_values, test_contrast_values = analyze_test_dataset()\n",
    "    \n",
    "    # Check for data quality issues\n",
    "    corrupted_images, malformed_labels = check_data_quality_issues()\n",
    "    \n",
    "    # Analyze low-light images\n",
    "    low_light_train, low_light_test = analyze_low_light_images(threshold=100)\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Results saved to:\", OUTPUT_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e049d7a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T18:39:29.912192Z",
     "iopub.status.busy": "2025-03-23T18:39:29.911996Z",
     "iopub.status.idle": "2025-03-23T18:39:50.133057Z",
     "shell.execute_reply": "2025-03-23T18:39:50.131968Z"
    },
    "papermill": {
     "duration": 20.228906,
     "end_time": "2025-03-23T18:39:50.134400",
     "exception": false,
     "start_time": "2025-03-23T18:39:29.905494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef4eeaf514f34ca1bb950d617649fe60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyzing dataset:   0%|          | 0/7500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c472b1c79021436cab5b65a32fd9adbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing stratification:   0%|          | 0/7500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (0, 0)\n",
      "DataFrame columns: []\n",
      "Warning: 'primary_class' column missing or DataFrame is empty. Using basic fold splitting.\n",
      "Successfully set up weighted dataset for training\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8x.pt to 'yolov8x.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131M/131M [00:00<00:00, 285MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: '\u001b[31m\u001b[1mcallbacks\u001b[0m' is not a valid YOLO argument. \n",
      "\n",
      "    Arguments received: ['yolo', '-f', '/tmp/tmpcxvyk3ad.json', '--HistoryManager.hist_file=:memory:']. Ultralytics 'yolo' commands use the following syntax:\n",
      "\n",
      "        yolo TASK MODE ARGS\n",
      "\n",
      "        Where   TASK (optional) is one of frozenset({'detect', 'obb', 'segment', 'pose', 'classify'})\n",
      "                MODE (required) is one of frozenset({'val', 'export', 'predict', 'benchmark', 'track', 'train'})\n",
      "                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n",
      "                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n",
      "\n",
      "    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n",
      "        yolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01\n",
      "\n",
      "    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n",
      "        yolo predict model=yolo11n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n",
      "\n",
      "    3. Val a pretrained detection model at batch-size 1 and image size 640:\n",
      "        yolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640\n",
      "\n",
      "    4. Export a YOLO11n classification model to ONNX format at image size 224 by 128 (no TASK required)\n",
      "        yolo export model=yolo11n-cls.pt format=onnx imgsz=224,128\n",
      "\n",
      "    5. Ultralytics solutions usage\n",
      "        yolo solutions count or in ['crop', 'blur', 'workout', 'heatmap', 'isegment', 'visioneye', 'speed', 'queue', 'analytics', 'inference', 'trackzone'] source=\"path/to/video/file.mp4\"\n",
      "\n",
      "    6. Run special commands:\n",
      "        yolo help\n",
      "        yolo checks\n",
      "        yolo version\n",
      "        yolo settings\n",
      "        yolo copy-cfg\n",
      "        yolo cfg\n",
      "        yolo solutions help\n",
      "\n",
      "    Docs: https://docs.ultralytics.com\n",
      "    Solutions: https://docs.ultralytics.com/solutions/\n",
      "    Community: https://community.ultralytics.com\n",
      "    GitHub: https://github.com/ultralytics/ultralytics\n",
      "    \n",
      "Training error: '\u001b[31m\u001b[1mcallbacks\u001b[0m' is not a valid YOLO argument. \n",
      "\n",
      "    Arguments received: ['yolo', '-f', '/tmp/tmpcxvyk3ad.json', '--HistoryManager.hist_file=:memory:']. Ultralytics 'yolo' commands use the following syntax:\n",
      "\n",
      "        yolo TASK MODE ARGS\n",
      "\n",
      "        Where   TASK (optional) is one of frozenset({'detect', 'obb', 'segment', 'pose', 'classify'})\n",
      "                MODE (required) is one of frozenset({'val', 'export', 'predict', 'benchmark', 'track', 'train'})\n",
      "                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n",
      "                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n",
      "\n",
      "    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n",
      "        yolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01\n",
      "\n",
      "    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n",
      "        yolo predict model=yolo11n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n",
      "\n",
      "    3. Val a pretrained detection model at batch-size 1 and image size 640:\n",
      "        yolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640\n",
      "\n",
      "    4. Export a YOLO11n classification model to ONNX format at image size 224 by 128 (no TASK required)\n",
      "        yolo export model=yolo11n-cls.pt format=onnx imgsz=224,128\n",
      "\n",
      "    5. Ultralytics solutions usage\n",
      "        yolo solutions count or in ['crop', 'blur', 'workout', 'heatmap', 'isegment', 'visioneye', 'speed', 'queue', 'analytics', 'inference', 'trackzone'] source=\"path/to/video/file.mp4\"\n",
      "\n",
      "    6. Run special commands:\n",
      "        yolo help\n",
      "        yolo checks\n",
      "        yolo version\n",
      "        yolo settings\n",
      "        yolo copy-cfg\n",
      "        yolo cfg\n",
      "        yolo solutions help\n",
      "\n",
      "    Docs: https://docs.ultralytics.com\n",
      "    Solutions: https://docs.ultralytics.com/solutions/\n",
      "    Community: https://community.ultralytics.com\n",
      "    GitHub: https://github.com/ultralytics/ultralytics\n",
      "    \n",
      "Training error: '\u001b[31m\u001b[1mcallbacks\u001b[0m' is not a valid YOLO argument. \n",
      "\n",
      "    Arguments received: ['yolo', '-f', '/tmp/tmpcxvyk3ad.json', '--HistoryManager.hist_file=:memory:']. Ultralytics 'yolo' commands use the following syntax:\n",
      "\n",
      "        yolo TASK MODE ARGS\n",
      "\n",
      "        Where   TASK (optional) is one of frozenset({'detect', 'obb', 'segment', 'pose', 'classify'})\n",
      "                MODE (required) is one of frozenset({'val', 'export', 'predict', 'benchmark', 'track', 'train'})\n",
      "                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n",
      "                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n",
      "\n",
      "    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n",
      "        yolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01\n",
      "\n",
      "    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n",
      "        yolo predict model=yolo11n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n",
      "\n",
      "    3. Val a pretrained detection model at batch-size 1 and image size 640:\n",
      "        yolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640\n",
      "\n",
      "    4. Export a YOLO11n classification model to ONNX format at image size 224 by 128 (no TASK required)\n",
      "        yolo export model=yolo11n-cls.pt format=onnx imgsz=224,128\n",
      "\n",
      "    5. Ultralytics solutions usage\n",
      "        yolo solutions count or in ['crop', 'blur', 'workout', 'heatmap', 'isegment', 'visioneye', 'speed', 'queue', 'analytics', 'inference', 'trackzone'] source=\"path/to/video/file.mp4\"\n",
      "\n",
      "    6. Run special commands:\n",
      "        yolo help\n",
      "        yolo checks\n",
      "        yolo version\n",
      "        yolo settings\n",
      "        yolo copy-cfg\n",
      "        yolo cfg\n",
      "        yolo solutions help\n",
      "\n",
      "    Docs: https://docs.ultralytics.com\n",
      "    Solutions: https://docs.ultralytics.com/solutions/\n",
      "    Community: https://community.ultralytics.com\n",
      "    GitHub: https://github.com/ultralytics/ultralytics\n",
      "    \n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8l.pt to 'yolov8l.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83.7M/83.7M [00:00<00:00, 205MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: '\u001b[31m\u001b[1mcallbacks\u001b[0m' is not a valid YOLO argument. \n",
      "\n",
      "    Arguments received: ['yolo', '-f', '/tmp/tmpcxvyk3ad.json', '--HistoryManager.hist_file=:memory:']. Ultralytics 'yolo' commands use the following syntax:\n",
      "\n",
      "        yolo TASK MODE ARGS\n",
      "\n",
      "        Where   TASK (optional) is one of frozenset({'detect', 'obb', 'segment', 'pose', 'classify'})\n",
      "                MODE (required) is one of frozenset({'val', 'export', 'predict', 'benchmark', 'track', 'train'})\n",
      "                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n",
      "                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n",
      "\n",
      "    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n",
      "        yolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01\n",
      "\n",
      "    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n",
      "        yolo predict model=yolo11n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n",
      "\n",
      "    3. Val a pretrained detection model at batch-size 1 and image size 640:\n",
      "        yolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640\n",
      "\n",
      "    4. Export a YOLO11n classification model to ONNX format at image size 224 by 128 (no TASK required)\n",
      "        yolo export model=yolo11n-cls.pt format=onnx imgsz=224,128\n",
      "\n",
      "    5. Ultralytics solutions usage\n",
      "        yolo solutions count or in ['crop', 'blur', 'workout', 'heatmap', 'isegment', 'visioneye', 'speed', 'queue', 'analytics', 'inference', 'trackzone'] source=\"path/to/video/file.mp4\"\n",
      "\n",
      "    6. Run special commands:\n",
      "        yolo help\n",
      "        yolo checks\n",
      "        yolo version\n",
      "        yolo settings\n",
      "        yolo copy-cfg\n",
      "        yolo cfg\n",
      "        yolo solutions help\n",
      "\n",
      "    Docs: https://docs.ultralytics.com\n",
      "    Solutions: https://docs.ultralytics.com/solutions/\n",
      "    Community: https://community.ultralytics.com\n",
      "    GitHub: https://github.com/ultralytics/ultralytics\n",
      "    \n",
      "Training error: '\u001b[31m\u001b[1mcallbacks\u001b[0m' is not a valid YOLO argument. \n",
      "\n",
      "    Arguments received: ['yolo', '-f', '/tmp/tmpcxvyk3ad.json', '--HistoryManager.hist_file=:memory:']. Ultralytics 'yolo' commands use the following syntax:\n",
      "\n",
      "        yolo TASK MODE ARGS\n",
      "\n",
      "        Where   TASK (optional) is one of frozenset({'detect', 'obb', 'segment', 'pose', 'classify'})\n",
      "                MODE (required) is one of frozenset({'val', 'export', 'predict', 'benchmark', 'track', 'train'})\n",
      "                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n",
      "                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n",
      "\n",
      "    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n",
      "        yolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01\n",
      "\n",
      "    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n",
      "        yolo predict model=yolo11n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n",
      "\n",
      "    3. Val a pretrained detection model at batch-size 1 and image size 640:\n",
      "        yolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640\n",
      "\n",
      "    4. Export a YOLO11n classification model to ONNX format at image size 224 by 128 (no TASK required)\n",
      "        yolo export model=yolo11n-cls.pt format=onnx imgsz=224,128\n",
      "\n",
      "    5. Ultralytics solutions usage\n",
      "        yolo solutions count or in ['crop', 'blur', 'workout', 'heatmap', 'isegment', 'visioneye', 'speed', 'queue', 'analytics', 'inference', 'trackzone'] source=\"path/to/video/file.mp4\"\n",
      "\n",
      "    6. Run special commands:\n",
      "        yolo help\n",
      "        yolo checks\n",
      "        yolo version\n",
      "        yolo settings\n",
      "        yolo copy-cfg\n",
      "        yolo cfg\n",
      "        yolo solutions help\n",
      "\n",
      "    Docs: https://docs.ultralytics.com\n",
      "    Solutions: https://docs.ultralytics.com/solutions/\n",
      "    Community: https://community.ultralytics.com\n",
      "    GitHub: https://github.com/ultralytics/ultralytics\n",
      "    \n",
      "Training error: '\u001b[31m\u001b[1mcallbacks\u001b[0m' is not a valid YOLO argument. \n",
      "\n",
      "    Arguments received: ['yolo', '-f', '/tmp/tmpcxvyk3ad.json', '--HistoryManager.hist_file=:memory:']. Ultralytics 'yolo' commands use the following syntax:\n",
      "\n",
      "        yolo TASK MODE ARGS\n",
      "\n",
      "        Where   TASK (optional) is one of frozenset({'detect', 'obb', 'segment', 'pose', 'classify'})\n",
      "                MODE (required) is one of frozenset({'val', 'export', 'predict', 'benchmark', 'track', 'train'})\n",
      "                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n",
      "                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n",
      "\n",
      "    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n",
      "        yolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01\n",
      "\n",
      "    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n",
      "        yolo predict model=yolo11n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n",
      "\n",
      "    3. Val a pretrained detection model at batch-size 1 and image size 640:\n",
      "        yolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640\n",
      "\n",
      "    4. Export a YOLO11n classification model to ONNX format at image size 224 by 128 (no TASK required)\n",
      "        yolo export model=yolo11n-cls.pt format=onnx imgsz=224,128\n",
      "\n",
      "    5. Ultralytics solutions usage\n",
      "        yolo solutions count or in ['crop', 'blur', 'workout', 'heatmap', 'isegment', 'visioneye', 'speed', 'queue', 'analytics', 'inference', 'trackzone'] source=\"path/to/video/file.mp4\"\n",
      "\n",
      "    6. Run special commands:\n",
      "        yolo help\n",
      "        yolo checks\n",
      "        yolo version\n",
      "        yolo settings\n",
      "        yolo copy-cfg\n",
      "        yolo cfg\n",
      "        yolo solutions help\n",
      "\n",
      "    Docs: https://docs.ultralytics.com\n",
      "    Solutions: https://docs.ultralytics.com/solutions/\n",
      "    Community: https://community.ultralytics.com\n",
      "    GitHub: https://github.com/ultralytics/ultralytics\n",
      "    \n",
      "No models loaded. Cannot generate predictions.\n",
      "Processing completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import yaml\n",
    "from ultralytics import YOLO\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "import gc\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from ultralytics.data.dataset import YOLODataset\n",
    "import torch\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "class YOLOWeightedDataset(YOLODataset):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the WeightedDataset with class balancing capabilities.\n",
    "        Additional parameters:\n",
    "            class_weights (dict, optional): Dictionary mapping class IDs to weights.\n",
    "                                           If not provided, will be calculated automatically.\n",
    "            weight_method (str): Method to use for weighting ('inverse', 'effective', 'manual').\n",
    "            effective_beta (float): Beta parameter for effective number of samples weighting.\n",
    "            aggregation (str): How to aggregate weights for multi-class images ('mean', 'max', 'sum').\n",
    "        \"\"\"\n",
    "        # Extract custom parameters before passing to parent\n",
    "        self.custom_class_weights = kwargs.pop('class_weights', None)\n",
    "        self.weight_method = kwargs.pop('weight_method', 'inverse')\n",
    "        self.effective_beta = kwargs.pop('effective_beta', 0.9)\n",
    "        self.aggregation_method = kwargs.pop('aggregation', 'mean')\n",
    "        \n",
    "        # Initialize parent class\n",
    "        super(YOLOWeightedDataset, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        # Set up aggregation function\n",
    "        if self.aggregation_method == 'mean':\n",
    "            self.agg_func = np.mean\n",
    "        elif self.aggregation_method == 'max':\n",
    "            self.agg_func = np.max\n",
    "        elif self.aggregation_method == 'sum':\n",
    "            self.agg_func = np.sum\n",
    "        else:\n",
    "            self.agg_func = np.mean\n",
    "            \n",
    "        # Count instances and calculate weights\n",
    "        self.train_mode = \"train\" in self.prefix\n",
    "        self.count_instances()\n",
    "        self.class_weights = self.calculate_class_weights()\n",
    "        self.weights = self.calculate_sample_weights()\n",
    "        self.probabilities = self.normalize_weights(self.weights)\n",
    "        \n",
    "    def count_instances(self):\n",
    "        \"\"\"Count instances of each class in the dataset\"\"\"\n",
    "        self.counts = np.zeros(len(self.data[\"names\"]))\n",
    "        self.img_with_class = [set() for _ in range(len(self.data[\"names\"]))]\n",
    "        \n",
    "        for i, label in enumerate(self.labels):\n",
    "            if label is None:\n",
    "                continue\n",
    "                \n",
    "            cls = label['cls'].reshape(-1).astype(int)\n",
    "            for class_id in cls:\n",
    "                if 0 <= class_id < len(self.counts):  # Ensure valid class id\n",
    "                    self.counts[class_id] += 1\n",
    "                    self.img_with_class[class_id].add(i)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        self.counts = np.where(self.counts == 0, 1, self.counts)\n",
    "        \n",
    "        # Calculate class distribution\n",
    "        self.class_dist = self.counts / np.sum(self.counts)\n",
    "        print(f\"Class counts: {self.counts}\")\n",
    "        print(f\"Class distribution: {self.class_dist}\")\n",
    "        \n",
    "    def calculate_class_weights(self):\n",
    "        \"\"\"Calculate weights for each class based on frequency\"\"\"\n",
    "        if self.custom_class_weights is not None:\n",
    "            # Use manually provided weights\n",
    "            weights = np.ones(len(self.data[\"names\"]))\n",
    "            for class_id, weight in self.custom_class_weights.items():\n",
    "                weights[class_id] = weight\n",
    "            return weights\n",
    "            \n",
    "        if self.weight_method == 'inverse':\n",
    "            # Inverse frequency weighting\n",
    "            weights = np.sum(self.counts) / (len(self.data[\"names\"]) * self.counts)\n",
    "        elif self.weight_method == 'effective':\n",
    "            # Effective number of samples weighting (Lin et al.)\n",
    "            # Formula: (1 - beta) / (1 - beta^n)\n",
    "            weights = (1 - self.effective_beta) / (1 - np.power(self.effective_beta, self.counts))\n",
    "        else:\n",
    "            # Balanced weighting (inverse of frequency)\n",
    "            weights = 1.0 / self.class_dist\n",
    "            \n",
    "        # Normalize weights to have mean of 1\n",
    "        weights = weights / np.mean(weights)\n",
    "        print(f\"Class weights: {weights}\")\n",
    "        return weights\n",
    "        \n",
    "    def calculate_sample_weights(self):\n",
    "        \"\"\"Calculate weight for each sample based on its class composition\"\"\"\n",
    "        weights = []\n",
    "        for i, label in enumerate(self.labels):\n",
    "            if label is None or 'cls' not in label:\n",
    "                # Give default weight to images without labels\n",
    "                weights.append(1.0)\n",
    "                continue\n",
    "                \n",
    "            cls = label['cls'].reshape(-1).astype(int)\n",
    "            if cls.size == 0:\n",
    "                weights.append(1.0)\n",
    "                continue\n",
    "                \n",
    "            # Filter out invalid class ids\n",
    "            valid_cls = cls[(cls >= 0) & (cls < len(self.class_weights))]\n",
    "            if len(valid_cls) == 0:\n",
    "                weights.append(1.0)\n",
    "                continue\n",
    "                \n",
    "            # Aggregate class weights according to selected method\n",
    "            class_weights_for_sample = self.class_weights[valid_cls]\n",
    "            weight = self.agg_func(class_weights_for_sample)\n",
    "            weights.append(weight)\n",
    "            \n",
    "        return weights\n",
    "        \n",
    "    def normalize_weights(self, weights):\n",
    "        \"\"\"Normalize weights to form a probability distribution\"\"\"\n",
    "        weights = np.array(weights)\n",
    "        if np.sum(weights) == 0:\n",
    "            return np.ones(len(weights)) / len(weights)\n",
    "        return weights / np.sum(weights)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return transformed label information based on the sampled index\"\"\"\n",
    "        if not self.train_mode:\n",
    "            # Use standard indexing for validation\n",
    "            return self.transforms(self.get_image_and_label(index))\n",
    "        else:\n",
    "            # Use weighted sampling for training\n",
    "            sampled_index = np.random.choice(len(self.labels), p=self.probabilities)\n",
    "            return self.transforms(self.get_image_and_label(sampled_index))\n",
    "\n",
    "\n",
    "def create_weighted_sampler(dataset):\n",
    "    \"\"\"Create a WeightedRandomSampler for use with DataLoader\"\"\"\n",
    "    return WeightedRandomSampler(\n",
    "        weights=dataset.weights,\n",
    "        num_samples=len(dataset),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "\n",
    "# Helper function to patch YOLOv8's build module\n",
    "def patch_yolo_dataloader():\n",
    "    import ultralytics.data.build as build\n",
    "    \n",
    "    # Store original build_yolo_dataset function\n",
    "    original_build_dataset = build.build_yolo_dataset\n",
    "    \n",
    "    # Define patched function\n",
    "    def patched_build_dataset(*args, **kwargs):\n",
    "        # Add weighted dataset parameter\n",
    "        use_weighted = kwargs.pop('weighted', False)\n",
    "        \n",
    "        # Call original function\n",
    "        dataset = original_build_dataset(*args, **kwargs)\n",
    "        \n",
    "        # Replace with weighted version if requested\n",
    "        if use_weighted and \"train\" in args[1]:\n",
    "            dataset = YOLOWeightedDataset(*args, **kwargs)\n",
    "            \n",
    "        return dataset\n",
    "    \n",
    "    # Replace original function\n",
    "    build.build_yolo_dataset = patched_build_dataset\n",
    "    \n",
    "    # Replace YOLODataset class\n",
    "    build.YOLODataset = YOLOWeightedDataset\n",
    "    \n",
    "    return \"YOLO dataloader patched with weighted sampling\"\n",
    "\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    # Data paths\n",
    "    data_dir = \"/kaggle/input/dlp-object-detection-week-10/final_dlp_data/final_dlp_data\"\n",
    "    train_images_dir = os.path.join(data_dir, \"train\", \"images\")\n",
    "    train_labels_dir = os.path.join(data_dir, \"train\", \"labels\")\n",
    "    test_images_dir = os.path.join(data_dir, \"test\", \"images\")\n",
    "    output_dir = \"outputs\"\n",
    "    yolo_data_dir = \"yolo_data\"\n",
    "    \n",
    "    # YOLO model configuration\n",
    "    yolo_models = [\"yolov8x.pt\", \"yolov8l.pt\"]\n",
    "    yolo_img_size = 1024\n",
    "    \n",
    "    # Training parameters\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    batch_size = 2\n",
    "    epochs = 30\n",
    "    folds = 3\n",
    "    \n",
    "    # Advanced training options\n",
    "    dropout = 0.1\n",
    "    freeze = 10\n",
    "    \n",
    "    # Learning rate configuration\n",
    "    lr0 = 0.01\n",
    "    lrf = 0.001\n",
    "    weight_decay = 0.0005\n",
    "    \n",
    "    # Inference parameters\n",
    "    confidence_threshold = 0.05\n",
    "    iou_threshold = 0.45\n",
    "    ensemble_conf_threshold = 0.25\n",
    "    \n",
    "    # Test-time augmentation\n",
    "    tta = True\n",
    "    tta_scales = [0.8, 1.0, 1.2]\n",
    "    tta_flips = [None, 'horizontal']\n",
    "    \n",
    "    # Class mapping - based on dataset analysis\n",
    "    class_names = {\n",
    "        0: \"aegypti\", 1: \"albopictus\", 2: \"anopheles\", \n",
    "        3: \"culex\", 4: \"culiseta\", 5: \"japonicus/koreicus\"\n",
    "    }\n",
    "\n",
    "    class_weights = {\n",
    "        0: 10.0,  # aegypti (very rare)\n",
    "        1: 1.0,  # albopictus (common)\n",
    "        2: 8.0,  # anopheles (very rare)\n",
    "        3: 1.0,  # culex (common)\n",
    "        4: 2.5,  # culiseta (uncommon)\n",
    "        5: 3.0  # japonicus/koreicus (uncommon)\n",
    "    }\n",
    "\n",
    "    # Class weighting parameters\n",
    "    use_weighted_dataset = True\n",
    "    weight_method = 'effective'  # 'inverse', 'effective', or 'manual'\n",
    "    effective_beta = 0.9  # For effective number of samples method\n",
    "    weight_aggregation = 'mean'  # 'mean', 'max', or 'sum'\n",
    "\n",
    "    # Additional augmentation for rare classes\n",
    "    augment_rare_classes = True\n",
    "    rare_class_ids = [0, 2]  # aegypti and anopheles\n",
    "    rare_augmentation_factor = 2.0  # Increase augmentation for rare classes\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(Config.output_dir, exist_ok=True)\n",
    "os.makedirs(Config.yolo_data_dir, exist_ok=True)\n",
    "\n",
    "def analyze_dataset():\n",
    "    \"\"\"Basic dataset analysis without logging\"\"\"\n",
    "    label_files = glob(os.path.join(Config.train_labels_dir, \"*.txt\"))\n",
    "    class_counts = {i: 0 for i in range(len(Config.class_names))}\n",
    "    \n",
    "    for label_file in tqdm(label_files, desc=\"Analyzing dataset\"):\n",
    "        img_file = os.path.join(\n",
    "            Config.train_images_dir, \n",
    "            os.path.basename(label_file).replace(\".txt\", \".jpg\")\n",
    "        )\n",
    "        \n",
    "        if os.path.exists(img_file):\n",
    "            # Class counts only\n",
    "            try:\n",
    "                with open(label_file, 'r') as f:\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) >= 5:\n",
    "                            class_id = int(parts[0])\n",
    "                            class_counts[class_id] += 1\n",
    "            except:\n",
    "                # Skip malformed label files\n",
    "                continue\n",
    "    \n",
    "    return class_counts\n",
    "\n",
    "def validate_label_file(label_path):\n",
    "    \"\"\"Validate and fix label file if needed\"\"\"\n",
    "    valid_lines = []\n",
    "    try:\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    try:\n",
    "                        class_id = int(parts[0])\n",
    "                        x_center = float(parts[1])\n",
    "                        y_center = float(parts[2])\n",
    "                        width = float(parts[3])\n",
    "                        height = float(parts[4])\n",
    "                        \n",
    "                        # Validate values are in range [0, 1]\n",
    "                        if (0 <= class_id < len(Config.class_names) and\n",
    "                            0 <= x_center <= 1 and 0 <= y_center <= 1 and\n",
    "                            0 <= width <= 1 and 0 <= height <= 1):\n",
    "                            valid_lines.append(line)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "        \n",
    "        # If file was modified, write back valid lines\n",
    "        if len(valid_lines) > 0:\n",
    "            with open(label_path, 'w') as f:\n",
    "                f.writelines(valid_lines)\n",
    "            return True\n",
    "        return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def prepare_stratified_folds():\n",
    "    \"\"\"Prepare dataset with stratified k-fold splitting based on class distribution\"\"\"\n",
    "    # Get all image and label files\n",
    "    label_files = glob(os.path.join(Config.train_labels_dir, \"*.txt\"))\n",
    "    \n",
    "    # Extract class distribution per image for stratification\n",
    "    image_data = []\n",
    "    \n",
    "    for label_file in tqdm(label_files, desc=\"Preparing stratification\"):\n",
    "        img_file = os.path.basename(label_file).replace(\".txt\", \".jpg\")\n",
    "        img_path = os.path.join(Config.train_images_dir, img_file)\n",
    "        \n",
    "        if os.path.exists(img_path):\n",
    "            # Validate and fix label file if needed\n",
    "            if not validate_label_file(label_file):\n",
    "                continue\n",
    "                \n",
    "            # Count classes in this image\n",
    "            class_counts = [0] * len(Config.class_names)\n",
    "            try:\n",
    "                with open(label_file, 'r') as f:\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) >= 5:\n",
    "                            class_id = int(parts[0])\n",
    "                            if 0 <= class_id < len(Config.class_names):\n",
    "                                class_counts[class_id] += 1\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            # Determine primary class (most frequent)\n",
    "            primary_class = np.argmax(class_counts) if sum(class_counts) > 0 else 0\n",
    "            image_data.append({\n",
    "                'image': img_file,\n",
    "                'label': os.path.basename(label_file),\n",
    "                'primary_class': primary_class,\n",
    "                'total_objects': sum(class_counts)\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame for easier manipulation\n",
    "    df = pd.DataFrame(image_data)\n",
    "    \n",
    "    # Debug: Check if DataFrame is properly created and has the expected columns\n",
    "    print(f\"DataFrame shape: {df.shape}\")\n",
    "    print(f\"DataFrame columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Check if primary_class column exists and has values\n",
    "    if 'primary_class' not in df.columns or df.empty:\n",
    "        print(\"Warning: 'primary_class' column missing or DataFrame is empty. Using basic fold splitting.\")\n",
    "        # Fallback: Create basic folds without stratification\n",
    "        total_images = len(image_data)\n",
    "        for fold in range(Config.folds):\n",
    "            # Create fold directories\n",
    "            fold_dir = os.path.join(Config.yolo_data_dir, f\"fold_{fold}\")\n",
    "            os.makedirs(os.path.join(fold_dir, \"images\", \"train\"), exist_ok=True)\n",
    "            os.makedirs(os.path.join(fold_dir, \"images\", \"val\"), exist_ok=True)\n",
    "            os.makedirs(os.path.join(fold_dir, \"labels\", \"train\"), exist_ok=True)\n",
    "            os.makedirs(os.path.join(fold_dir, \"labels\", \"val\"), exist_ok=True)\n",
    "            \n",
    "            # Simple split based on fold index\n",
    "            val_indices = np.arange(fold * (total_images // Config.folds), \n",
    "                                   (fold + 1) * (total_images // Config.folds))\n",
    "            \n",
    "            for i, item in enumerate(image_data):\n",
    "                is_val = i in val_indices\n",
    "                \n",
    "                # Determine destination folders\n",
    "                img_dest_dir = os.path.join(fold_dir, \"images\", \"val\" if is_val else \"train\")\n",
    "                label_dest_dir = os.path.join(fold_dir, \"labels\", \"val\" if is_val else \"train\")\n",
    "                \n",
    "                # Copy files\n",
    "                shutil.copy(\n",
    "                    os.path.join(Config.train_images_dir, item['image']),\n",
    "                    os.path.join(img_dest_dir, item['image'])\n",
    "                )\n",
    "                shutil.copy(\n",
    "                    os.path.join(Config.train_labels_dir, item['label']),\n",
    "                    os.path.join(label_dest_dir, item['label'])\n",
    "                )\n",
    "            \n",
    "            # Create YAML config\n",
    "            yaml_content = {\n",
    "                'path': os.path.abspath(fold_dir),\n",
    "                'train': 'images/train',\n",
    "                'val': 'images/val',\n",
    "                'names': Config.class_names,\n",
    "                'nc': len(Config.class_names)\n",
    "            }\n",
    "            \n",
    "            with open(os.path.join(fold_dir, 'data.yaml'), 'w') as f:\n",
    "                yaml.dump(yaml_content, f, default_flow_style=False)\n",
    "        \n",
    "        return Config.folds\n",
    "    \n",
    "    # Setup k-fold cross-validation with stratification by primary class\n",
    "    skf = StratifiedKFold(n_splits=Config.folds, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    # Create YAML config for each fold\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['primary_class'])):\n",
    "        # Create fold directories\n",
    "        fold_dir = os.path.join(Config.yolo_data_dir, f\"fold_{fold}\")\n",
    "        os.makedirs(os.path.join(fold_dir, \"images\", \"train\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(fold_dir, \"images\", \"val\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(fold_dir, \"labels\", \"train\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(fold_dir, \"labels\", \"val\"), exist_ok=True)\n",
    "        \n",
    "        # Split data\n",
    "        train_df = df.iloc[train_idx]\n",
    "        val_df = df.iloc[val_idx]\n",
    "        \n",
    "        # Copy files\n",
    "        for _, row in tqdm(train_df.iterrows(), desc=f\"Copying fold {fold} train files\", total=len(train_df)):\n",
    "            shutil.copy(\n",
    "                os.path.join(Config.train_images_dir, row['image']),\n",
    "                os.path.join(fold_dir, \"images\", \"train\", row['image'])\n",
    "            )\n",
    "            shutil.copy(\n",
    "                os.path.join(Config.train_labels_dir, row['label']),\n",
    "                os.path.join(fold_dir, \"labels\", \"train\", row['label'])\n",
    "            )\n",
    "        \n",
    "        for _, row in tqdm(val_df.iterrows(), desc=f\"Copying fold {fold} val files\", total=len(val_df)):\n",
    "            shutil.copy(\n",
    "                os.path.join(Config.train_images_dir, row['image']),\n",
    "                os.path.join(fold_dir, \"images\", \"val\", row['image'])\n",
    "            )\n",
    "            shutil.copy(\n",
    "                os.path.join(Config.train_labels_dir, row['label']),\n",
    "                os.path.join(fold_dir, \"labels\", \"val\", row['label'])\n",
    "            )\n",
    "        \n",
    "        # Create YAML config\n",
    "        yaml_content = {\n",
    "            'path': os.path.abspath(fold_dir),\n",
    "            'train': 'images/train',\n",
    "            'val': 'images/val',\n",
    "            'names': Config.class_names,\n",
    "            'nc': len(Config.class_names)\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(fold_dir, 'data.yaml'), 'w') as f:\n",
    "            yaml.dump(yaml_content, f, default_flow_style=False)\n",
    "    \n",
    "    return Config.folds\n",
    "\n",
    "\n",
    "# Function to apply the weighted dataset patch\n",
    "def setup_weighted_dataloading():\n",
    "    # Patch the YOLO dataloader to use our weighted dataset\n",
    "    try:\n",
    "        import ultralytics.data.build as build\n",
    "        from ultralytics.data.dataset import YOLODataset\n",
    "\n",
    "        # Save original build function\n",
    "        original_build_dataset = build.build_yolo_dataset\n",
    "\n",
    "        # Define the patched function\n",
    "        def patched_build_dataset(cfg, path, mode='train', batch=None, **kwargs):\n",
    "            # Get the usual dataset first\n",
    "            dataset = original_build_dataset(cfg, path, mode, batch, **kwargs)\n",
    "\n",
    "            # For training, replace with weighted dataset\n",
    "            if Config.use_weighted_dataset and 'train' in mode:\n",
    "                # Replace with our weighted version\n",
    "                dataset = YOLOWeightedDataset(\n",
    "                    cfg=cfg,\n",
    "                    path=path,\n",
    "                    mode=mode,\n",
    "                    batch=batch,\n",
    "                    class_weights=Config.class_weights,\n",
    "                    weight_method=Config.weight_method,\n",
    "                    effective_beta=Config.effective_beta,\n",
    "                    aggregation=Config.weight_aggregation,\n",
    "                    **kwargs\n",
    "                )\n",
    "                print(f\"Using weighted dataset for {mode} with {Config.weight_method} weighting\")\n",
    "\n",
    "            return dataset\n",
    "\n",
    "        # Replace the build function\n",
    "        build.build_yolo_dataset = patched_build_dataset\n",
    "\n",
    "        print(\"Successfully set up weighted dataset for training\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to set up weighted dataset: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Modified train_yolo_models function\n",
    "def train_yolo_models():\n",
    "    \"\"\"Train YOLOv8 models with cross-validation and weighted sampling\"\"\"\n",
    "    # Apply weighted dataset patch\n",
    "    setup_weighted_dataloading()\n",
    "\n",
    "    models = []\n",
    "\n",
    "    # Loop through models and folds\n",
    "    for model_path in Config.yolo_models:\n",
    "        for fold in range(Config.folds):\n",
    "            fold_dir = os.path.join(Config.yolo_data_dir, f\"fold_{fold}\")\n",
    "            data_yaml_path = os.path.join(fold_dir, 'data.yaml')\n",
    "\n",
    "            # Load model\n",
    "            model = YOLO(model_path)\n",
    "\n",
    "            # Get class names from the YAML file\n",
    "            with open(data_yaml_path, 'r') as f:\n",
    "                data_yaml = yaml.safe_load(f)\n",
    "\n",
    "            # Create callback for monitoring rare classes\n",
    "            class MonitorRareClasses:\n",
    "                def __init__(self, rare_class_ids):\n",
    "                    self.rare_class_ids = rare_class_ids\n",
    "                    self.best_rare_map = 0\n",
    "\n",
    "                def on_val_end(self, trainer):\n",
    "                    # Extract metrics for rare classes\n",
    "                    metrics = trainer.metrics\n",
    "                    if hasattr(metrics, 'ap_class'):\n",
    "                        rare_maps = [metrics.ap_class[i] for i in self.rare_class_ids if i < len(metrics.ap_class)]\n",
    "                        if rare_maps:\n",
    "                            rare_map = sum(rare_maps) / len(rare_maps)\n",
    "                            if rare_map > self.best_rare_map:\n",
    "                                self.best_rare_map = rare_map\n",
    "                                print(f\"\\nNew best rare class mAP: {rare_map:.4f}\")\n",
    "                                # Save a special checkpoint for best rare class performance\n",
    "                                model_name = f\"{trainer.args.project}/{trainer.args.name}/weights/best_rare_classes.pt\"\n",
    "                                trainer.model.save(model_name)\n",
    "\n",
    "            # Create monitor callback\n",
    "            rare_monitor = MonitorRareClasses(Config.rare_class_ids)\n",
    "\n",
    "            # Train with custom hyperparameters\n",
    "            try:\n",
    "                model.train(\n",
    "                    data=data_yaml_path,\n",
    "                    epochs=Config.epochs,\n",
    "                    imgsz=Config.yolo_img_size,\n",
    "                    batch=Config.batch_size,\n",
    "                    dropout=Config.dropout,\n",
    "                    freeze=Config.freeze,\n",
    "                    name=f\"{Path(model_path).stem}_fold{fold}\",\n",
    "                    project=Config.output_dir,\n",
    "                    pretrained=True,\n",
    "                    optimizer='AdamW',\n",
    "                    lr0=Config.lr0,\n",
    "                    lrf=Config.lrf,\n",
    "                    weight_decay=Config.weight_decay,\n",
    "                    warmup_epochs=5,\n",
    "                    warmup_momentum=0.8,\n",
    "                    box=7.5,\n",
    "                    cls=0.5,\n",
    "                    dfl=1.5,\n",
    "                    hsv_h=0.015,\n",
    "                    hsv_s=0.8,\n",
    "                    hsv_v=0.5,\n",
    "                    degrees=15,\n",
    "                    translate=0.2,\n",
    "                    scale=0.6,\n",
    "                    fliplr=0.5,\n",
    "                    flipud=0.2,\n",
    "                    mosaic=1.0,\n",
    "                    mixup=0.3,\n",
    "                    copy_paste=0.3,\n",
    "                    auto_augment='randaugment',\n",
    "                    cache=True,\n",
    "                    rect=False,\n",
    "                    cos_lr=True,\n",
    "                    close_mosaic=5,\n",
    "                    amp=True,\n",
    "                    overlap_mask=True,\n",
    "                    multi_scale=True,\n",
    "                    # No need for class_weights parameter as we're handling it in the dataset\n",
    "                    callbacks=[rare_monitor]\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Training error: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Get best models for ensemble\n",
    "            best_model_path = os.path.join(\n",
    "                Config.output_dir,\n",
    "                f\"{Path(model_path).stem}_fold{fold}\",\n",
    "                \"weights\",\n",
    "                \"best.pt\"\n",
    "            )\n",
    "\n",
    "            best_rare_path = os.path.join(\n",
    "                Config.output_dir,\n",
    "                f\"{Path(model_path).stem}_fold{fold}\",\n",
    "                \"weights\",\n",
    "                \"best_rare_classes.pt\"\n",
    "            )\n",
    "\n",
    "            if os.path.exists(best_model_path):\n",
    "                models.append(best_model_path)\n",
    "\n",
    "            if os.path.exists(best_rare_path):\n",
    "                models.append(best_rare_path)\n",
    "\n",
    "            # Clean up GPU memory\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    return models\n",
    "\n",
    "# Enhanced prediction function to favor rare classes when confidence is close\n",
    "def ensemble_predictions(predictions, img_shape):\n",
    "    \"\"\"Ensemble multiple predictions with weighted box fusion and rare class preference\"\"\"\n",
    "    try:\n",
    "        from ensemble_boxes import weighted_boxes_fusion\n",
    "    except ImportError:\n",
    "        print(\"ensemble_boxes not installed. Installing...\")\n",
    "        import pip\n",
    "        pip.main(['install', 'ensemble-boxes'])\n",
    "        from ensemble_boxes import weighted_boxes_fusion\n",
    "\n",
    "    boxes_list = []\n",
    "    scores_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    h, w = img_shape[:2]\n",
    "\n",
    "    # Extract boxes, scores, and labels from each prediction\n",
    "    for pred in predictions:\n",
    "        if len(pred.boxes) > 0:\n",
    "            # Get normalized xyxy boxes\n",
    "            boxes = pred.boxes.xyxyn.cpu().numpy()\n",
    "            scores = pred.boxes.conf.cpu().numpy()\n",
    "            labels = pred.boxes.cls.cpu().numpy().astype(int)\n",
    "\n",
    "            # Apply score boosting for rare classes\n",
    "            for i, label in enumerate(labels):\n",
    "                if label in Config.rare_class_ids:\n",
    "                    # Boost the confidence of rare classes\n",
    "                    scores[i] = min(1.0, scores[i] * 1.1)  # 10% boost but cap at 1.0\n",
    "\n",
    "            boxes_list.append(boxes)\n",
    "            scores_list.append(scores)\n",
    "            labels_list.append(labels)\n",
    "        else:\n",
    "            # Add empty lists if no detections\n",
    "            boxes_list.append([])\n",
    "            scores_list.append([])\n",
    "            labels_list.append([])\n",
    "\n",
    "    # Skip fusion if no boxes\n",
    "    if not any(boxes_list):\n",
    "        return [], [], []\n",
    "\n",
    "    # Apply weighted box fusion\n",
    "    try:\n",
    "        # Use higher weights for predictions from models optimized for rare classes\n",
    "        weights = None\n",
    "        if len(boxes_list) > 1:\n",
    "            weights = [1.0] * len(boxes_list)\n",
    "            for i, model_path in enumerate(model_paths):\n",
    "                if \"best_rare_classes\" in model_path:\n",
    "                    weights[i] = 1.5  # Higher weight for rare-class-optimized models\n",
    "\n",
    "        boxes, scores, labels = weighted_boxes_fusion(\n",
    "            boxes_list, scores_list, labels_list,\n",
    "            weights=weights,\n",
    "            iou_thr=Config.iou_threshold,\n",
    "            skip_box_thr=Config.confidence_threshold\n",
    "        )\n",
    "\n",
    "        # Denormalize boxes to get pixel coordinates\n",
    "        boxes[:, [0, 2]] *= w\n",
    "        boxes[:, [1, 3]] *= h\n",
    "\n",
    "        return boxes, scores, labels\n",
    "    except Exception as e:\n",
    "        print(f\"Box fusion error: {e}\")\n",
    "        # Fallback to first prediction if fusion fails\n",
    "        if boxes_list and boxes_list[0]:\n",
    "            return (boxes_list[0] * np.array([[w, h, w, h]])), scores_list[0], labels_list[0]\n",
    "        return [], [], []\n",
    "\n",
    "def apply_tta(model, img, scales, flips):\n",
    "    \"\"\"Apply test-time augmentation with various scales and flips\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    # Handle PIL decompression bomb warning by resizing large images\n",
    "    h, w = img.shape[:2]\n",
    "    max_pixels = 89000000  # Just under the PIL limit\n",
    "    \n",
    "    if h * w > max_pixels:\n",
    "        scale_factor = np.sqrt(max_pixels / (h * w))\n",
    "        h_new, w_new = int(h * scale_factor), int(w * scale_factor)\n",
    "        img = cv2.resize(img, (w_new, h_new))\n",
    "        h, w = h_new, w_new\n",
    "    \n",
    "    for scale in scales:\n",
    "        # Scale the image\n",
    "        new_h, new_w = int(h * scale), int(w * scale)\n",
    "        scaled_img = cv2.resize(img, (new_w, new_h))\n",
    "        \n",
    "        for flip in flips:\n",
    "            test_img = scaled_img.copy()\n",
    "            \n",
    "            # Apply flip if needed\n",
    "            if flip == 'horizontal':\n",
    "                test_img = cv2.flip(test_img, 1)\n",
    "            \n",
    "            # Run prediction\n",
    "            try:\n",
    "                pred = model.predict(test_img, conf=Config.confidence_threshold, iou=Config.iou_threshold, verbose=False)[0]\n",
    "                \n",
    "                # If horizontal flip, adjust the predictions\n",
    "                if flip == 'horizontal' and len(pred.boxes) > 0:\n",
    "                    # Get xyxy format (non-normalized)\n",
    "                    boxes = pred.boxes.xyxy.cpu().numpy()\n",
    "                    for box in boxes:\n",
    "                        # Flip x-coordinates\n",
    "                        box[0], box[2] = new_w - box[2], new_w - box[0]\n",
    "                \n",
    "                # Store prediction\n",
    "                predictions.append(pred)\n",
    "            except Exception as e:\n",
    "                print(f\"Prediction error: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def generate_predictions(model_paths, test_dir):\n",
    "    \"\"\"Generate predictions using ensemble of models with TTA\"\"\"\n",
    "    # Load models\n",
    "    models = []\n",
    "    for path in model_paths:\n",
    "        try:\n",
    "            models.append(YOLO(path))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {path}: {e}\")\n",
    "    \n",
    "    if not models:\n",
    "        print(\"No models loaded. Cannot generate predictions.\")\n",
    "        return\n",
    "    \n",
    "    # Get test images\n",
    "    test_images = glob(os.path.join(test_dir, \"*.jpg\"))\n",
    "    \n",
    "    all_predictions = []\n",
    "    \n",
    "    for img_path in tqdm(test_images, desc=\"Generating predictions\"):\n",
    "        img_id = os.path.basename(img_path)\n",
    "        \n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                raise ValueError(f\"Failed to load image: {img_path}\")\n",
    "            \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            h, w = img.shape[:2]\n",
    "            \n",
    "            all_model_predictions = []\n",
    "            \n",
    "            # Process each model\n",
    "            for model in models:\n",
    "                if Config.tta:\n",
    "                    # Apply test-time augmentation\n",
    "                    preds = apply_tta(model, img, Config.tta_scales, Config.tta_flips)\n",
    "                    all_model_predictions.extend(preds)\n",
    "                else:\n",
    "                    # Regular prediction\n",
    "                    try:\n",
    "                        pred = model.predict(img, conf=Config.confidence_threshold, iou=Config.iou_threshold, verbose=False)[0]\n",
    "                        all_model_predictions.append(pred)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Prediction error on {img_id}: {e}\")\n",
    "            \n",
    "            # Ensemble predictions if multiple\n",
    "            if len(all_model_predictions) > 1:\n",
    "                boxes, scores, labels = ensemble_predictions(all_model_predictions, img.shape)\n",
    "            elif len(all_model_predictions) == 1:\n",
    "                # Just one prediction, use it directly\n",
    "                pred = all_model_predictions[0]\n",
    "                if len(pred.boxes) > 0:\n",
    "                    boxes = pred.boxes.xyxy.cpu().numpy()\n",
    "                    scores = pred.boxes.conf.cpu().numpy()\n",
    "                    labels = pred.boxes.cls.cpu().numpy().astype(int)\n",
    "                    \n",
    "                    # Filter by final confidence threshold\n",
    "                    mask = scores >= Config.ensemble_conf_threshold\n",
    "                    boxes = boxes[mask]\n",
    "                    scores = scores[mask]\n",
    "                    labels = labels[mask]\n",
    "                else:\n",
    "                    boxes, scores, labels = [], [], []\n",
    "            else:\n",
    "                boxes, scores, labels = [], [], []\n",
    "            \n",
    "            # Convert to submission format\n",
    "            if len(boxes) > 0:\n",
    "                for box, score, label in zip(boxes, scores, labels):\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    \n",
    "                    # Convert to center, width, height and normalize\n",
    "                    x_center = ((x1 + x2) / 2) / w\n",
    "                    y_center = ((y1 + y2) / 2) / h\n",
    "                    width = (x2 - x1) / w\n",
    "                    height = (y2 - y1) / h\n",
    "                    \n",
    "                    # Ensure values are between 0 and 1\n",
    "                    x_center = max(0, min(1, x_center))\n",
    "                    y_center = max(0, min(1, y_center))\n",
    "                    width = max(0, min(1, width))\n",
    "                    height = max(0, min(1, height))\n",
    "                    \n",
    "                    # Ensure label is valid\n",
    "                    if label >= 0 and label < len(Config.class_names):\n",
    "                        class_name = Config.class_names.get(label, \"Unknown\")\n",
    "                        \n",
    "                        all_predictions.append({\n",
    "                            'ImageID': img_id,\n",
    "                            'LabelName': class_name,\n",
    "                            'Conf': float(score),\n",
    "                            'xcenter': float(x_center),\n",
    "                            'ycenter': float(y_center),\n",
    "                            'bbx_width': float(width),\n",
    "                            'bbx_height': float(height)\n",
    "                        })\n",
    "            else:\n",
    "                # Default predictions for empty images - based on class distribution\n",
    "                # Add one prediction for each of the two most common classes\n",
    "                common_classes = [\"albopictus\", \"culex\"]\n",
    "                \n",
    "                for idx, class_name in enumerate(common_classes):\n",
    "                    # Create different positions for the default boxes\n",
    "                    x_center = 0.4 + (idx * 0.2)\n",
    "                    y_center = 0.4 + (idx * 0.2)\n",
    "                    \n",
    "                    all_predictions.append({\n",
    "                        'ImageID': img_id,\n",
    "                        'LabelName': class_name,\n",
    "                        'Conf': float(Config.confidence_threshold),\n",
    "                        'xcenter': x_center,\n",
    "                        'ycenter': y_center,\n",
    "                        'bbx_width': 0.3,\n",
    "                        'bbx_height': 0.3\n",
    "                    })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_id}: {e}\")\n",
    "            # Add fallback prediction\n",
    "            all_predictions.append({\n",
    "                'ImageID': img_id,\n",
    "                'LabelName': \"albopictus\",  # Most common class\n",
    "                'Conf': float(Config.confidence_threshold),\n",
    "                'xcenter': 0.5,\n",
    "                'ycenter': 0.5,\n",
    "                'bbx_width': 0.3,\n",
    "                'bbx_height': 0.3\n",
    "            })\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission_df = pd.DataFrame(all_predictions)\n",
    "    submission_df.insert(0, 'id', range(len(submission_df)))\n",
    "    \n",
    "    # Save submission file\n",
    "    submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "def main():\n",
    "    # Analyze dataset - basic analysis only, no logging\n",
    "    analyze_dataset()\n",
    "    \n",
    "    # Prepare stratified folds\n",
    "    prepare_stratified_folds()\n",
    "    \n",
    "    # Train models\n",
    "    model_paths = train_yolo_models()\n",
    "    \n",
    "    # Generate ensemble predictions\n",
    "    generate_predictions(model_paths, Config.test_images_dir)\n",
    "    \n",
    "    print(\"Processing completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11404671,
     "sourceId": 96038,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 32.212306,
   "end_time": "2025-03-23T18:39:51.963436",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-23T18:39:19.751130",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0c7f19d545d840cf85df0523c1741e28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "0f5ba27d787743ddb2ba0a27b48a1747": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2470286ee18f4df19b9c9c89d29db5f1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "251f856ee940459c98270cdc765662d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_793c751909ea40cc9351475aa1b26d2a",
       "placeholder": "​",
       "style": "IPY_MODEL_e70d89c0e5584d00b2373623302f44d7",
       "tabbable": null,
       "tooltip": null,
       "value": " 7500/7500 [00:04&lt;00:00, 1750.78it/s]"
      }
     },
     "3c23d100659a406baa9e0691cdf46d14": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5bdaf39932464737a244224384a69828": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6a448e8ba1014b569e5138db407da4d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "793c751909ea40cc9351475aa1b26d2a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9ece509052764cce88a3ab955964a75a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_cff4addd7a2446499c531191dd381bd2",
       "placeholder": "​",
       "style": "IPY_MODEL_ca3132702a8d4b9db0fbd5c681013241",
       "tabbable": null,
       "tooltip": null,
       "value": " 7500/7500 [00:00&lt;00:00, 124367.55it/s]"
      }
     },
     "a65b3637f28d4732a616ee8e9902ebdd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5bdaf39932464737a244224384a69828",
       "max": 7500.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_cc24632c6bdb489baefb99a9bce4f70b",
       "tabbable": null,
       "tooltip": null,
       "value": 7500.0
      }
     },
     "b78f12d1a7f24badb80ebf6e03b66b20": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3c23d100659a406baa9e0691cdf46d14",
       "placeholder": "​",
       "style": "IPY_MODEL_0f5ba27d787743ddb2ba0a27b48a1747",
       "tabbable": null,
       "tooltip": null,
       "value": "Analyzing dataset: 100%"
      }
     },
     "c1fc954d0d954398a76056deda269a47": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c472b1c79021436cab5b65a32fd9adbe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_db4bd5595b2d47528f94b44504cf92e4",
        "IPY_MODEL_a65b3637f28d4732a616ee8e9902ebdd",
        "IPY_MODEL_9ece509052764cce88a3ab955964a75a"
       ],
       "layout": "IPY_MODEL_2470286ee18f4df19b9c9c89d29db5f1",
       "tabbable": null,
       "tooltip": null
      }
     },
     "c5689441bc214ab69b54d24f9301c2de": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ca3132702a8d4b9db0fbd5c681013241": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "cc24632c6bdb489baefb99a9bce4f70b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "cff4addd7a2446499c531191dd381bd2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "db4bd5595b2d47528f94b44504cf92e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_df460d08fccc44a0894eeed53e89cdd4",
       "placeholder": "​",
       "style": "IPY_MODEL_6a448e8ba1014b569e5138db407da4d9",
       "tabbable": null,
       "tooltip": null,
       "value": "Preparing stratification: 100%"
      }
     },
     "df460d08fccc44a0894eeed53e89cdd4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e231b92fe1a44710ad93d69d5a501dca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c5689441bc214ab69b54d24f9301c2de",
       "max": 7500.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0c7f19d545d840cf85df0523c1741e28",
       "tabbable": null,
       "tooltip": null,
       "value": 7500.0
      }
     },
     "e70d89c0e5584d00b2373623302f44d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ef4eeaf514f34ca1bb950d617649fe60": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b78f12d1a7f24badb80ebf6e03b66b20",
        "IPY_MODEL_e231b92fe1a44710ad93d69d5a501dca",
        "IPY_MODEL_251f856ee940459c98270cdc765662d9"
       ],
       "layout": "IPY_MODEL_c1fc954d0d954398a76056deda269a47",
       "tabbable": null,
       "tooltip": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
