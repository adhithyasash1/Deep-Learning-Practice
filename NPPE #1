{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":93282,"databundleVersionId":11098970,"sourceType":"competition"},{"sourceId":79488,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":66780,"modelId":91102},{"sourceId":104449,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":68809,"modelId":91102}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"!pip install -U bitsandbytes trl peft\n\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n# Hugging Face imports\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM, \n    TrainingArguments, \n    Trainer,\n    BitsAndBytesConfig\n)\n\n# PEFT for LoRA and Adapters\nfrom peft import LoraConfig, get_peft_model\n\nfrom transformers.trainer_utils import get_last_checkpoint\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Helpers","metadata":{}},{"cell_type":"code","source":"'''\nimport shutil\nfrom IPython.display import FileLink, display\n\n# Zip the submission file\nshutil.make_archive(\"submission1_5_5\", \"zip\", root_dir=\"/kaggle/working\", base_dir=\"submission1_5_5.csv\")\n\n# Display a clickable download link for the zip file\ndisplay(FileLink(\"submission2.zip\"))\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nimport torch\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\n\n# Use a smaller batch size if memory is an issue.\neval_dataloader = DataLoader(eval_dataset, batch_size=2, collate_fn=collate_fn)\n\nmodel.eval()\npred_labels = []\ntrue_labels = []\n\nwith torch.no_grad():\n    for batch in eval_dataloader:\n        input_ids = batch[\"input_ids\"].to(model.device)\n        attention_mask = batch[\"attention_mask\"].to(model.device)\n        generated_ids = model.generate(\n            input_ids,\n            attention_mask=attention_mask,\n            max_new_tokens=10,\n            do_sample=False,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n        for gen_ids, true_label in zip(generated_ids, batch[\"true_label\"]):\n            decoded = tokenizer.decode(gen_ids, skip_special_tokens=True)\n            # Extract the predicted sentiment in a simple way.\n            if \"Answer:\" in decoded:\n                pred = decoded.split(\"Answer:\")[-1].strip().split()[0]\n            else:\n                pred = \"Negative\"\n            # Map to expected labels.\n            pred_labels.append(\"Positive\" if pred.lower().startswith(\"pos\") else \"Negative\")\n            true_labels.append(true_label)\n\n# Compute the F1 score using a weighted average.\nf1 = f1_score(true_labels, pred_labels, average=\"weighted\")\nprint(\"F1 Score:\", f1)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Code 1","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n# Hugging Face imports\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    EarlyStoppingCallback,\n    Trainer,\n    TrainingArguments,\n)\nfrom transformers.trainer_utils import get_last_checkpoint\n\n# PEFT for LoRA and Adapters\nfrom peft import LoraConfig, get_peft_model\n\n# For metrics\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n\n# -------------------------------\n# 1. Setup: Load Tokenizer and Model (Memory‑Efficient with Quantization & LoRA)\n# -------------------------------\nmodel_name_or_path = \"/kaggle/input/llama-3.1/transformers/8b-instruct/2\"\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, local_files_only=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name_or_path,\n    quantization_config=quantization_config,\n    device_map=\"auto\",\n    local_files_only=True,\n    trust_remote_code=True,\n)\n\nmodel.gradient_checkpointing_enable()\nmodel.enable_input_require_grads()  # Ensure proper gradient flow\nmodel.config.use_cache = False\n\n# Optionally adjust model dropout (if supported)\nmodel.config.hidden_dropout_prob = 0.1\nmodel.config.attention_dropout = 0.1\n\n# -------------------------------\n# 2. Configure LoRA for Fine‑Tuning (with dropout added)\n# -------------------------------\nlora_config = LoraConfig(\n    r=128,                     # LoRA rank\n    lora_alpha=16,           # Scaling factor\n    lora_dropout=0.1,        # Dropout in LoRA layers\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    bias=\"none\",\n    use_rslora=True,\n    task_type=\"CAUSAL_LM\",\n)\nmodel = get_peft_model(model, lora_config)\nprint(\"LoRA-enabled model. Trainable parameters:\")\nmodel.print_trainable_parameters()\n\n# -------------------------------\n# 3. Create Custom Dataset with Efficient Prompting\n# -------------------------------\n# For evaluation, we include the true label for metric computation.\nclass SentimentDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length=512, is_train=True, include_true_label=False):\n        self.df = df.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.is_train = is_train\n        self.include_true_label = include_true_label\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        sentence = row[\"sentence\"]\n        language = row[\"language\"]\n        prompt = f\"Classify the sentiment of the following text in {language}: \\\"{sentence}\\\".\\nAnswer:\"\n        \n        if self.is_train:\n            label_text = row[\"label\"]  # Expected to be \"Positive\" or \"Negative\"\n            full_input = prompt + \" \" + label_text\n            tokenized_full = self.tokenizer(\n                full_input,\n                truncation=True,\n                max_length=self.max_length,\n                return_tensors=\"pt\",\n                return_attention_mask=True\n            )\n            tokenized_prompt = self.tokenizer(\n                prompt,\n                truncation=True,\n                max_length=self.max_length,\n                return_tensors=\"pt\"\n            )\n            input_ids = tokenized_full.input_ids[0]\n            attention_mask = tokenized_full.attention_mask[0]\n            labels = input_ids.clone()\n            prompt_len = tokenized_prompt.input_ids.shape[1]\n            labels[:prompt_len] = -100  # Mask the prompt tokens.\n            item = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n            if self.include_true_label:\n                item[\"true_label\"] = label_text\n            return item\n        else:\n            tokenized = self.tokenizer(\n                prompt,\n                truncation=True,\n                max_length=self.max_length,\n                return_tensors=\"pt\",\n                return_attention_mask=True\n            )\n            item = {\"input_ids\": tokenized.input_ids[0],\n                    \"attention_mask\": tokenized.attention_mask[0],\n                    \"ID\": row[\"ID\"]}\n            if self.include_true_label and \"label\" in row:\n                item[\"true_label\"] = row[\"label\"]\n            return item\n\n# -------------------------------\n# 4. Define Data Collator for Efficient Padding\n# -------------------------------\ndef collate_fn(batch):\n    input_ids = [x[\"input_ids\"] for x in batch]\n    attention_masks = [x.get(\"attention_mask\", None) for x in batch]\n    batch_input = tokenizer.pad({\"input_ids\": input_ids, \"attention_mask\": attention_masks}, return_tensors=\"pt\")\n    \n    if \"labels\" in batch[0]:\n        labels = [x[\"labels\"] for x in batch]\n        batch_labels = tokenizer.pad({\"input_ids\": labels}, return_tensors=\"pt\")[\"input_ids\"]\n        batch_labels[batch_labels == tokenizer.pad_token_id] = -100\n        batch_input[\"labels\"] = batch_labels\n    else:\n        ids = [x[\"ID\"] for x in batch]\n        batch_input[\"ID\"] = ids\n    \n    if \"true_label\" in batch[0]:\n        batch_input[\"true_label\"] = [x[\"true_label\"] for x in batch]\n    return batch_input\n\n# -------------------------------\n# 5. Load and Prepare Data\n# -------------------------------\ntrain_df = pd.read_csv(\"/kaggle/input/multi-lingual-sentiment-analysis/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/multi-lingual-sentiment-analysis/test.csv\")\n\ntrain_dataset = SentimentDataset(train_df, tokenizer, max_length=512, is_train=True, include_true_label=False)\neval_dataset = SentimentDataset(\n    train_df.sample(frac=0.1, random_state=42).reset_index(drop=True),\n    tokenizer, max_length=512, is_train=True, include_true_label=True\n)\ntest_dataset = SentimentDataset(test_df, tokenizer, max_length=512, is_train=False, include_true_label=False)\n\n# -------------------------------\n# 6. Define Training Arguments (unchanged)\n# -------------------------------\ntraining_args = TrainingArguments(\n    output_dir=\"./qlora_finetuned\",\n    # auto_find_batch_size=True,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=10,        \n    learning_rate=1e-5,\n    warmup_ratio=0.03,\n    max_grad_norm=0.3,\n    optim=\"paged_adamw_32bit\",\n    weight_decay=0.01,\n    logging_steps=10,\n    save_steps=100,\n    fp16=True,\n    gradient_accumulation_steps=8,\n    save_total_limit=2,\n    eval_strategy=\"steps\",\n    eval_steps=100,\n    load_best_model_at_end=True,          # Enable best model loading for early stopping\n    metric_for_best_model=\"eval_loss\",     # Specify which metric to monitor\n    greater_is_better=False,\n    lr_scheduler_type=\"cosine\",\n    report_to=\"none\",\n)\n\n# -------------------------------\n# 7. Integrate Metrics: Compute F1 Score, Accuracy, Classification Report, and Confusion Matrix\n# -------------------------------\n# Extract true labels from the evaluation dataset for metric calculation.\neval_true_labels = [item[\"true_label\"] for item in eval_dataset]\n\ndef compute_metrics(eval_preds):\n    preds, _ = eval_preds  # We ignore label_ids because we use our stored true labels.\n    decoded_preds = [tokenizer.decode(pred, skip_special_tokens=True) for pred in preds]\n    pred_sentiments = []\n    for text in decoded_preds:\n        if \"Answer:\" in text:\n            sentiment = text.split(\"Answer:\")[-1].strip().split()[0]\n        else:\n            sentiment = text.strip().split()[0] if text.strip().split() else \"Negative\"\n        pred_sentiments.append(sentiment)\n    acc = accuracy_score(eval_true_labels, pred_sentiments)\n    f1 = f1_score(eval_true_labels, pred_sentiments, average=\"weighted\")\n    report = classification_report(eval_true_labels, pred_sentiments, output_dict=True)\n    cm = confusion_matrix(eval_true_labels, pred_sentiments)\n    return {\"accuracy\": acc, \"f1\": f1, \"classification_report\": report, \"confusion_matrix\": cm}\n\n# -------------------------------\n# 8. Instantiate Trainer with Early Stopping and Checkpoint Resumption\n# -------------------------------\nif not os.path.exists(training_args.output_dir):\n    os.makedirs(training_args.output_dir, exist_ok=True)\nlast_checkpoint = get_last_checkpoint(training_args.output_dir)\n\ncallbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n\nif last_checkpoint is None:\n    print(\"No checkpoint found. Training from scratch.\")\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        data_collator=collate_fn,\n        callbacks=callbacks,\n        compute_metrics=compute_metrics,\n    )\n    trainer.train()\nelse:\n    print(f\"Resuming training from checkpoint {last_checkpoint}\")\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        data_collator=collate_fn,\n        callbacks=callbacks,\n        compute_metrics=compute_metrics,\n    )\n    trainer.train(resume_from_checkpoint=last_checkpoint)\n\n# Save the fine‑tuned model and tokenizer.\nmodel.save_pretrained(\"./qlora_finetuned_model\")\ntokenizer.save_pretrained(\"./qlora_finetuned_model\")\n\n# -------------------------------\n# 9. Inference on Test Set & Generate Submission\n# -------------------------------\ntest_dataloader = DataLoader(test_dataset, batch_size=4, collate_fn=collate_fn)\nmodel.eval()\npredictions = []\nsample_ids = []\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        input_ids = batch[\"input_ids\"].to(model.device)\n        attention_mask = batch.get(\"attention_mask\", None)\n        if attention_mask is not None:\n            attention_mask = attention_mask.to(model.device)\n        generated = model.generate(\n            input_ids,\n            attention_mask=attention_mask,\n            max_new_tokens=10,\n            do_sample=False,\n            pad_token_id=tokenizer.eos_token_id,\n            temperature=1.0,\n            top_p=1.0,\n        )\n        for gen_ids in generated:\n            decoded = tokenizer.decode(gen_ids, skip_special_tokens=True)\n            answer_part = decoded.split(\"Answer:\")[-1].strip() if \"Answer:\" in decoded else decoded.strip()\n            tokens = answer_part.split()\n            if tokens:\n                pred_label = tokens[0]\n                final_pred = \"Positive\" if pred_label.lower().startswith(\"pos\") else \"Negative\"\n            else:\n                final_pred = \"Negative\"\n            predictions.append(final_pred)\n        sample_ids.extend(batch[\"ID\"])\n\nsubmission_df = pd.DataFrame({\"ID\": sample_ids, \"label\": predictions}).sort_values(\"ID\")\nsubmission_df.to_csv(\"/kaggle/working/submission2.csv\", index=False)\nprint(\"Submission file saved as /kaggle/working/submission2.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}