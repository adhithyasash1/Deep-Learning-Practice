{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fb4dbd4",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-05T17:22:07.230983Z",
     "iopub.status.busy": "2025-04-05T17:22:07.230726Z",
     "iopub.status.idle": "2025-04-05T17:22:07.243273Z",
     "shell.execute_reply": "2025-04-05T17:22:07.242579Z"
    },
    "papermill": {
     "duration": 0.017588,
     "end_time": "2025-04-05T17:22:07.244607",
     "exception": false,
     "start_time": "2025-04-05T17:22:07.227019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom PIL import Image\\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\\nfrom skimage.metrics import structural_similarity as ssim\\nimport cv2\\nimport torch\\nfrom torchvision import transforms\\nimport seaborn as sns\\nfrom tqdm import tqdm\\n\\ndef analyze_dataset(train_dir, gt_dir=None, plot=True, save_stats=True):\\n    \"\"\"\\n    Performs comprehensive analysis on low-light images dataset\\n    \\n    Args:\\n        train_dir: Directory containing low-res noisy images\\n        gt_dir: Directory containing high-res clean images (ground truth)\\n        plot: Whether to display plots\\n        save_stats: Whether to save statistics to CSV\\n    \\n    Returns:\\n        Dictionary containing dataset statistics\\n    \"\"\"\\n    print(f\"Analyzing dataset in {train_dir}...\")\\n    stats = {}\\n    \\n    # Get all image files\\n    train_files = sorted([f for f in os.listdir(train_dir) if f.endswith((\\'.png\\', \\'.jpg\\', \\'.jpeg\\', \\'.bmp\\', \\'.tiff\\'))])\\n    if gt_dir:\\n        gt_files = sorted([f for f in os.listdir(gt_dir) if f.endswith((\\'.png\\', \\'.jpg\\', \\'.jpeg\\', \\'.bmp\\', \\'.tiff\\'))])\\n    \\n    # Image count\\n    stats[\\'total_images\\'] = len(train_files)\\n    print(f\"Total images found: {stats[\\'total_images\\']}\")\\n    \\n    # Initialize data containers\\n    resolutions = []\\n    brightness_stats = {\\'train\\': [], \\'gt\\': []}\\n    contrast_stats = {\\'train\\': [], \\'gt\\': []}\\n    noise_levels = []\\n    psnr_values = []\\n    ssim_values = []\\n    histograms = {\\'train\\': np.zeros(256), \\'gt\\': np.zeros(256)}\\n    \\n    # Analyze each image\\n    for i, train_file in enumerate(tqdm(train_files[:min(100, len(train_files))], desc=\"Analyzing images\")):\\n        # Load train image\\n        train_path = os.path.join(train_dir, train_file)\\n        train_img = np.array(Image.open(train_path).convert(\\'RGB\\'))\\n        \\n        # Store resolution\\n        resolutions.append(train_img.shape[:2])\\n        \\n        # Calculate brightness\\n        train_brightness = np.mean(train_img)\\n        brightness_stats[\\'train\\'].append(train_brightness)\\n        \\n        # Calculate contrast\\n        train_contrast = np.std(train_img)\\n        contrast_stats[\\'train\\'].append(train_contrast)\\n        \\n        # Update histogram\\n        for channel in range(3):\\n            histograms[\\'train\\'] += np.histogram(train_img[:,:,channel], bins=256, range=(0, 256))[0]\\n        \\n        # If ground truth is available\\n        if gt_dir:\\n            # Replace train prefix with gt prefix if needed\\n            gt_file = train_file\\n            if train_file.startswith(\\'train_\\'):\\n                gt_file = train_file.replace(\\'train_\\', \\'gt_\\')\\n            if gt_file in gt_files:\\n                gt_path = os.path.join(gt_dir, gt_file)\\n                gt_img = np.array(Image.open(gt_path).convert(\\'RGB\\'))\\n                \\n                # Calculate brightness for gt\\n                gt_brightness = np.mean(gt_img)\\n                brightness_stats[\\'gt\\'].append(gt_brightness)\\n                \\n                # Calculate contrast for gt\\n                gt_contrast = np.std(gt_img)\\n                contrast_stats[\\'gt\\'].append(gt_contrast)\\n                \\n                # Update gt histogram\\n                for channel in range(3):\\n                    histograms[\\'gt\\'] += np.histogram(gt_img[:,:,channel], bins=256, range=(0, 256))[0]\\n                \\n                # Calculate noise level (difference between train and gt)\\n                # Resize gt to match train if needed\\n                if train_img.shape != gt_img.shape:\\n                    gt_img_resized = cv2.resize(gt_img, (train_img.shape[1], train_img.shape[0]), \\n                                               interpolation=cv2.INTER_CUBIC)\\n                else:\\n                    gt_img_resized = gt_img\\n                \\n                # Noise is the RMSE between images\\n                noise = np.sqrt(np.mean((train_img.astype(float) - gt_img_resized.astype(float)) ** 2))\\n                noise_levels.append(noise)\\n                \\n                # Convert to grayscale for PSNR and SSIM\\n                train_gray = cv2.cvtColor(train_img, cv2.COLOR_RGB2GRAY)\\n                gt_resized_gray = cv2.cvtColor(gt_img_resized, cv2.COLOR_RGB2GRAY)\\n                \\n                # Calculate PSNR\\n                psnr_val = psnr(gt_resized_gray, train_gray, data_range=255)\\n                psnr_values.append(psnr_val)\\n                \\n                # Calculate SSIM\\n                ssim_val = ssim(gt_resized_gray, train_gray, data_range=255)\\n                ssim_values.append(ssim_val)\\n    \\n    # Compute statistics\\n    stats[\\'resolution_min\\'] = min(resolutions, key=lambda x: x[0] * x[1])\\n    stats[\\'resolution_max\\'] = max(resolutions, key=lambda x: x[0] * x[1])\\n    stats[\\'resolution_mode\\'] = max(set(resolutions), key=resolutions.count)\\n    \\n    stats[\\'brightness_train_mean\\'] = np.mean(brightness_stats[\\'train\\'])\\n    stats[\\'brightness_train_std\\'] = np.std(brightness_stats[\\'train\\'])\\n    stats[\\'contrast_train_mean\\'] = np.mean(contrast_stats[\\'train\\'])\\n    stats[\\'contrast_train_std\\'] = np.std(contrast_stats[\\'train\\'])\\n    \\n    if gt_dir and brightness_stats[\\'gt\\']:\\n        stats[\\'brightness_gt_mean\\'] = np.mean(brightness_stats[\\'gt\\'])\\n        stats[\\'brightness_gt_std\\'] = np.std(brightness_stats[\\'gt\\'])\\n        stats[\\'contrast_gt_mean\\'] = np.mean(contrast_stats[\\'gt\\'])\\n        stats[\\'contrast_gt_std\\'] = np.std(contrast_stats[\\'gt\\'])\\n        stats[\\'noise_level_mean\\'] = np.mean(noise_levels)\\n        stats[\\'noise_level_std\\'] = np.std(noise_levels)\\n        stats[\\'psnr_mean\\'] = np.mean(psnr_values)\\n        stats[\\'psnr_std\\'] = np.std(psnr_values)\\n        stats[\\'ssim_mean\\'] = np.mean(ssim_values)\\n        stats[\\'ssim_std\\'] = np.std(ssim_values)\\n    \\n    # Print statistics\\n    print(\"\\n----- Dataset Statistics -----\")\\n    print(f\"Resolution: Mode={stats[\\'resolution_mode\\']}, Min={stats[\\'resolution_min\\']}, Max={stats[\\'resolution_max\\']}\")\\n    print(f\"Train Brightness: Mean={stats[\\'brightness_train_mean\\']:.2f}, Std={stats[\\'brightness_train_std\\']:.2f}\")\\n    print(f\"Train Contrast: Mean={stats[\\'contrast_train_mean\\']:.2f}, Std={stats[\\'contrast_train_std\\']:.2f}\")\\n    \\n    if gt_dir and brightness_stats[\\'gt\\']:\\n        print(f\"GT Brightness: Mean={stats[\\'brightness_gt_mean\\']:.2f}, Std={stats[\\'brightness_gt_std\\']:.2f}\")\\n        print(f\"GT Contrast: Mean={stats[\\'contrast_gt_mean\\']:.2f}, Std={stats[\\'contrast_gt_std\\']:.2f}\")\\n        print(f\"Noise Level: Mean={stats[\\'noise_level_mean\\']:.2f}, Std={stats[\\'noise_level_std\\']:.2f}\")\\n        print(f\"PSNR: Mean={stats[\\'psnr_mean\\']:.2f}dB, Std={stats[\\'psnr_std\\']:.2f}dB\")\\n        print(f\"SSIM: Mean={stats[\\'ssim_mean\\']:.4f}, Std={stats[\\'ssim_std\\']:.4f}\")\\n    \\n    # Save stats to CSV if requested\\n    if save_stats:\\n        stats_df = pd.DataFrame({k: [v] for k, v in stats.items() if not isinstance(v, tuple)})\\n        stats_df.to_csv(\\'dataset_stats.csv\\', index=False)\\n        print(\"Statistics saved to dataset_stats.csv\")\\n    \\n    # Create plots if requested\\n    if plot:\\n        # Plot histograms\\n        plt.figure(figsize=(15, 10))\\n        \\n        # Image histograms\\n        plt.subplot(2, 2, 1)\\n        plt.plot(histograms[\\'train\\'] / sum(histograms[\\'train\\']), \\'r-\\', label=\\'Train\\')\\n        if gt_dir and brightness_stats[\\'gt\\']:\\n            plt.plot(histograms[\\'gt\\'] / sum(histograms[\\'gt\\']), \\'g-\\', label=\\'Ground Truth\\')\\n        plt.title(\\'Pixel Intensity Distribution\\')\\n        plt.xlabel(\\'Pixel Value\\')\\n        plt.ylabel(\\'Frequency\\')\\n        plt.legend()\\n        \\n        # Brightness comparison\\n        plt.subplot(2, 2, 2)\\n        sns.histplot(brightness_stats[\\'train\\'], color=\\'red\\', label=\\'Train\\', kde=True, stat=\\'density\\')\\n        if gt_dir and brightness_stats[\\'gt\\']:\\n            sns.histplot(brightness_stats[\\'gt\\'], color=\\'green\\', label=\\'Ground Truth\\', kde=True, stat=\\'density\\')\\n        plt.title(\\'Image Brightness Distribution\\')\\n        plt.xlabel(\\'Mean Brightness\\')\\n        plt.ylabel(\\'Density\\')\\n        plt.legend()\\n        \\n        # Contrast comparison\\n        plt.subplot(2, 2, 3)\\n        sns.histplot(contrast_stats[\\'train\\'], color=\\'red\\', label=\\'Train\\', kde=True, stat=\\'density\\')\\n        if gt_dir and brightness_stats[\\'gt\\']:\\n            sns.histplot(contrast_stats[\\'gt\\'], color=\\'green\\', label=\\'Ground Truth\\', kde=True, stat=\\'density\\')\\n        plt.title(\\'Image Contrast Distribution\\')\\n        plt.xlabel(\\'Standard Deviation (Contrast)\\')\\n        plt.ylabel(\\'Density\\')\\n        plt.legend()\\n        \\n        # PSNR distribution\\n        if gt_dir and psnr_values:\\n            plt.subplot(2, 2, 4)\\n            sns.histplot(psnr_values, kde=True, color=\\'blue\\')\\n            plt.title(\\'PSNR Distribution\\')\\n            plt.xlabel(\\'PSNR (dB)\\')\\n            plt.ylabel(\\'Frequency\\')\\n        \\n        plt.tight_layout()\\n        plt.savefig(\\'dataset_analysis.png\\')\\n        plt.show()\\n        \\n        # Display sample images\\n        num_samples = min(5, len(train_files))\\n        plt.figure(figsize=(15, 4*num_samples))\\n        \\n        for i in range(num_samples):\\n            # Load train image\\n            train_path = os.path.join(train_dir, train_files[i])\\n            train_img = np.array(Image.open(train_path).convert(\\'RGB\\'))\\n            \\n            plt.subplot(num_samples, 2, 2*i+1)\\n            plt.imshow(train_img)\\n            plt.title(f\\'Train Image: {train_files[i]}\\')\\n            plt.axis(\\'off\\')\\n            \\n            # Load gt image if available\\n            if gt_dir:\\n                gt_file = train_files[i]\\n                if train_files[i].startswith(\\'train_\\'):\\n                    gt_file = train_files[i].replace(\\'train_\\', \\'gt_\\')\\n                if gt_file in gt_files:\\n                    gt_path = os.path.join(gt_dir, gt_file)\\n                    gt_img = np.array(Image.open(gt_path).convert(\\'RGB\\'))\\n                    \\n                    plt.subplot(num_samples, 2, 2*i+2)\\n                    plt.imshow(gt_img)\\n                    plt.title(f\\'GT Image: {gt_file}\\')\\n                    plt.axis(\\'off\\')\\n        \\n        plt.tight_layout()\\n        plt.savefig(\\'sample_images.png\\')\\n        plt.show()\\n    \\n    return stats\\n\\ndef visualize_super_resolution_task(train_dir, gt_dir, num_samples=3):\\n    \"\"\"\\n    Visualizes the super-resolution task by showing low-res input and high-res output\\n    \\n    Args:\\n        train_dir: Directory containing low-res noisy images\\n        gt_dir: Directory containing high-res clean images (ground truth)\\n        num_samples: Number of samples to visualize\\n    \"\"\"\\n    train_files = sorted([f for f in os.listdir(train_dir) if f.endswith((\\'.png\\', \\'.jpg\\', \\'.jpeg\\', \\'.bmp\\', \\'.tiff\\'))])\\n    gt_files = sorted([f for f in os.listdir(gt_dir) if f.endswith((\\'.png\\', \\'.jpg\\', \\'.jpeg\\', \\'.bmp\\', \\'.tiff\\'))])\\n    \\n    plt.figure(figsize=(15, 5*num_samples))\\n    \\n    for i in range(min(num_samples, len(train_files))):\\n        # Load train image\\n        train_path = os.path.join(train_dir, train_files[i])\\n        train_img = np.array(Image.open(train_path).convert(\\'RGB\\'))\\n        \\n        # Find matching GT image\\n        gt_file = train_files[i]\\n        if train_files[i].startswith(\\'train_\\'):\\n            gt_file = train_files[i].replace(\\'train_\\', \\'gt_\\')\\n        if gt_file in gt_files:\\n            gt_path = os.path.join(gt_dir, gt_file)\\n            gt_img = np.array(Image.open(gt_path).convert(\\'RGB\\'))\\n            \\n            # Display images and size info\\n            plt.subplot(num_samples, 3, 3*i+1)\\n            plt.imshow(train_img)\\n            plt.title(f\\'Low-res Input: {train_img.shape}\\')\\n            plt.axis(\\'off\\')\\n            \\n            # Display upscaled input (bicubic)\\n            if train_img.shape != gt_img.shape:\\n                upscaled = cv2.resize(train_img, (gt_img.shape[1], gt_img.shape[0]), \\n                                     interpolation=cv2.INTER_CUBIC)\\n                plt.subplot(num_samples, 3, 3*i+2)\\n                plt.imshow(upscaled)\\n                plt.title(f\\'Bicubic Upscaled: {upscaled.shape}\\')\\n                plt.axis(\\'off\\')\\n            \\n            # Display ground truth\\n            plt.subplot(num_samples, 3, 3*i+3)\\n            plt.imshow(gt_img)\\n            plt.title(f\\'High-res Target: {gt_img.shape}\\')\\n            plt.axis(\\'off\\')\\n    \\n    plt.tight_layout()\\n    plt.savefig(\\'super_resolution_examples.png\\')\\n    plt.show()\\n\\ndef estimate_model_complexity(train_dir, gt_dir):\\n    \"\"\"\\n    Estimates the complexity of the super-resolution and denoising task\\n    based on image sizes, upscaling factor, and noise characteristics\\n    \"\"\"\\n    train_files = sorted([f for f in os.listdir(train_dir) if f.endswith((\\'.png\\', \\'.jpg\\', \\'.jpeg\\', \\'.bmp\\', \\'.tiff\\'))])\\n    gt_files = sorted([f for f in os.listdir(gt_dir) if f.endswith((\\'.png\\', \\'.jpg\\', \\'.jpeg\\', \\'.bmp\\', \\'.tiff\\'))])\\n    \\n    scale_factors = []\\n    noise_levels = []\\n    \\n    # Sample images for analysis\\n    sample_size = min(50, len(train_files))\\n    for i in range(sample_size):\\n        # Load train image\\n        train_path = os.path.join(train_dir, train_files[i])\\n        train_img = np.array(Image.open(train_path).convert(\\'RGB\\'))\\n        \\n        # Find matching GT image\\n        gt_file = train_files[i]\\n        if train_files[i].startswith(\\'train_\\'):\\n            gt_file = train_files[i].replace(\\'train_\\', \\'gt_\\')\\n        if gt_file in gt_files:\\n            gt_path = os.path.join(gt_dir, gt_file)\\n            gt_img = np.array(Image.open(gt_path).convert(\\'RGB\\'))\\n            \\n            # Calculate scale factor\\n            h_scale = gt_img.shape[0] / train_img.shape[0]\\n            w_scale = gt_img.shape[1] / train_img.shape[1]\\n            scale_factors.append((h_scale, w_scale))\\n            \\n            # Resize ground truth to match input size for noise comparison\\n            gt_resized = cv2.resize(gt_img, (train_img.shape[1], train_img.shape[0]), \\n                                   interpolation=cv2.INTER_AREA)\\n            \\n            # Calculate noise level (RMSE)\\n            noise = np.sqrt(np.mean((train_img.astype(float) - gt_resized.astype(float)) ** 2))\\n            noise_levels.append(noise)\\n    \\n    # Calculate average scale factor\\n    avg_h_scale = np.mean([s[0] for s in scale_factors])\\n    avg_w_scale = np.mean([s[1] for s in scale_factors])\\n    \\n    # Calculate average noise level\\n    avg_noise = np.mean(noise_levels)\\n    \\n    print(\"\\n----- Task Complexity Analysis -----\")\\n    print(f\"Average Scale Factor: {avg_h_scale:.2f}x{avg_w_scale:.2f}\")\\n    print(f\"Average Noise Level: {avg_noise:.2f}\")\\n    \\n    # Determine complexity based on findings\\n    if avg_h_scale >= 3.5 and avg_w_scale >= 3.5:\\n        sr_complexity = \"High (4x or higher)\"\\n    elif avg_h_scale >= 1.5 and avg_w_scale >= 1.5:\\n        sr_complexity = \"Medium (2x)\"\\n    else:\\n        sr_complexity = \"Low (less than 2x)\"\\n    \\n    if avg_noise > 30:\\n        noise_complexity = \"High (significant noise)\"\\n    elif avg_noise > 15:\\n        noise_complexity = \"Medium (moderate noise)\"\\n    else:\\n        noise_complexity = \"Low (minimal noise)\"\\n    \\n    print(f\"Super-Resolution Complexity: {sr_complexity}\")\\n    print(f\"Denoising Complexity: {noise_complexity}\")\\n    \\n    # Suggest model architectures based on complexity\\n    print(\"\\nRecommended Model Architectures:\")\\n    if sr_complexity == \"High (4x or higher)\" and noise_complexity == \"High (significant noise)\":\\n        print(\"- Advanced GAN-based models (e.g., ESRGAN with noise handling)\")\\n        print(\"- Deep residual networks with attention mechanisms\")\\n        print(\"- Two-stage pipeline: denoising followed by super-resolution\")\\n    elif noise_complexity == \"High (significant noise)\":\\n        print(\"- Dedicated denoising networks (e.g., DnCNN) followed by SR\")\\n        print(\"- Attention-based models that can focus on noise patterns\")\\n    else:\\n        print(\"- EDSR or SRResNet based architectures\")\\n        print(\"- Lighter GAN-based models\")\\n    \\n    return {\\n        \\'avg_scale_factor\\': (avg_h_scale, avg_w_scale),\\n        \\'avg_noise\\': avg_noise,\\n        \\'sr_complexity\\': sr_complexity,\\n        \\'noise_complexity\\': noise_complexity\\n    }\\n\\ndef analyze_test_data(test_dir):\\n    \"\"\"\\n    Analyzes test data to understand its characteristics\\n    \"\"\"\\n    print(f\"Analyzing test data in {test_dir}...\")\\n    \\n    # Get all image files\\n    test_files = sorted([f for f in os.listdir(test_dir) if f.endswith((\\'.png\\', \\'.jpg\\', \\'.jpeg\\', \\'.bmp\\', \\'.tiff\\'))])\\n    \\n    # Image count\\n    total_images = len(test_files)\\n    print(f\"Total test images found: {total_images}\")\\n    \\n    # Initialize data containers\\n    resolutions = []\\n    brightness_stats = []\\n    contrast_stats = []\\n    histograms = np.zeros(256)\\n    \\n    # Analyze each image\\n    for i, test_file in enumerate(tqdm(test_files[:min(100, len(test_files))], desc=\"Analyzing test images\")):\\n        # Load test image\\n        test_path = os.path.join(test_dir, test_file)\\n        test_img = np.array(Image.open(test_path).convert(\\'RGB\\'))\\n        \\n        # Store resolution\\n        resolutions.append(test_img.shape[:2])\\n        \\n        # Calculate brightness\\n        test_brightness = np.mean(test_img)\\n        brightness_stats.append(test_brightness)\\n        \\n        # Calculate contrast\\n        test_contrast = np.std(test_img)\\n        contrast_stats.append(test_contrast)\\n        \\n        # Update histogram\\n        for channel in range(3):\\n            histograms += np.histogram(test_img[:,:,channel], bins=256, range=(0, 256))[0]\\n    \\n    # Compute statistics\\n    resolution_min = min(resolutions, key=lambda x: x[0] * x[1])\\n    resolution_max = max(resolutions, key=lambda x: x[0] * x[1])\\n    resolution_mode = max(set(resolutions), key=resolutions.count)\\n    \\n    brightness_mean = np.mean(brightness_stats)\\n    brightness_std = np.std(brightness_stats)\\n    contrast_mean = np.mean(contrast_stats)\\n    contrast_std = np.std(contrast_stats)\\n    \\n    # Print statistics\\n    print(\"\\n----- Test Dataset Statistics -----\")\\n    print(f\"Resolution: Mode={resolution_mode}, Min={resolution_min}, Max={resolution_max}\")\\n    print(f\"Brightness: Mean={brightness_mean:.2f}, Std={brightness_std:.2f}\")\\n    print(f\"Contrast: Mean={contrast_mean:.2f}, Std={contrast_std:.2f}\")\\n    \\n    # Create plots\\n    plt.figure(figsize=(15, 5))\\n    \\n    # Image histograms\\n    plt.subplot(1, 3, 1)\\n    plt.plot(histograms / sum(histograms), \\'b-\\')\\n    plt.title(\\'Test Images Pixel Intensity Distribution\\')\\n    plt.xlabel(\\'Pixel Value\\')\\n    plt.ylabel(\\'Frequency\\')\\n    \\n    # Brightness histogram\\n    plt.subplot(1, 3, 2)\\n    sns.histplot(brightness_stats, color=\\'blue\\', kde=True, stat=\\'density\\')\\n    plt.title(\\'Test Images Brightness Distribution\\')\\n    plt.xlabel(\\'Mean Brightness\\')\\n    plt.ylabel(\\'Density\\')\\n    \\n    # Contrast histogram\\n    plt.subplot(1, 3, 3)\\n    sns.histplot(contrast_stats, color=\\'blue\\', kde=True, stat=\\'density\\')\\n    plt.title(\\'Test Images Contrast Distribution\\')\\n    plt.xlabel(\\'Standard Deviation (Contrast)\\')\\n    plt.ylabel(\\'Density\\')\\n    \\n    plt.tight_layout()\\n    plt.savefig(\\'test_data_analysis.png\\')\\n    plt.show()\\n    \\n    # Display sample test images\\n    num_samples = min(5, len(test_files))\\n    plt.figure(figsize=(15, 3*num_samples))\\n    \\n    for i in range(num_samples):\\n        # Load test image\\n        test_path = os.path.join(test_dir, test_files[i])\\n        test_img = np.array(Image.open(test_path).convert(\\'RGB\\'))\\n        \\n        plt.subplot(num_samples, 1, i+1)\\n        plt.imshow(test_img)\\n        plt.title(f\\'Test Image: {test_files[i]} - Shape: {test_img.shape}\\')\\n        plt.axis(\\'off\\')\\n    \\n    plt.tight_layout()\\n    plt.savefig(\\'test_sample_images.png\\')\\n    plt.show()\\n    \\n    return {\\n        \\'total_images\\': total_images,\\n        \\'resolution_min\\': resolution_min,\\n        \\'resolution_max\\': resolution_max,\\n        \\'resolution_mode\\': resolution_mode,\\n        \\'brightness_mean\\': brightness_mean,\\n        \\'brightness_std\\': brightness_std,\\n        \\'contrast_mean\\': contrast_mean,\\n        \\'contrast_std\\': contrast_std\\n    }\\n\\ndef main():\\n    # Use the provided Kaggle paths\\n    base_dir = \"/kaggle/input/dlp-jan-2025-nppe-3/archive\"\\n    train_dir = os.path.join(base_dir, \"train/train\")\\n    gt_dir = os.path.join(base_dir, \"train/gt\")\\n    val_dir = os.path.join(base_dir, \"val/val\")\\n    val_gt_dir = os.path.join(base_dir, \"val/gt\")\\n    test_dir = os.path.join(base_dir, \"test\")\\n    \\n    # Analyze training data\\n    print(\"\\n===== ANALYZING TRAINING DATA =====\")\\n    train_stats = analyze_dataset(train_dir, gt_dir)\\n    \\n    # Analyze validation data\\n    print(\"\\n===== ANALYZING VALIDATION DATA =====\")\\n    val_stats = analyze_dataset(val_dir, val_gt_dir)\\n    \\n    # Analyze test data\\n    print(\"\\n===== ANALYZING TEST DATA =====\")\\n    test_stats = analyze_test_data(test_dir)\\n    \\n    # Visualize super-resolution task\\n    print(\"\\n===== VISUALIZING SUPER-RESOLUTION TASK =====\")\\n    visualize_super_resolution_task(train_dir, gt_dir)\\n    \\n    # Estimate model complexity\\n    print(\"\\n===== ESTIMATING MODEL COMPLEXITY =====\")\\n    complexity = estimate_model_complexity(train_dir, gt_dir)\\n    \\n    print(\"\\nAnalysis complete! Please use these insights to build an appropriate model.\")\\n    print(\"Based on the analysis results, please share the key statistics with me so I can recommend the best model architecture.\")\\n\\nif __name__ == \"__main__\":\\n    main()\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "def analyze_dataset(train_dir, gt_dir=None, plot=True, save_stats=True):\n",
    "    \"\"\"\n",
    "    Performs comprehensive analysis on low-light images dataset\n",
    "    \n",
    "    Args:\n",
    "        train_dir: Directory containing low-res noisy images\n",
    "        gt_dir: Directory containing high-res clean images (ground truth)\n",
    "        plot: Whether to display plots\n",
    "        save_stats: Whether to save statistics to CSV\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing dataset statistics\n",
    "    \"\"\"\n",
    "    print(f\"Analyzing dataset in {train_dir}...\")\n",
    "    stats = {}\n",
    "    \n",
    "    # Get all image files\n",
    "    train_files = sorted([f for f in os.listdir(train_dir) if f.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))])\n",
    "    if gt_dir:\n",
    "        gt_files = sorted([f for f in os.listdir(gt_dir) if f.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))])\n",
    "    \n",
    "    # Image count\n",
    "    stats['total_images'] = len(train_files)\n",
    "    print(f\"Total images found: {stats['total_images']}\")\n",
    "    \n",
    "    # Initialize data containers\n",
    "    resolutions = []\n",
    "    brightness_stats = {'train': [], 'gt': []}\n",
    "    contrast_stats = {'train': [], 'gt': []}\n",
    "    noise_levels = []\n",
    "    psnr_values = []\n",
    "    ssim_values = []\n",
    "    histograms = {'train': np.zeros(256), 'gt': np.zeros(256)}\n",
    "    \n",
    "    # Analyze each image\n",
    "    for i, train_file in enumerate(tqdm(train_files[:min(100, len(train_files))], desc=\"Analyzing images\")):\n",
    "        # Load train image\n",
    "        train_path = os.path.join(train_dir, train_file)\n",
    "        train_img = np.array(Image.open(train_path).convert('RGB'))\n",
    "        \n",
    "        # Store resolution\n",
    "        resolutions.append(train_img.shape[:2])\n",
    "        \n",
    "        # Calculate brightness\n",
    "        train_brightness = np.mean(train_img)\n",
    "        brightness_stats['train'].append(train_brightness)\n",
    "        \n",
    "        # Calculate contrast\n",
    "        train_contrast = np.std(train_img)\n",
    "        contrast_stats['train'].append(train_contrast)\n",
    "        \n",
    "        # Update histogram\n",
    "        for channel in range(3):\n",
    "            histograms['train'] += np.histogram(train_img[:,:,channel], bins=256, range=(0, 256))[0]\n",
    "        \n",
    "        # If ground truth is available\n",
    "        if gt_dir:\n",
    "            # Replace train prefix with gt prefix if needed\n",
    "            gt_file = train_file\n",
    "            if train_file.startswith('train_'):\n",
    "                gt_file = train_file.replace('train_', 'gt_')\n",
    "            if gt_file in gt_files:\n",
    "                gt_path = os.path.join(gt_dir, gt_file)\n",
    "                gt_img = np.array(Image.open(gt_path).convert('RGB'))\n",
    "                \n",
    "                # Calculate brightness for gt\n",
    "                gt_brightness = np.mean(gt_img)\n",
    "                brightness_stats['gt'].append(gt_brightness)\n",
    "                \n",
    "                # Calculate contrast for gt\n",
    "                gt_contrast = np.std(gt_img)\n",
    "                contrast_stats['gt'].append(gt_contrast)\n",
    "                \n",
    "                # Update gt histogram\n",
    "                for channel in range(3):\n",
    "                    histograms['gt'] += np.histogram(gt_img[:,:,channel], bins=256, range=(0, 256))[0]\n",
    "                \n",
    "                # Calculate noise level (difference between train and gt)\n",
    "                # Resize gt to match train if needed\n",
    "                if train_img.shape != gt_img.shape:\n",
    "                    gt_img_resized = cv2.resize(gt_img, (train_img.shape[1], train_img.shape[0]), \n",
    "                                               interpolation=cv2.INTER_CUBIC)\n",
    "                else:\n",
    "                    gt_img_resized = gt_img\n",
    "                \n",
    "                # Noise is the RMSE between images\n",
    "                noise = np.sqrt(np.mean((train_img.astype(float) - gt_img_resized.astype(float)) ** 2))\n",
    "                noise_levels.append(noise)\n",
    "                \n",
    "                # Convert to grayscale for PSNR and SSIM\n",
    "                train_gray = cv2.cvtColor(train_img, cv2.COLOR_RGB2GRAY)\n",
    "                gt_resized_gray = cv2.cvtColor(gt_img_resized, cv2.COLOR_RGB2GRAY)\n",
    "                \n",
    "                # Calculate PSNR\n",
    "                psnr_val = psnr(gt_resized_gray, train_gray, data_range=255)\n",
    "                psnr_values.append(psnr_val)\n",
    "                \n",
    "                # Calculate SSIM\n",
    "                ssim_val = ssim(gt_resized_gray, train_gray, data_range=255)\n",
    "                ssim_values.append(ssim_val)\n",
    "    \n",
    "    # Compute statistics\n",
    "    stats['resolution_min'] = min(resolutions, key=lambda x: x[0] * x[1])\n",
    "    stats['resolution_max'] = max(resolutions, key=lambda x: x[0] * x[1])\n",
    "    stats['resolution_mode'] = max(set(resolutions), key=resolutions.count)\n",
    "    \n",
    "    stats['brightness_train_mean'] = np.mean(brightness_stats['train'])\n",
    "    stats['brightness_train_std'] = np.std(brightness_stats['train'])\n",
    "    stats['contrast_train_mean'] = np.mean(contrast_stats['train'])\n",
    "    stats['contrast_train_std'] = np.std(contrast_stats['train'])\n",
    "    \n",
    "    if gt_dir and brightness_stats['gt']:\n",
    "        stats['brightness_gt_mean'] = np.mean(brightness_stats['gt'])\n",
    "        stats['brightness_gt_std'] = np.std(brightness_stats['gt'])\n",
    "        stats['contrast_gt_mean'] = np.mean(contrast_stats['gt'])\n",
    "        stats['contrast_gt_std'] = np.std(contrast_stats['gt'])\n",
    "        stats['noise_level_mean'] = np.mean(noise_levels)\n",
    "        stats['noise_level_std'] = np.std(noise_levels)\n",
    "        stats['psnr_mean'] = np.mean(psnr_values)\n",
    "        stats['psnr_std'] = np.std(psnr_values)\n",
    "        stats['ssim_mean'] = np.mean(ssim_values)\n",
    "        stats['ssim_std'] = np.std(ssim_values)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\n----- Dataset Statistics -----\")\n",
    "    print(f\"Resolution: Mode={stats['resolution_mode']}, Min={stats['resolution_min']}, Max={stats['resolution_max']}\")\n",
    "    print(f\"Train Brightness: Mean={stats['brightness_train_mean']:.2f}, Std={stats['brightness_train_std']:.2f}\")\n",
    "    print(f\"Train Contrast: Mean={stats['contrast_train_mean']:.2f}, Std={stats['contrast_train_std']:.2f}\")\n",
    "    \n",
    "    if gt_dir and brightness_stats['gt']:\n",
    "        print(f\"GT Brightness: Mean={stats['brightness_gt_mean']:.2f}, Std={stats['brightness_gt_std']:.2f}\")\n",
    "        print(f\"GT Contrast: Mean={stats['contrast_gt_mean']:.2f}, Std={stats['contrast_gt_std']:.2f}\")\n",
    "        print(f\"Noise Level: Mean={stats['noise_level_mean']:.2f}, Std={stats['noise_level_std']:.2f}\")\n",
    "        print(f\"PSNR: Mean={stats['psnr_mean']:.2f}dB, Std={stats['psnr_std']:.2f}dB\")\n",
    "        print(f\"SSIM: Mean={stats['ssim_mean']:.4f}, Std={stats['ssim_std']:.4f}\")\n",
    "    \n",
    "    # Save stats to CSV if requested\n",
    "    if save_stats:\n",
    "        stats_df = pd.DataFrame({k: [v] for k, v in stats.items() if not isinstance(v, tuple)})\n",
    "        stats_df.to_csv('dataset_stats.csv', index=False)\n",
    "        print(\"Statistics saved to dataset_stats.csv\")\n",
    "    \n",
    "    # Create plots if requested\n",
    "    if plot:\n",
    "        # Plot histograms\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Image histograms\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(histograms['train'] / sum(histograms['train']), 'r-', label='Train')\n",
    "        if gt_dir and brightness_stats['gt']:\n",
    "            plt.plot(histograms['gt'] / sum(histograms['gt']), 'g-', label='Ground Truth')\n",
    "        plt.title('Pixel Intensity Distribution')\n",
    "        plt.xlabel('Pixel Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Brightness comparison\n",
    "        plt.subplot(2, 2, 2)\n",
    "        sns.histplot(brightness_stats['train'], color='red', label='Train', kde=True, stat='density')\n",
    "        if gt_dir and brightness_stats['gt']:\n",
    "            sns.histplot(brightness_stats['gt'], color='green', label='Ground Truth', kde=True, stat='density')\n",
    "        plt.title('Image Brightness Distribution')\n",
    "        plt.xlabel('Mean Brightness')\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Contrast comparison\n",
    "        plt.subplot(2, 2, 3)\n",
    "        sns.histplot(contrast_stats['train'], color='red', label='Train', kde=True, stat='density')\n",
    "        if gt_dir and brightness_stats['gt']:\n",
    "            sns.histplot(contrast_stats['gt'], color='green', label='Ground Truth', kde=True, stat='density')\n",
    "        plt.title('Image Contrast Distribution')\n",
    "        plt.xlabel('Standard Deviation (Contrast)')\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend()\n",
    "        \n",
    "        # PSNR distribution\n",
    "        if gt_dir and psnr_values:\n",
    "            plt.subplot(2, 2, 4)\n",
    "            sns.histplot(psnr_values, kde=True, color='blue')\n",
    "            plt.title('PSNR Distribution')\n",
    "            plt.xlabel('PSNR (dB)')\n",
    "            plt.ylabel('Frequency')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('dataset_analysis.png')\n",
    "        plt.show()\n",
    "        \n",
    "        # Display sample images\n",
    "        num_samples = min(5, len(train_files))\n",
    "        plt.figure(figsize=(15, 4*num_samples))\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            # Load train image\n",
    "            train_path = os.path.join(train_dir, train_files[i])\n",
    "            train_img = np.array(Image.open(train_path).convert('RGB'))\n",
    "            \n",
    "            plt.subplot(num_samples, 2, 2*i+1)\n",
    "            plt.imshow(train_img)\n",
    "            plt.title(f'Train Image: {train_files[i]}')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Load gt image if available\n",
    "            if gt_dir:\n",
    "                gt_file = train_files[i]\n",
    "                if train_files[i].startswith('train_'):\n",
    "                    gt_file = train_files[i].replace('train_', 'gt_')\n",
    "                if gt_file in gt_files:\n",
    "                    gt_path = os.path.join(gt_dir, gt_file)\n",
    "                    gt_img = np.array(Image.open(gt_path).convert('RGB'))\n",
    "                    \n",
    "                    plt.subplot(num_samples, 2, 2*i+2)\n",
    "                    plt.imshow(gt_img)\n",
    "                    plt.title(f'GT Image: {gt_file}')\n",
    "                    plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('sample_images.png')\n",
    "        plt.show()\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def visualize_super_resolution_task(train_dir, gt_dir, num_samples=3):\n",
    "    \"\"\"\n",
    "    Visualizes the super-resolution task by showing low-res input and high-res output\n",
    "    \n",
    "    Args:\n",
    "        train_dir: Directory containing low-res noisy images\n",
    "        gt_dir: Directory containing high-res clean images (ground truth)\n",
    "        num_samples: Number of samples to visualize\n",
    "    \"\"\"\n",
    "    train_files = sorted([f for f in os.listdir(train_dir) if f.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))])\n",
    "    gt_files = sorted([f for f in os.listdir(gt_dir) if f.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))])\n",
    "    \n",
    "    plt.figure(figsize=(15, 5*num_samples))\n",
    "    \n",
    "    for i in range(min(num_samples, len(train_files))):\n",
    "        # Load train image\n",
    "        train_path = os.path.join(train_dir, train_files[i])\n",
    "        train_img = np.array(Image.open(train_path).convert('RGB'))\n",
    "        \n",
    "        # Find matching GT image\n",
    "        gt_file = train_files[i]\n",
    "        if train_files[i].startswith('train_'):\n",
    "            gt_file = train_files[i].replace('train_', 'gt_')\n",
    "        if gt_file in gt_files:\n",
    "            gt_path = os.path.join(gt_dir, gt_file)\n",
    "            gt_img = np.array(Image.open(gt_path).convert('RGB'))\n",
    "            \n",
    "            # Display images and size info\n",
    "            plt.subplot(num_samples, 3, 3*i+1)\n",
    "            plt.imshow(train_img)\n",
    "            plt.title(f'Low-res Input: {train_img.shape}')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Display upscaled input (bicubic)\n",
    "            if train_img.shape != gt_img.shape:\n",
    "                upscaled = cv2.resize(train_img, (gt_img.shape[1], gt_img.shape[0]), \n",
    "                                     interpolation=cv2.INTER_CUBIC)\n",
    "                plt.subplot(num_samples, 3, 3*i+2)\n",
    "                plt.imshow(upscaled)\n",
    "                plt.title(f'Bicubic Upscaled: {upscaled.shape}')\n",
    "                plt.axis('off')\n",
    "            \n",
    "            # Display ground truth\n",
    "            plt.subplot(num_samples, 3, 3*i+3)\n",
    "            plt.imshow(gt_img)\n",
    "            plt.title(f'High-res Target: {gt_img.shape}')\n",
    "            plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('super_resolution_examples.png')\n",
    "    plt.show()\n",
    "\n",
    "def estimate_model_complexity(train_dir, gt_dir):\n",
    "    \"\"\"\n",
    "    Estimates the complexity of the super-resolution and denoising task\n",
    "    based on image sizes, upscaling factor, and noise characteristics\n",
    "    \"\"\"\n",
    "    train_files = sorted([f for f in os.listdir(train_dir) if f.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))])\n",
    "    gt_files = sorted([f for f in os.listdir(gt_dir) if f.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))])\n",
    "    \n",
    "    scale_factors = []\n",
    "    noise_levels = []\n",
    "    \n",
    "    # Sample images for analysis\n",
    "    sample_size = min(50, len(train_files))\n",
    "    for i in range(sample_size):\n",
    "        # Load train image\n",
    "        train_path = os.path.join(train_dir, train_files[i])\n",
    "        train_img = np.array(Image.open(train_path).convert('RGB'))\n",
    "        \n",
    "        # Find matching GT image\n",
    "        gt_file = train_files[i]\n",
    "        if train_files[i].startswith('train_'):\n",
    "            gt_file = train_files[i].replace('train_', 'gt_')\n",
    "        if gt_file in gt_files:\n",
    "            gt_path = os.path.join(gt_dir, gt_file)\n",
    "            gt_img = np.array(Image.open(gt_path).convert('RGB'))\n",
    "            \n",
    "            # Calculate scale factor\n",
    "            h_scale = gt_img.shape[0] / train_img.shape[0]\n",
    "            w_scale = gt_img.shape[1] / train_img.shape[1]\n",
    "            scale_factors.append((h_scale, w_scale))\n",
    "            \n",
    "            # Resize ground truth to match input size for noise comparison\n",
    "            gt_resized = cv2.resize(gt_img, (train_img.shape[1], train_img.shape[0]), \n",
    "                                   interpolation=cv2.INTER_AREA)\n",
    "            \n",
    "            # Calculate noise level (RMSE)\n",
    "            noise = np.sqrt(np.mean((train_img.astype(float) - gt_resized.astype(float)) ** 2))\n",
    "            noise_levels.append(noise)\n",
    "    \n",
    "    # Calculate average scale factor\n",
    "    avg_h_scale = np.mean([s[0] for s in scale_factors])\n",
    "    avg_w_scale = np.mean([s[1] for s in scale_factors])\n",
    "    \n",
    "    # Calculate average noise level\n",
    "    avg_noise = np.mean(noise_levels)\n",
    "    \n",
    "    print(\"\\n----- Task Complexity Analysis -----\")\n",
    "    print(f\"Average Scale Factor: {avg_h_scale:.2f}x{avg_w_scale:.2f}\")\n",
    "    print(f\"Average Noise Level: {avg_noise:.2f}\")\n",
    "    \n",
    "    # Determine complexity based on findings\n",
    "    if avg_h_scale >= 3.5 and avg_w_scale >= 3.5:\n",
    "        sr_complexity = \"High (4x or higher)\"\n",
    "    elif avg_h_scale >= 1.5 and avg_w_scale >= 1.5:\n",
    "        sr_complexity = \"Medium (2x)\"\n",
    "    else:\n",
    "        sr_complexity = \"Low (less than 2x)\"\n",
    "    \n",
    "    if avg_noise > 30:\n",
    "        noise_complexity = \"High (significant noise)\"\n",
    "    elif avg_noise > 15:\n",
    "        noise_complexity = \"Medium (moderate noise)\"\n",
    "    else:\n",
    "        noise_complexity = \"Low (minimal noise)\"\n",
    "    \n",
    "    print(f\"Super-Resolution Complexity: {sr_complexity}\")\n",
    "    print(f\"Denoising Complexity: {noise_complexity}\")\n",
    "    \n",
    "    # Suggest model architectures based on complexity\n",
    "    print(\"\\nRecommended Model Architectures:\")\n",
    "    if sr_complexity == \"High (4x or higher)\" and noise_complexity == \"High (significant noise)\":\n",
    "        print(\"- Advanced GAN-based models (e.g., ESRGAN with noise handling)\")\n",
    "        print(\"- Deep residual networks with attention mechanisms\")\n",
    "        print(\"- Two-stage pipeline: denoising followed by super-resolution\")\n",
    "    elif noise_complexity == \"High (significant noise)\":\n",
    "        print(\"- Dedicated denoising networks (e.g., DnCNN) followed by SR\")\n",
    "        print(\"- Attention-based models that can focus on noise patterns\")\n",
    "    else:\n",
    "        print(\"- EDSR or SRResNet based architectures\")\n",
    "        print(\"- Lighter GAN-based models\")\n",
    "    \n",
    "    return {\n",
    "        'avg_scale_factor': (avg_h_scale, avg_w_scale),\n",
    "        'avg_noise': avg_noise,\n",
    "        'sr_complexity': sr_complexity,\n",
    "        'noise_complexity': noise_complexity\n",
    "    }\n",
    "\n",
    "def analyze_test_data(test_dir):\n",
    "    \"\"\"\n",
    "    Analyzes test data to understand its characteristics\n",
    "    \"\"\"\n",
    "    print(f\"Analyzing test data in {test_dir}...\")\n",
    "    \n",
    "    # Get all image files\n",
    "    test_files = sorted([f for f in os.listdir(test_dir) if f.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))])\n",
    "    \n",
    "    # Image count\n",
    "    total_images = len(test_files)\n",
    "    print(f\"Total test images found: {total_images}\")\n",
    "    \n",
    "    # Initialize data containers\n",
    "    resolutions = []\n",
    "    brightness_stats = []\n",
    "    contrast_stats = []\n",
    "    histograms = np.zeros(256)\n",
    "    \n",
    "    # Analyze each image\n",
    "    for i, test_file in enumerate(tqdm(test_files[:min(100, len(test_files))], desc=\"Analyzing test images\")):\n",
    "        # Load test image\n",
    "        test_path = os.path.join(test_dir, test_file)\n",
    "        test_img = np.array(Image.open(test_path).convert('RGB'))\n",
    "        \n",
    "        # Store resolution\n",
    "        resolutions.append(test_img.shape[:2])\n",
    "        \n",
    "        # Calculate brightness\n",
    "        test_brightness = np.mean(test_img)\n",
    "        brightness_stats.append(test_brightness)\n",
    "        \n",
    "        # Calculate contrast\n",
    "        test_contrast = np.std(test_img)\n",
    "        contrast_stats.append(test_contrast)\n",
    "        \n",
    "        # Update histogram\n",
    "        for channel in range(3):\n",
    "            histograms += np.histogram(test_img[:,:,channel], bins=256, range=(0, 256))[0]\n",
    "    \n",
    "    # Compute statistics\n",
    "    resolution_min = min(resolutions, key=lambda x: x[0] * x[1])\n",
    "    resolution_max = max(resolutions, key=lambda x: x[0] * x[1])\n",
    "    resolution_mode = max(set(resolutions), key=resolutions.count)\n",
    "    \n",
    "    brightness_mean = np.mean(brightness_stats)\n",
    "    brightness_std = np.std(brightness_stats)\n",
    "    contrast_mean = np.mean(contrast_stats)\n",
    "    contrast_std = np.std(contrast_stats)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\n----- Test Dataset Statistics -----\")\n",
    "    print(f\"Resolution: Mode={resolution_mode}, Min={resolution_min}, Max={resolution_max}\")\n",
    "    print(f\"Brightness: Mean={brightness_mean:.2f}, Std={brightness_std:.2f}\")\n",
    "    print(f\"Contrast: Mean={contrast_mean:.2f}, Std={contrast_std:.2f}\")\n",
    "    \n",
    "    # Create plots\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Image histograms\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(histograms / sum(histograms), 'b-')\n",
    "    plt.title('Test Images Pixel Intensity Distribution')\n",
    "    plt.xlabel('Pixel Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Brightness histogram\n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.histplot(brightness_stats, color='blue', kde=True, stat='density')\n",
    "    plt.title('Test Images Brightness Distribution')\n",
    "    plt.xlabel('Mean Brightness')\n",
    "    plt.ylabel('Density')\n",
    "    \n",
    "    # Contrast histogram\n",
    "    plt.subplot(1, 3, 3)\n",
    "    sns.histplot(contrast_stats, color='blue', kde=True, stat='density')\n",
    "    plt.title('Test Images Contrast Distribution')\n",
    "    plt.xlabel('Standard Deviation (Contrast)')\n",
    "    plt.ylabel('Density')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('test_data_analysis.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Display sample test images\n",
    "    num_samples = min(5, len(test_files))\n",
    "    plt.figure(figsize=(15, 3*num_samples))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Load test image\n",
    "        test_path = os.path.join(test_dir, test_files[i])\n",
    "        test_img = np.array(Image.open(test_path).convert('RGB'))\n",
    "        \n",
    "        plt.subplot(num_samples, 1, i+1)\n",
    "        plt.imshow(test_img)\n",
    "        plt.title(f'Test Image: {test_files[i]} - Shape: {test_img.shape}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('test_sample_images.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'total_images': total_images,\n",
    "        'resolution_min': resolution_min,\n",
    "        'resolution_max': resolution_max,\n",
    "        'resolution_mode': resolution_mode,\n",
    "        'brightness_mean': brightness_mean,\n",
    "        'brightness_std': brightness_std,\n",
    "        'contrast_mean': contrast_mean,\n",
    "        'contrast_std': contrast_std\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Use the provided Kaggle paths\n",
    "    base_dir = \"/kaggle/input/dlp-jan-2025-nppe-3/archive\"\n",
    "    train_dir = os.path.join(base_dir, \"train/train\")\n",
    "    gt_dir = os.path.join(base_dir, \"train/gt\")\n",
    "    val_dir = os.path.join(base_dir, \"val/val\")\n",
    "    val_gt_dir = os.path.join(base_dir, \"val/gt\")\n",
    "    test_dir = os.path.join(base_dir, \"test\")\n",
    "    \n",
    "    # Analyze training data\n",
    "    print(\"\\n===== ANALYZING TRAINING DATA =====\")\n",
    "    train_stats = analyze_dataset(train_dir, gt_dir)\n",
    "    \n",
    "    # Analyze validation data\n",
    "    print(\"\\n===== ANALYZING VALIDATION DATA =====\")\n",
    "    val_stats = analyze_dataset(val_dir, val_gt_dir)\n",
    "    \n",
    "    # Analyze test data\n",
    "    print(\"\\n===== ANALYZING TEST DATA =====\")\n",
    "    test_stats = analyze_test_data(test_dir)\n",
    "    \n",
    "    # Visualize super-resolution task\n",
    "    print(\"\\n===== VISUALIZING SUPER-RESOLUTION TASK =====\")\n",
    "    visualize_super_resolution_task(train_dir, gt_dir)\n",
    "    \n",
    "    # Estimate model complexity\n",
    "    print(\"\\n===== ESTIMATING MODEL COMPLEXITY =====\")\n",
    "    complexity = estimate_model_complexity(train_dir, gt_dir)\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Please use these insights to build an appropriate model.\")\n",
    "    print(\"Based on the analysis results, please share the key statistics with me so I can recommend the best model architecture.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98422aeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T17:22:07.250581Z",
     "iopub.status.busy": "2025-04-05T17:22:07.250345Z",
     "iopub.status.idle": "2025-04-05T19:11:24.193142Z",
     "shell.execute_reply": "2025-04-05T19:11:24.192133Z"
    },
    "papermill": {
     "duration": 6557.347685,
     "end_time": "2025-04-05T19:11:24.594919",
     "exception": false,
     "start_time": "2025-04-05T17:22:07.247234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03354c60932f45db96774407d5b1d013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/548 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bcbb769ba9542e98acfd76d76f7f407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
      "100%|| 548M/548M [00:02<00:00, 219MB/s]\n",
      "<ipython-input-2-9c48cfac95b3>:264: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Training:   0%|          | 0/139 [00:00<?, ?it/s]<ipython-input-2-9c48cfac95b3>:276: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "<ipython-input-2-9c48cfac95b3>:291: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/50 - Training: 100%|| 139/139 [01:14<00:00,  1.86it/s]\n",
      "Epoch 1/50 - Validation: 100%|| 34/34 [00:51<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.7836, PSNR: 25.11dB | Val Loss: 0.4636, PSNR: 31.73dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 - Training: 100%|| 139/139 [01:18<00:00,  1.76it/s]\n",
      "Epoch 2/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 - Train Loss: 0.5260, PSNR: 30.94dB | Val Loss: 0.3969, PSNR: 33.14dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 - Training: 100%|| 139/139 [01:18<00:00,  1.76it/s]\n",
      "Epoch 3/50 - Validation: 100%|| 34/34 [00:49<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 - Train Loss: 0.4827, PSNR: 32.23dB | Val Loss: 0.3714, PSNR: 33.64dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 - Training: 100%|| 139/139 [01:19<00:00,  1.76it/s]\n",
      "Epoch 4/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 - Train Loss: 0.5064, PSNR: 32.01dB | Val Loss: 0.3907, PSNR: 32.87dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 - Training: 100%|| 139/139 [01:18<00:00,  1.77it/s]\n",
      "Epoch 5/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 - Train Loss: 0.4663, PSNR: 32.82dB | Val Loss: 0.3508, PSNR: 34.42dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 - Training: 100%|| 139/139 [01:19<00:00,  1.75it/s]\n",
      "Epoch 6/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 - Train Loss: 0.4528, PSNR: 33.17dB | Val Loss: 0.3386, PSNR: 35.13dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 - Training: 100%|| 139/139 [01:18<00:00,  1.76it/s]\n",
      "Epoch 7/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 - Train Loss: 0.4471, PSNR: 33.22dB | Val Loss: 0.3401, PSNR: 34.23dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 - Training: 100%|| 139/139 [01:19<00:00,  1.76it/s]\n",
      "Epoch 8/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 - Train Loss: 0.4420, PSNR: 33.47dB | Val Loss: 0.3271, PSNR: 35.64dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 - Training: 100%|| 139/139 [01:19<00:00,  1.76it/s]\n",
      "Epoch 9/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 - Train Loss: 0.4332, PSNR: 33.69dB | Val Loss: 0.3178, PSNR: 36.14dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 - Training: 100%|| 139/139 [01:19<00:00,  1.76it/s]\n",
      "Epoch 10/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 - Train Loss: 0.4342, PSNR: 33.60dB | Val Loss: 0.3154, PSNR: 36.20dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 - Training: 100%|| 139/139 [01:18<00:00,  1.77it/s]\n",
      "Epoch 11/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 - Train Loss: 0.4310, PSNR: 33.61dB | Val Loss: 0.3110, PSNR: 36.37dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 - Training: 100%|| 139/139 [01:18<00:00,  1.76it/s]\n",
      "Epoch 12/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 - Train Loss: 0.4273, PSNR: 33.70dB | Val Loss: 0.3166, PSNR: 35.98dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 - Training: 100%|| 139/139 [01:18<00:00,  1.77it/s]\n",
      "Epoch 13/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 - Train Loss: 0.4253, PSNR: 33.83dB | Val Loss: 0.3072, PSNR: 36.62dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 - Training: 100%|| 139/139 [01:19<00:00,  1.76it/s]\n",
      "Epoch 14/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 - Train Loss: 0.4210, PSNR: 33.93dB | Val Loss: 0.3069, PSNR: 36.65dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 - Training: 100%|| 139/139 [01:18<00:00,  1.77it/s]\n",
      "Epoch 15/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 - Train Loss: 0.4198, PSNR: 33.96dB | Val Loss: 0.3059, PSNR: 36.45dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 - Training: 100%|| 139/139 [01:19<00:00,  1.76it/s]\n",
      "Epoch 16/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 - Train Loss: 0.4185, PSNR: 33.96dB | Val Loss: 0.3032, PSNR: 36.74dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50 - Training: 100%|| 139/139 [01:19<00:00,  1.76it/s]\n",
      "Epoch 17/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50 - Train Loss: 0.4203, PSNR: 33.92dB | Val Loss: 0.3022, PSNR: 36.60dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 - Training: 100%|| 139/139 [01:18<00:00,  1.77it/s]\n",
      "Epoch 18/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 - Train Loss: 0.4200, PSNR: 33.96dB | Val Loss: 0.3005, PSNR: 36.87dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50 - Training: 100%|| 139/139 [01:18<00:00,  1.76it/s]\n",
      "Epoch 19/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50 - Train Loss: 0.4148, PSNR: 34.06dB | Val Loss: 0.3028, PSNR: 36.85dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50 - Training: 100%|| 139/139 [01:18<00:00,  1.76it/s]\n",
      "Epoch 20/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50 - Train Loss: 0.4169, PSNR: 33.98dB | Val Loss: 0.3018, PSNR: 36.83dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 - Training: 100%|| 139/139 [01:18<00:00,  1.77it/s]\n",
      "Epoch 21/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 - Train Loss: 0.4153, PSNR: 34.02dB | Val Loss: 0.2992, PSNR: 36.99dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50 - Training: 100%|| 139/139 [01:19<00:00,  1.76it/s]\n",
      "Epoch 22/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50 - Train Loss: 0.4181, PSNR: 33.98dB | Val Loss: 0.2972, PSNR: 36.95dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50 - Training: 100%|| 139/139 [01:18<00:00,  1.77it/s]\n",
      "Epoch 23/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50 - Train Loss: 0.4158, PSNR: 34.01dB | Val Loss: 0.2997, PSNR: 36.87dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50 - Training: 100%|| 139/139 [01:18<00:00,  1.77it/s]\n",
      "Epoch 24/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50 - Train Loss: 0.4150, PSNR: 33.99dB | Val Loss: 0.3016, PSNR: 36.55dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50 - Training: 100%|| 139/139 [01:18<00:00,  1.76it/s]\n",
      "Epoch 25/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50 - Train Loss: 0.4103, PSNR: 34.16dB | Val Loss: 0.2954, PSNR: 36.97dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50 - Training: 100%|| 139/139 [01:18<00:00,  1.77it/s]\n",
      "Epoch 26/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50 - Train Loss: 0.4113, PSNR: 34.12dB | Val Loss: 0.2962, PSNR: 37.11dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50 - Training: 100%|| 139/139 [01:18<00:00,  1.76it/s]\n",
      "Epoch 27/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50 - Train Loss: 0.4113, PSNR: 34.11dB | Val Loss: 0.2945, PSNR: 37.07dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50 - Training: 100%|| 139/139 [01:18<00:00,  1.76it/s]\n",
      "Epoch 28/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50 - Train Loss: 0.4116, PSNR: 34.10dB | Val Loss: 0.2942, PSNR: 37.14dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50 - Training: 100%|| 139/139 [01:18<00:00,  1.76it/s]\n",
      "Epoch 29/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50 - Train Loss: 0.4068, PSNR: 34.27dB | Val Loss: 0.2931, PSNR: 37.22dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50 - Training: 100%|| 139/139 [01:18<00:00,  1.76it/s]\n",
      "Epoch 30/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50 - Train Loss: 0.4086, PSNR: 34.18dB | Val Loss: 0.2927, PSNR: 37.20dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50 - Training: 100%|| 139/139 [01:19<00:00,  1.76it/s]\n",
      "Epoch 31/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50 - Train Loss: 0.4100, PSNR: 34.17dB | Val Loss: 0.2925, PSNR: 37.19dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50 - Training: 100%|| 139/139 [01:18<00:00,  1.76it/s]\n",
      "Epoch 32/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50 - Train Loss: 0.4089, PSNR: 34.17dB | Val Loss: 0.2957, PSNR: 37.12dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50 - Training: 100%|| 139/139 [01:18<00:00,  1.77it/s]\n",
      "Epoch 33/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50 - Train Loss: 0.4019, PSNR: 34.36dB | Val Loss: 0.2924, PSNR: 37.22dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50 - Training: 100%|| 139/139 [01:18<00:00,  1.77it/s]\n",
      "Epoch 34/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50 - Train Loss: 0.4080, PSNR: 34.20dB | Val Loss: 0.2930, PSNR: 37.21dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50 - Training: 100%|| 139/139 [01:18<00:00,  1.77it/s]\n",
      "Epoch 35/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50 - Train Loss: 0.4061, PSNR: 34.24dB | Val Loss: 0.2929, PSNR: 37.22dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50 - Training: 100%|| 139/139 [01:18<00:00,  1.76it/s]\n",
      "Epoch 36/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50 - Train Loss: 0.4067, PSNR: 34.26dB | Val Loss: 0.2921, PSNR: 37.27dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50 - Training: 100%|| 139/139 [01:18<00:00,  1.76it/s]\n",
      "Epoch 37/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50 - Train Loss: 0.4076, PSNR: 34.24dB | Val Loss: 0.2904, PSNR: 37.33dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50 - Training: 100%|| 139/139 [01:18<00:00,  1.76it/s]\n",
      "Epoch 38/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50 - Train Loss: 0.4067, PSNR: 34.25dB | Val Loss: 0.2911, PSNR: 37.31dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50 - Training: 100%|| 139/139 [01:19<00:00,  1.76it/s]\n",
      "Epoch 39/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50 - Train Loss: 0.4051, PSNR: 34.29dB | Val Loss: 0.2915, PSNR: 37.26dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50 - Training: 100%|| 139/139 [01:19<00:00,  1.76it/s]\n",
      "Epoch 40/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50 - Train Loss: 0.4063, PSNR: 34.25dB | Val Loss: 0.2918, PSNR: 37.19dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50 - Training: 100%|| 139/139 [01:18<00:00,  1.76it/s]\n",
      "Epoch 41/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50 - Train Loss: 0.4053, PSNR: 34.27dB | Val Loss: 0.2895, PSNR: 37.33dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50 - Training: 100%|| 139/139 [01:19<00:00,  1.76it/s]\n",
      "Epoch 42/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50 - Train Loss: 0.4092, PSNR: 34.16dB | Val Loss: 0.2905, PSNR: 37.31dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50 - Training: 100%|| 139/139 [01:19<00:00,  1.76it/s]\n",
      "Epoch 43/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50 - Train Loss: 0.4043, PSNR: 34.27dB | Val Loss: 0.2924, PSNR: 37.23dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50 - Training: 100%|| 139/139 [01:18<00:00,  1.77it/s]\n",
      "Epoch 44/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50 - Train Loss: 0.4057, PSNR: 34.25dB | Val Loss: 0.2904, PSNR: 37.32dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50 - Training: 100%|| 139/139 [01:19<00:00,  1.76it/s]\n",
      "Epoch 45/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50 - Train Loss: 0.4032, PSNR: 34.33dB | Val Loss: 0.2910, PSNR: 37.32dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50 - Training: 100%|| 139/139 [01:18<00:00,  1.77it/s]\n",
      "Epoch 46/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50 - Train Loss: 0.4060, PSNR: 34.26dB | Val Loss: 0.2897, PSNR: 37.36dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50 - Training: 100%|| 139/139 [01:18<00:00,  1.76it/s]\n",
      "Epoch 47/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50 - Train Loss: 0.4037, PSNR: 34.31dB | Val Loss: 0.2898, PSNR: 37.35dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50 - Training: 100%|| 139/139 [01:18<00:00,  1.77it/s]\n",
      "Epoch 48/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50 - Train Loss: 0.4062, PSNR: 34.25dB | Val Loss: 0.2900, PSNR: 37.35dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50 - Training: 100%|| 139/139 [01:18<00:00,  1.76it/s]\n",
      "Epoch 49/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50 - Train Loss: 0.4033, PSNR: 34.29dB | Val Loss: 0.2894, PSNR: 37.36dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Training: 100%|| 139/139 [01:18<00:00,  1.76it/s]\n",
      "Epoch 50/50 - Validation: 100%|| 34/34 [00:50<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.4056, PSNR: 34.26dB | Val Loss: 0.2895, PSNR: 37.36dB\n",
      "Running inference on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference: 100%|| 15/15 [00:09<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved to /kaggle/working/output/submission.csv\n",
      "Submission file created at /kaggle/working/output/submission.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "from PIL import Image\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from transformers import Dinov2Model\n",
    "from PIL import Image\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "class Config:\n",
    "    BASE_DIR = \"/kaggle/input/dlp-jan-2025-nppe-3/archive\"\n",
    "    TRAIN_LR_PATH = os.path.join(BASE_DIR, \"train/train\")\n",
    "    TRAIN_HR_PATH = os.path.join(BASE_DIR, \"train/gt\")\n",
    "    VAL_LR_PATH = os.path.join(BASE_DIR, \"val/val\")\n",
    "    VAL_HR_PATH = os.path.join(BASE_DIR, \"val/gt\")\n",
    "    TEST_LR_PATH = os.path.join(BASE_DIR, \"test\")\n",
    "    OUTPUT_PATH = '/kaggle/working/output'\n",
    "    CHECKPOINT_PATH = '/kaggle/working/checkpoints'\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_WORKERS = 2\n",
    "    PATCH_SIZE = 128\n",
    "    SR_SCALE = 4\n",
    "    NUM_EPOCHS = 50\n",
    "    NUM_ITERATIONS = 8  # increased iterations for deeper refinement\n",
    "    BASE_LR = 1e-4\n",
    "    L1_WEIGHT = 1.0\n",
    "    CHARBONNIER_WEIGHT = 0.5\n",
    "    ADV_WEIGHT = 0.001  # adversarial loss weight\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "    os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "class LowLightDataset(Dataset):\n",
    "    def __init__(self, lr_path, hr_path=None, patch_size=128, scale=4, augment=True,\n",
    "                 noise_min=0, noise_max=0.1, is_test=False):\n",
    "        self.lr_path = lr_path\n",
    "        self.hr_path = hr_path\n",
    "        self.patch_size = patch_size\n",
    "        self.scale = scale\n",
    "        self.augment = augment\n",
    "        self.noise_min = noise_min\n",
    "        self.noise_max = noise_max\n",
    "        self.is_test = is_test\n",
    "        self.lr_images = sorted([f for f in os.listdir(lr_path)\n",
    "                                 if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))])\n",
    "        if hr_path is not None and os.path.exists(hr_path):\n",
    "            self.hr_images = sorted([f for f in os.listdir(hr_path)\n",
    "                                     if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))])\n",
    "        else:\n",
    "            self.hr_images = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr_img = Image.open(os.path.join(self.lr_path, self.lr_images[idx])).convert('L')\n",
    "        lr_img = torch.from_numpy(np.array(lr_img)).float().unsqueeze(0) / 255.0\n",
    "\n",
    "        hr_img = None\n",
    "        if self.hr_path is not None and self.hr_images is not None:\n",
    "            hr_img = Image.open(os.path.join(self.hr_path, self.hr_images[idx])).convert('L')\n",
    "            hr_img = torch.from_numpy(np.array(hr_img)).float().unsqueeze(0) / 255.0\n",
    "\n",
    "        # Optional curriculum learning: adjust augmentation/noise as training evolves\n",
    "        if self.augment and not self.is_test and hr_img is not None:\n",
    "            h, w = lr_img.shape[1], lr_img.shape[2]\n",
    "            if h > self.patch_size and w > self.patch_size:\n",
    "                h_start = random.randint(0, h - self.patch_size)\n",
    "                w_start = random.randint(0, w - self.patch_size)\n",
    "                lr_img = lr_img[:, h_start:h_start + self.patch_size, w_start:w_start + self.patch_size]\n",
    "                hr_h_start, hr_w_start = h_start * self.scale, w_start * self.scale\n",
    "                hr_img = hr_img[:, hr_h_start:hr_h_start + self.patch_size * self.scale,\n",
    "                                hr_w_start:hr_w_start + self.patch_size * self.scale]\n",
    "            if random.random() < 0.5:\n",
    "                lr_img = torch.flip(lr_img, [1])\n",
    "                hr_img = torch.flip(hr_img, [1])\n",
    "            if random.random() < 0.5:\n",
    "                lr_img = torch.flip(lr_img, [2])\n",
    "                hr_img = torch.flip(hr_img, [2])\n",
    "            if random.random() < 0.5:\n",
    "                noise_level = random.uniform(self.noise_min, self.noise_max)\n",
    "                lr_img = torch.clamp(lr_img + torch.randn_like(lr_img) * noise_level, 0, 1)\n",
    "        if self.is_test:\n",
    "            return {'lr': lr_img, 'name': self.lr_images[idx]}\n",
    "        return {'lr': lr_img, 'hr': hr_img, 'name': self.lr_images[idx]}\n",
    "\n",
    "def compute_spatial_dims(num_tokens, target_aspect_ratio):\n",
    "    best = None\n",
    "    best_diff = float('inf')\n",
    "    for h_candidate in range(1, num_tokens + 1):\n",
    "        if num_tokens % h_candidate == 0:\n",
    "            w_candidate = num_tokens // h_candidate\n",
    "            ratio = h_candidate / w_candidate\n",
    "            diff = abs(ratio - target_aspect_ratio)\n",
    "            if diff < best_diff:\n",
    "                best_diff = diff\n",
    "                best = (h_candidate, w_candidate)\n",
    "    return best\n",
    "\n",
    "\n",
    "class DiffusionRefinementBlock(nn.Module):\n",
    "    def __init__(self, in_channels, feature_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, feature_dim, kernel_size=3, padding=1)\n",
    "        self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.conv2 = nn.Conv2d(feature_dim, feature_dim, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        cat = torch.cat([x, cond], dim=1)\n",
    "        out = self.conv1(cat)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        return x + out  # Residual connection\n",
    "\n",
    "\n",
    "class SR3Dinov2Model(nn.Module):\n",
    "    def __init__(self, scale_factor=4, grayscale=True, num_iterations=8):\n",
    "        super().__init__()\n",
    "        self.input_channels = 1 if grayscale else 3\n",
    "        self.output_channels = 1 if grayscale else 3\n",
    "        if grayscale:\n",
    "            self.to_rgb = nn.Conv2d(1, 3, kernel_size=1)\n",
    "        self.dinov2 = Dinov2Model.from_pretrained(\"facebook/dinov2-base\")\n",
    "        for i in range(-5, 0):\n",
    "            for param in self.dinov2.encoder.layer[i].parameters():\n",
    "                param.requires_grad = False\n",
    "        dinov2_dim = 768\n",
    "        condition_dim = 64\n",
    "\n",
    "        self.adapter = nn.Sequential(\n",
    "            nn.Conv2d(dinov2_dim, condition_dim, kernel_size=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.shallow = nn.Sequential(\n",
    "            nn.Conv2d(1, condition_dim, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.iterations = num_iterations\n",
    "        self.diffusion_blocks = nn.ModuleList([\n",
    "            DiffusionRefinementBlock(in_channels=condition_dim * 2, feature_dim=condition_dim)\n",
    "            for _ in range(num_iterations)\n",
    "        ])\n",
    "        self.upsampling = nn.Sequential(\n",
    "            nn.Conv2d(condition_dim, condition_dim * 4, kernel_size=3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(condition_dim, condition_dim * 4, kernel_size=3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.output_conv = nn.Conv2d(condition_dim, self.output_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_rgb = self.to_rgb(x) if self.input_channels == 1 else x\n",
    "        with torch.no_grad():\n",
    "            outputs = self.dinov2(x_rgb)\n",
    "            features = outputs.last_hidden_state[:, 1:, :]\n",
    "        num_tokens = features.shape[1]\n",
    "        target_ratio = x.shape[-2] / x.shape[-1]\n",
    "        h_spatial, w_spatial = compute_spatial_dims(num_tokens, target_ratio)\n",
    "        features_reshaped = features.transpose(1, 2).reshape(x.shape[0], 768, h_spatial, w_spatial)\n",
    "        condition = self.adapter(features_reshaped)\n",
    "        shallow_feat = self.shallow(x)\n",
    "        if condition.shape[-2:] != shallow_feat.shape[-2:]:\n",
    "            condition = F.interpolate(condition, size=shallow_feat.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        feat = shallow_feat\n",
    "        for block in self.diffusion_blocks:\n",
    "            feat = block(feat, condition)\n",
    "        upscaled = self.upsampling(feat)\n",
    "        output = self.output_conv(upscaled)\n",
    "        return output\n",
    "\n",
    "\n",
    "class CharbonnierLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        diff = x - y\n",
    "        return torch.mean(torch.sqrt(diff * diff + self.eps))\n",
    "\n",
    "\n",
    "class VGGPerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Using VGG19's first few layers for feature extraction\n",
    "        vgg = torchvision.models.vgg19(pretrained=True).features\n",
    "        self.layers = nn.Sequential(*list(vgg[:16])).eval()\n",
    "        for param in self.layers.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # If inputs are grayscale, replicate channels\n",
    "        if x.size(1) == 1:\n",
    "            x = x.repeat(1, 3, 1, 1)\n",
    "            y = y.repeat(1, 3, 1, 1)\n",
    "        x_features = self.layers(x)\n",
    "        y_features = self.layers(y)\n",
    "        return F.l1_loss(x_features, y_features)\n",
    "\n",
    "\n",
    "def ssim_loss(img1, img2, window_size=11):\n",
    "    C1 = 0.01 ** 2\n",
    "    C2 = 0.03 ** 2\n",
    "    mu1 = F.avg_pool2d(img1, window_size, stride=1, padding=window_size // 2)\n",
    "    mu2 = F.avg_pool2d(img2, window_size, stride=1, padding=window_size // 2)\n",
    "    sigma1_sq = F.avg_pool2d(img1 * img1, window_size, stride=1, padding=window_size // 2) - mu1 ** 2\n",
    "    sigma2_sq = F.avg_pool2d(img2 * img2, window_size, stride=1, padding=window_size // 2) - mu2 ** 2\n",
    "    sigma12 = F.avg_pool2d(img1 * img2, window_size, stride=1, padding=window_size // 2) - mu1 * mu2\n",
    "    ssim_map = ((2 * mu1 * mu2 + C1) * (2 * sigma12 + C2)) / ((mu1 ** 2 + mu2 ** 2 + C1) * (sigma1_sq + sigma2_sq + C2))\n",
    "    return 1 - ssim_map.mean()\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_channels=1):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 1, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def train_model(model, discriminator, train_loader, val_loader, adv_loss_fn,\n",
    "                optimizer_G, optimizer_D, scheduler_G, scheduler_D, vgg_loss, epochs, device):\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    best_psnr = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        discriminator.train()\n",
    "        train_loss, train_psnr = 0.0, 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs} - Training\"):\n",
    "            lr_imgs = batch['lr'].to(device)\n",
    "            hr_imgs = batch['hr'].to(device)\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                sr_imgs = model(lr_imgs)\n",
    "                loss_l1 = F.l1_loss(sr_imgs, hr_imgs)\n",
    "                loss_charb = CharbonnierLoss()(sr_imgs, hr_imgs)\n",
    "                loss_perc = vgg_loss(sr_imgs, hr_imgs)\n",
    "                loss_ssim = ssim_loss(sr_imgs, hr_imgs)\n",
    "                g_loss = loss_l1 + loss_charb + loss_perc + loss_ssim\n",
    "                pred_fake = discriminator(sr_imgs)\n",
    "                adv_loss = adv_loss_fn(pred_fake, torch.ones_like(pred_fake))\n",
    "                total_g_loss = g_loss + cfg.ADV_WEIGHT * adv_loss\n",
    "            scaler.scale(total_g_loss).backward()\n",
    "            scaler.step(optimizer_G)\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                pred_real = discriminator(hr_imgs)\n",
    "                loss_real = adv_loss_fn(pred_real, torch.ones_like(pred_real))\n",
    "                pred_fake_detach = discriminator(sr_imgs.detach())\n",
    "                loss_fake = adv_loss_fn(pred_fake_detach, torch.zeros_like(pred_fake_detach))\n",
    "                d_loss = 0.5 * (loss_real + loss_fake)\n",
    "            scaler.scale(d_loss).backward()\n",
    "            scaler.step(optimizer_D)\n",
    "            scaler.update()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                psnr = 10 * torch.log10(1 / F.mse_loss(sr_imgs, hr_imgs))\n",
    "            train_loss += total_g_loss.item()\n",
    "            train_psnr += psnr.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_psnr /= len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_psnr = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{epochs} - Validation\"):\n",
    "                lr_imgs = batch['lr'].to(device)\n",
    "                hr_imgs = batch['hr'].to(device)\n",
    "                sr_imgs = model(lr_imgs)\n",
    "                loss_l1 = F.l1_loss(sr_imgs, hr_imgs)\n",
    "                loss_charb = CharbonnierLoss()(sr_imgs, hr_imgs)\n",
    "                loss_perc = vgg_loss(sr_imgs, hr_imgs)\n",
    "                loss_ssim = ssim_loss(sr_imgs, hr_imgs)\n",
    "                total_loss = loss_l1 + loss_charb + loss_perc + loss_ssim\n",
    "                psnr = 10 * torch.log10(1 / F.mse_loss(sr_imgs, hr_imgs))\n",
    "                val_loss += total_loss.item()\n",
    "                val_psnr += psnr.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        val_psnr /= len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Train Loss: {train_loss:.4f}, PSNR: {train_psnr:.2f}dB | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, PSNR: {val_psnr:.2f}dB\")\n",
    "\n",
    "        if val_psnr > best_psnr:\n",
    "            best_psnr = val_psnr\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "                'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'psnr': val_psnr\n",
    "            }, os.path.join(cfg.CHECKPOINT_PATH, 'model_best.pth'))\n",
    "        scheduler_G.step()\n",
    "        scheduler_D.step()\n",
    "    return model\n",
    "\n",
    "\n",
    "def inference(model, test_loader, output_dir, device):\n",
    "    model.eval()\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Running inference\"):\n",
    "            lr_imgs = batch['lr'].to(device)\n",
    "            names = batch['name']\n",
    "            sr_imgs = model(lr_imgs)\n",
    "            sr_imgs = torch.clamp(sr_imgs, 0, 1)\n",
    "            for i, name in enumerate(names):\n",
    "                output_name = name.replace('test_', 'gt_')\n",
    "                save_path = os.path.join(output_dir, output_name)\n",
    "                save_img = (sr_imgs[i].cpu().squeeze(0).numpy() * 255).astype(np.uint8)\n",
    "                Image.fromarray(save_img).save(save_path)\n",
    "\n",
    "\n",
    "def images_to_csv(folder_path, output_csv):\n",
    "    data_rows = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            image = Image.open(image_path).convert('L')\n",
    "            image_array = np.array(image).flatten()[::8]\n",
    "            image_id = filename.split('.')[0]\n",
    "            data_rows.append([image_id, *image_array])\n",
    "    column_names = ['ID'] + [f'pixel_{i}' for i in range(len(data_rows[0]) - 1)]\n",
    "    df = pd.DataFrame(data_rows, columns=column_names)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Successfully saved to {output_csv}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    set_seed()\n",
    "    for directory in [cfg.TRAIN_LR_PATH, cfg.TRAIN_HR_PATH, cfg.VAL_LR_PATH, cfg.VAL_HR_PATH, cfg.TEST_LR_PATH]:\n",
    "        if not os.path.exists(directory):\n",
    "            print(f\"Warning: Directory {directory} does not exist!\")\n",
    "    train_dataset = LowLightDataset(\n",
    "        lr_path=cfg.TRAIN_LR_PATH,\n",
    "        hr_path=cfg.TRAIN_HR_PATH,\n",
    "        patch_size=cfg.PATCH_SIZE,\n",
    "        scale=cfg.SR_SCALE,\n",
    "        augment=True,\n",
    "        noise_min=0.01,\n",
    "        noise_max=0.1\n",
    "    )\n",
    "    val_dataset = LowLightDataset(\n",
    "        lr_path=cfg.VAL_LR_PATH,\n",
    "        hr_path=cfg.VAL_HR_PATH,\n",
    "        patch_size=cfg.PATCH_SIZE,\n",
    "        scale=cfg.SR_SCALE,\n",
    "        augment=False\n",
    "    )\n",
    "    test_dataset = LowLightDataset(\n",
    "        lr_path=cfg.TEST_LR_PATH,\n",
    "        hr_path=None,\n",
    "        is_test=True\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                              shuffle=True, num_workers=cfg.NUM_WORKERS, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                            shuffle=False, num_workers=cfg.NUM_WORKERS, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=4,\n",
    "                             shuffle=False, num_workers=cfg.NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "    model = SR3Dinov2Model(scale_factor=cfg.SR_SCALE, grayscale=True, num_iterations=cfg.NUM_ITERATIONS).to(cfg.DEVICE)\n",
    "    discriminator = Discriminator(input_channels=1).to(cfg.DEVICE)\n",
    "    vgg_loss = VGGPerceptualLoss().to(cfg.DEVICE)\n",
    "    adv_loss_fn = nn.BCEWithLogitsLoss().to(cfg.DEVICE)\n",
    "\n",
    "    optimizer_G = optim.AdamW(model.parameters(), lr=cfg.BASE_LR)\n",
    "    optimizer_D = optim.AdamW(discriminator.parameters(), lr=cfg.BASE_LR)\n",
    "    scheduler_G = optim.lr_scheduler.CosineAnnealingLR(optimizer_G, T_max=cfg.NUM_EPOCHS, eta_min=1e-6)\n",
    "    scheduler_D = optim.lr_scheduler.CosineAnnealingLR(optimizer_D, T_max=cfg.NUM_EPOCHS, eta_min=1e-6)\n",
    "\n",
    "    print(\"Training model...\")\n",
    "    model = train_model(\n",
    "        model, discriminator, train_loader, val_loader, adv_loss_fn,\n",
    "        optimizer_G, optimizer_D, scheduler_G, scheduler_D, vgg_loss,\n",
    "        cfg.NUM_EPOCHS, cfg.DEVICE\n",
    "    )\n",
    "\n",
    "    print(\"Running inference on test set...\")\n",
    "    output_dir = os.path.join(cfg.OUTPUT_PATH, 'results')\n",
    "    inference(model, test_loader, output_dir, cfg.DEVICE)\n",
    "    submission_csv = os.path.join(cfg.OUTPUT_PATH, 'submission.csv')\n",
    "    images_to_csv(output_dir, submission_csv)\n",
    "    print(f\"Submission file created at {submission_csv}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11639668,
     "sourceId": 97753,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6564.278553,
   "end_time": "2025-04-05T19:11:28.790877",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-05T17:22:04.512324",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03354c60932f45db96774407d5b1d013": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_94fd142e0ed840248fb6790798f2a031",
        "IPY_MODEL_95f2139e667a426cb2e24d12e60f20a0",
        "IPY_MODEL_97213d22ea6d4ff388261c4d7f88f5f6"
       ],
       "layout": "IPY_MODEL_0de6eccf7bfa4c35809325ff807d2e22",
       "tabbable": null,
       "tooltip": null
      }
     },
     "05e57a06b2424b8a9a7ba726a39363a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "08d283dd788f4e5e8978c8fb1c7ab06c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0a45e1c6356449fbba5992f324fb2c91": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0de6eccf7bfa4c35809325ff807d2e22": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1d48013bdcaa431a88e2317bca7a018b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5c6a77124c4b4b929eb9c66f1c54e04a",
       "max": 346345912.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_05e57a06b2424b8a9a7ba726a39363a5",
       "tabbable": null,
       "tooltip": null,
       "value": 346345912.0
      }
     },
     "2fb947dc000c4135addb79e016fb485e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a01744cba03f43d3ab60672113efad77",
       "placeholder": "",
       "style": "IPY_MODEL_3009243201b44c58ab60f64d3caf09b5",
       "tabbable": null,
       "tooltip": null,
       "value": "346M/346M[00:00&lt;00:00,373MB/s]"
      }
     },
     "3009243201b44c58ab60f64d3caf09b5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "502bd7f875f140dfb3d4d7aca2eb514f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5bcb5caf61c84fdebb6eeaa17fe385de": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_beaffa3d28a74393bfb5dddf9e1ed065",
       "placeholder": "",
       "style": "IPY_MODEL_0a45e1c6356449fbba5992f324fb2c91",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors:100%"
      }
     },
     "5bcbb769ba9542e98acfd76d76f7f407": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5bcb5caf61c84fdebb6eeaa17fe385de",
        "IPY_MODEL_1d48013bdcaa431a88e2317bca7a018b",
        "IPY_MODEL_2fb947dc000c4135addb79e016fb485e"
       ],
       "layout": "IPY_MODEL_c57ad797f18d4e06a2ea6410879dc77e",
       "tabbable": null,
       "tooltip": null
      }
     },
     "5c6a77124c4b4b929eb9c66f1c54e04a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "72ad63296f364737b2bbb7959ef5c079": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8dbfabe962a04835b82fdbaca6ef281d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "94cfca15a1ed4d078596282eacf2701f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "94fd142e0ed840248fb6790798f2a031": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_502bd7f875f140dfb3d4d7aca2eb514f",
       "placeholder": "",
       "style": "IPY_MODEL_72ad63296f364737b2bbb7959ef5c079",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json:100%"
      }
     },
     "95f2139e667a426cb2e24d12e60f20a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_08d283dd788f4e5e8978c8fb1c7ab06c",
       "max": 548.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_94cfca15a1ed4d078596282eacf2701f",
       "tabbable": null,
       "tooltip": null,
       "value": 548.0
      }
     },
     "97213d22ea6d4ff388261c4d7f88f5f6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8dbfabe962a04835b82fdbaca6ef281d",
       "placeholder": "",
       "style": "IPY_MODEL_ae7b38d84a7d4b58baa1f8dbca0b3f4e",
       "tabbable": null,
       "tooltip": null,
       "value": "548/548[00:00&lt;00:00,52.9kB/s]"
      }
     },
     "a01744cba03f43d3ab60672113efad77": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ae7b38d84a7d4b58baa1f8dbca0b3f4e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "beaffa3d28a74393bfb5dddf9e1ed065": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c57ad797f18d4e06a2ea6410879dc77e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
