{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f12b712",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T09:30:36.946745Z",
     "iopub.status.busy": "2025-03-31T09:30:36.946494Z",
     "iopub.status.idle": "2025-03-31T09:30:42.481883Z",
     "shell.execute_reply": "2025-03-31T09:30:42.480655Z"
    },
    "papermill": {
     "duration": 5.540275,
     "end_time": "2025-03-31T09:30:42.483757",
     "exception": false,
     "start_time": "2025-03-31T09:30:36.943482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting piqa\r\n",
      "  Downloading piqa-1.3.2-py3-none-any.whl.metadata (5.8 kB)\r\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\r\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.5)\r\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\r\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\r\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\r\n",
      "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\r\n",
      "Collecting pytorch-msssim\r\n",
      "  Downloading pytorch_msssim-1.0.0-py3-none-any.whl.metadata (8.0 kB)\r\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\r\n",
      "Collecting torch_optimizer\r\n",
      "  Downloading torch_optimizer-0.3.0-py3-none-any.whl.metadata (55 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.9.0.post0)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\r\n",
      "Collecting pytorch-ranger>=0.1.1 (from torch_optimizer)\r\n",
      "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl.metadata (509 bytes)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\r\n",
      "Downloading piqa-1.3.2-py3-none-any.whl (32 kB)\r\n",
      "Downloading pytorch_msssim-1.0.0-py3-none-any.whl (7.7 kB)\r\n",
      "Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\r\n",
      "Installing collected packages: pytorch-ranger, pytorch-msssim, torch_optimizer, piqa\r\n",
      "Successfully installed piqa-1.3.2 pytorch-msssim-1.0.0 pytorch-ranger-0.1.1 torch_optimizer-0.3.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install piqa opencv-python matplotlib scikit-learn pillow tqdm torch torchvision opencv-contrib-python pytorch-msssim transformers torch_optimizer pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a73916e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T09:30:42.491189Z",
     "iopub.status.busy": "2025-03-31T09:30:42.490915Z",
     "iopub.status.idle": "2025-03-31T09:30:47.512260Z",
     "shell.execute_reply": "2025-03-31T09:30:47.511284Z"
    },
    "papermill": {
     "duration": 5.026367,
     "end_time": "2025-03-31T09:30:47.513697",
     "exception": false,
     "start_time": "2025-03-31T09:30:42.487330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\r\n",
      "Collecting albumentations\r\n",
      "  Downloading albumentations-2.0.5-py3-none-any.whl.metadata (41 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\r\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\r\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.11.0a2)\r\n",
      "Collecting albucore==0.0.23 (from albumentations)\r\n",
      "  Downloading albucore-0.0.23-py3-none-any.whl.metadata (5.3 kB)\r\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\r\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.23->albumentations) (3.11.1)\r\n",
      "Collecting simsimd>=5.9.2 (from albucore==0.0.23->albumentations)\r\n",
      "  Downloading simsimd-6.2.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (66 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2.4.1)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (2.29.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (4.12.2)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24.4->albumentations) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24.4->albumentations) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.24.4->albumentations) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.24.4->albumentations) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.24.4->albumentations) (2024.2.0)\r\n",
      "Downloading albumentations-2.0.5-py3-none-any.whl (290 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.6/290.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading albucore-0.0.23-py3-none-any.whl (14 kB)\r\n",
      "Downloading simsimd-6.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (632 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m632.7/632.7 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: simsimd, albucore, albumentations\r\n",
      "  Attempting uninstall: albucore\r\n",
      "    Found existing installation: albucore 0.0.19\r\n",
      "    Uninstalling albucore-0.0.19:\r\n",
      "      Successfully uninstalled albucore-0.0.19\r\n",
      "  Attempting uninstall: albumentations\r\n",
      "    Found existing installation: albumentations 1.4.20\r\n",
      "    Uninstalling albumentations-1.4.20:\r\n",
      "      Successfully uninstalled albumentations-1.4.20\r\n",
      "Successfully installed albucore-0.0.23 albumentations-2.0.5 simsimd-6.2.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -U albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89fb92dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T09:30:47.522670Z",
     "iopub.status.busy": "2025-03-31T09:30:47.522441Z",
     "iopub.status.idle": "2025-03-31T12:31:11.558632Z",
     "shell.execute_reply": "2025-03-31T12:31:11.557611Z"
    },
    "papermill": {
     "duration": 10824.758786,
     "end_time": "2025-03-31T12:31:12.276560",
     "exception": false,
     "start_time": "2025-03-31T09:30:47.517774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-f4882d847d17>:73: UserWarning: Argument(s) 'max_holes, max_height, max_width' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(max_holes=8, max_height=16, max_width=16, p=0.3),\n",
      "<ipython-input-3-f4882d847d17>:75: UserWarning: Argument(s) 'std_limit' are not valid for transform GaussNoise\n",
      "  A.GaussNoise(std_limit=(10 / 255, 50 / 255), p=0.5),\n",
      "Epoch 1 Training: 100%|██████████| 418/418 [05:47<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Training Loss: 0.7442, RMSE: 0.9252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:13<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.6917, RMSE: 0.8491\n",
      "Low-Light Loss: 0.6908\n",
      "Best model saved with val loss: 0.6917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training: 100%|██████████| 418/418 [05:46<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 - Training Loss: 0.6650, RMSE: 0.7948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:13<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.6046, RMSE: 0.6923\n",
      "Low-Light Loss: 0.6043\n",
      "Best model saved with val loss: 0.6046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training: 100%|██████████| 418/418 [05:46<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 - Training Loss: 0.6128, RMSE: 0.7191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:12<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.6047, RMSE: 0.7001\n",
      "Low-Light Loss: 0.6053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training: 100%|██████████| 418/418 [05:45<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 - Training Loss: 0.6074, RMSE: 0.7144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:12<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.6050, RMSE: 0.6965\n",
      "Low-Light Loss: 0.6055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training: 100%|██████████| 418/418 [05:45<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 - Training Loss: 0.6054, RMSE: 0.7122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:13<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.6040, RMSE: 0.7018\n",
      "Low-Light Loss: 0.6047\n",
      "Best model saved with val loss: 0.6040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Training: 100%|██████████| 418/418 [05:46<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 - Training Loss: 0.6031, RMSE: 0.7081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:13<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5693, RMSE: 0.6512\n",
      "Low-Light Loss: 0.5687\n",
      "Best model saved with val loss: 0.5693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Training: 100%|██████████| 418/418 [05:46<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 - Training Loss: 0.5934, RMSE: 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:12<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5635, RMSE: 0.6396\n",
      "Low-Light Loss: 0.5631\n",
      "Best model saved with val loss: 0.5635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Training: 100%|██████████| 418/418 [05:46<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 - Training Loss: 0.5886, RMSE: 0.6844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:13<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5569, RMSE: 0.6353\n",
      "Low-Light Loss: 0.5555\n",
      "Best model saved with val loss: 0.5569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 Training: 100%|██████████| 418/418 [05:45<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 - Training Loss: 0.5833, RMSE: 0.6767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:13<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5560, RMSE: 0.6294\n",
      "Low-Light Loss: 0.5553\n",
      "Best model saved with val loss: 0.5560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 Training: 100%|██████████| 418/418 [05:46<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 - Training Loss: 0.5800, RMSE: 0.6708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:13<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5477, RMSE: 0.6256\n",
      "Low-Light Loss: 0.5464\n",
      "Best model saved with val loss: 0.5477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 Training: 100%|██████████| 418/418 [05:44<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 - Training Loss: 0.5766, RMSE: 0.6640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:13<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5450, RMSE: 0.6243\n",
      "Low-Light Loss: 0.5437\n",
      "Best model saved with val loss: 0.5450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 Training: 100%|██████████| 418/418 [05:45<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 - Training Loss: 0.5778, RMSE: 0.6629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:12<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5435, RMSE: 0.6177\n",
      "Low-Light Loss: 0.5422\n",
      "Best model saved with val loss: 0.5435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 Training: 100%|██████████| 418/418 [05:44<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 - Training Loss: 0.5793, RMSE: 0.6644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:13<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5467, RMSE: 0.6210\n",
      "Low-Light Loss: 0.5457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 Training: 100%|██████████| 418/418 [05:43<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 - Training Loss: 0.5822, RMSE: 0.6664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:13<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5464, RMSE: 0.6226\n",
      "Low-Light Loss: 0.5452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 Training: 100%|██████████| 418/418 [05:44<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 - Training Loss: 0.5837, RMSE: 0.6674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:13<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5475, RMSE: 0.6260\n",
      "Low-Light Loss: 0.5462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 Training: 100%|██████████| 418/418 [05:45<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 - Training Loss: 0.5850, RMSE: 0.6701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:12<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5527, RMSE: 0.6293\n",
      "Low-Light Loss: 0.5514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 Training: 100%|██████████| 418/418 [05:44<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 - Training Loss: 0.5843, RMSE: 0.6658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:12<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5486, RMSE: 0.6258\n",
      "Low-Light Loss: 0.5472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 Training: 100%|██████████| 418/418 [05:45<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 - Training Loss: 0.5807, RMSE: 0.6591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:13<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5458, RMSE: 0.6234\n",
      "Low-Light Loss: 0.5442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 Training: 100%|██████████| 418/418 [05:45<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 - Training Loss: 0.5791, RMSE: 0.6566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:13<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5421, RMSE: 0.6103\n",
      "Low-Light Loss: 0.5409\n",
      "Best model saved with val loss: 0.5421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 Training: 100%|██████████| 418/418 [05:44<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 - Training Loss: 0.5809, RMSE: 0.6579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:13<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5429, RMSE: 0.6120\n",
      "Low-Light Loss: 0.5415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21 Training: 100%|██████████| 418/418 [05:45<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 - Training Loss: 0.5781, RMSE: 0.6535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:13<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5407, RMSE: 0.6088\n",
      "Low-Light Loss: 0.5396\n",
      "Best model saved with val loss: 0.5407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22 Training: 100%|██████████| 418/418 [05:46<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 - Training Loss: 0.5748, RMSE: 0.6507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:13<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5369, RMSE: 0.6109\n",
      "Low-Light Loss: 0.5355\n",
      "Best model saved with val loss: 0.5369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23 Training: 100%|██████████| 418/418 [05:46<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 - Training Loss: 0.5713, RMSE: 0.6436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:13<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5332, RMSE: 0.6011\n",
      "Low-Light Loss: 0.5323\n",
      "Best model saved with val loss: 0.5332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24 Training: 100%|██████████| 418/418 [05:47<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 - Training Loss: 0.5691, RMSE: 0.6408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:13<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5302, RMSE: 0.5967\n",
      "Low-Light Loss: 0.5293\n",
      "Best model saved with val loss: 0.5302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25 Training: 100%|██████████| 418/418 [05:46<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 - Training Loss: 0.5680, RMSE: 0.6370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:13<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5295, RMSE: 0.5960\n",
      "Low-Light Loss: 0.5285\n",
      "Best model saved with val loss: 0.5295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26 Training: 100%|██████████| 418/418 [05:46<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 - Training Loss: 0.5681, RMSE: 0.6359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:13<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5324, RMSE: 0.6125\n",
      "Low-Light Loss: 0.5308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27 Training: 100%|██████████| 418/418 [05:46<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 - Training Loss: 0.5669, RMSE: 0.6337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:13<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5261, RMSE: 0.5907\n",
      "Low-Light Loss: 0.5250\n",
      "Best model saved with val loss: 0.5261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28 Training: 100%|██████████| 418/418 [05:46<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 - Training Loss: 0.5664, RMSE: 0.6327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:13<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5247, RMSE: 0.5921\n",
      "Low-Light Loss: 0.5232\n",
      "Best model saved with val loss: 0.5247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29 Training: 100%|██████████| 418/418 [05:46<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 - Training Loss: 0.5668, RMSE: 0.6337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:13<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5265, RMSE: 0.5931\n",
      "Low-Light Loss: 0.5252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30 Training: 100%|██████████| 418/418 [05:46<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 - Training Loss: 0.5668, RMSE: 0.6316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:13<00:00,  4.00it/s]\n",
      "<ipython-input-3-f4882d847d17>:575: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(\"best_model.pth\", map_location=self.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5231, RMSE: 0.5924\n",
      "Low-Light Loss: 0.5216\n",
      "Best model saved with val loss: 0.5231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Predictions: 100%|██████████| 53/53 [00:23<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved to submission.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from pytorch_msssim import SSIM\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# ------------------- Configuration & Reproducibility -------------------\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Reproducibility\n",
    "        self.seed = 42\n",
    "        # Device settings\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Data & Image settings\n",
    "        self.image_size = 256\n",
    "        self.output_size = 128\n",
    "        self.batch_size = 16\n",
    "        self.num_workers = 4\n",
    "        # Training settings\n",
    "        self.num_epochs = 30\n",
    "        self.base_lr = 2e-5\n",
    "        self.weight_decay = 1e-4\n",
    "        self.use_amp = True\n",
    "        self.clip_value = 1.0\n",
    "        # Paths (update these paths as needed)\n",
    "        self.train_rgb_dir = \"/kaggle/input/depth-estimation/competition-data/competition-data/training/images\"\n",
    "        self.train_depth_dir = \"/kaggle/input/depth-estimation/competition-data/competition-data/training/depths\"\n",
    "        self.val_rgb_dir = \"/kaggle/input/depth-estimation/competition-data/competition-data/validation/images\"\n",
    "        self.val_depth_dir = \"/kaggle/input/depth-estimation/competition-data/competition-data/validation/depths\"\n",
    "        self.test_rgb_dir = \"/kaggle/input/depth-estimation/competition-data/competition-data/testing/images\"\n",
    "        self.output_dir = \"predictions_folder\"\n",
    "        self.csv_path = \"submission.csv\"\n",
    "        # Depth normalization parameters\n",
    "        self.global_mean = 88.2930\n",
    "        self.global_std = 61.7739\n",
    "\n",
    "def set_global_seeds(seed: int):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ------------------- Data Augmentations & Preprocessing -------------------\n",
    "def apply_clahe(image: np.ndarray, **kwargs) -> np.ndarray:\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l)\n",
    "    merged = cv2.merge((cl, a, b))\n",
    "    return cv2.cvtColor(merged, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "def get_transforms(config: Config):\n",
    "    train_transform = A.Compose([\n",
    "        A.Lambda(image=apply_clahe),\n",
    "        A.Resize(config.image_size, config.image_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.CoarseDropout(max_holes=8, max_height=16, max_width=16, p=0.3),\n",
    "        A.RandomBrightnessContrast(p=0.3),\n",
    "        A.GaussNoise(std_limit=(10 / 255, 50 / 255), p=0.5),\n",
    "        A.MotionBlur(blur_limit=7, p=0.3),  # Add motion blur\n",
    "        A.ISONoise(p=0.5),\n",
    "        A.HueSaturationValue(p=0.3),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                    std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "    ], additional_targets={\"mask\": \"mask\"})\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Lambda(image=apply_clahe),\n",
    "        A.Resize(config.image_size, config.image_size),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                    std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "    ], additional_targets={\"mask\": \"mask\"})\n",
    "\n",
    "    return train_transform, val_transform\n",
    "\n",
    "\n",
    "# ------------------- Dataset -------------------\n",
    "class DepthDataset(Dataset):\n",
    "    def __init__(self, rgb_dir: str, depth_dir: Optional[str] = None,\n",
    "                 transform: Optional[A.Compose] = None, is_val: bool = False):\n",
    "        self.rgb_dir = rgb_dir\n",
    "        self.depth_dir = depth_dir\n",
    "        self.transform = transform\n",
    "        self.filenames = sorted(os.listdir(rgb_dir), key=lambda x: int(''.join(filter(str.isdigit, x)) or 0))\n",
    "        self.is_val = is_val\n",
    "        self.low_light_indices = []\n",
    "        self.noisy_indices = []\n",
    "        if is_val and depth_dir:\n",
    "            self._split_validation_set()\n",
    "\n",
    "    def _split_validation_set(self):\n",
    "        for idx, fname in enumerate(self.filenames):\n",
    "            rgb_path = os.path.join(self.rgb_dir, fname)\n",
    "            image = cv2.imread(rgb_path, cv2.IMREAD_COLOR)\n",
    "            brightness = np.mean(image)\n",
    "            noise_level = np.std(image)\n",
    "            if brightness < 50:\n",
    "                self.low_light_indices.append(idx)\n",
    "            if noise_level > 30:\n",
    "                self.noisy_indices.append(idx)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        fname = self.filenames[idx]\n",
    "        rgb_path = os.path.join(self.rgb_dir, fname)\n",
    "        image = cv2.imread(rgb_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        depth = None\n",
    "        if self.depth_dir:\n",
    "            depth_path = os.path.join(self.depth_dir, fname)\n",
    "            depth = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED).astype(np.float32)\n",
    "            depth = cv2.resize(depth, (config.image_size, config.image_size))\n",
    "            depth = (depth - config.global_mean) / (config.global_std + 1e-8)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=depth if depth is not None else image)\n",
    "            image = augmented[\"image\"]\n",
    "            if depth is not None:\n",
    "                depth = augmented[\"mask\"]\n",
    "        if depth is not None:\n",
    "            depth = torch.from_numpy(depth).float() if isinstance(depth, np.ndarray) else depth.clone().detach().float()\n",
    "            depth = depth.unsqueeze(0)\n",
    "            depth = F.interpolate(depth.unsqueeze(0), size=(config.output_size, config.output_size),\n",
    "                                  mode='bilinear', align_corners=False).squeeze(0)\n",
    "        return (image, depth) if depth is not None else (image, fname)\n",
    "\n",
    "# ------------------- Model Components -------------------\n",
    "def conv_block(in_channels: int, out_channels: int, dropout: float = 0.0):\n",
    "    layers = [\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    ]\n",
    "    if dropout > 0:\n",
    "        layers.append(nn.Dropout2d(dropout))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "# Channel Attention (already provided)\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes: int, ratio: int = 16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        return self.sigmoid(avg_out + max_out)\n",
    "\n",
    "# Spatial Attention Module\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size=7, padding=3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
    "        return self.sigmoid(self.conv1(x_cat))\n",
    "\n",
    "# Residual Block used in the upgraded denoising branch\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += identity\n",
    "        return self.relu(out)\n",
    "\n",
    "# Upgraded Denoising Branch\n",
    "class DenoisingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenoisingNet, self).__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.resblock1 = ResidualBlock(64)\n",
    "        self.resblock2 = ResidualBlock(64)\n",
    "        self.resblock3 = ResidualBlock(64)\n",
    "        self.out_conv = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
    "        self.out_act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.initial(x)\n",
    "        x = self.resblock1(x)\n",
    "        x = self.resblock2(x)\n",
    "        x = self.resblock3(x)\n",
    "        x = self.out_conv(x)\n",
    "        return self.out_act(x)\n",
    "\n",
    "# Coarse Network\n",
    "class CoarseNet(nn.Module):\n",
    "    def __init__(self, output_size: int):\n",
    "        super(CoarseNet, self).__init__()\n",
    "        self.encoder1 = conv_block(3, 64, dropout=0.2)\n",
    "        self.encoder2 = conv_block(64, 128, dropout=0.2)\n",
    "        # Increase capacity: add an extra block or increase channels\n",
    "        self.encoder3 = conv_block(128, 256, dropout=0.2)\n",
    "        # Extra block: increasing channels further\n",
    "        self.encoder3_extra = conv_block(256, 256, dropout=0.2)\n",
    "        self.encoder4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, dilation=2, padding=2),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.up4 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder4 = conv_block(512, 256, dropout=0.2)\n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder3 = conv_block(256, 128, dropout=0.2)\n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder2 = conv_block(128, 64, dropout=0.2)\n",
    "        self.decoder1 = nn.Conv2d(64, 1, kernel_size=3, padding=1)\n",
    "        self.final_pool = nn.AdaptiveAvgPool2d((output_size, output_size))\n",
    "    \n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(self.pool(e1))\n",
    "        e3 = self.encoder3(self.pool(e2))\n",
    "        # Pass through an extra block for more capacity\n",
    "        e3 = self.encoder3_extra(e3)\n",
    "        e4 = self.encoder4(self.pool(e3))\n",
    "        u4 = self.up4(e4)\n",
    "        d4 = self.decoder4(torch.cat([u4, e3], dim=1))\n",
    "        u3 = self.up3(d4)\n",
    "        d3 = self.decoder3(torch.cat([u3, e2], dim=1))\n",
    "        u2 = self.up2(d3)\n",
    "        d2 = self.decoder2(torch.cat([u2, e1], dim=1))\n",
    "        coarse = self.decoder1(d2)\n",
    "        return self.final_pool(coarse)\n",
    "\n",
    "# Fine Network with Spatial Attention and Residual Connections\n",
    "class FineNet(nn.Module):\n",
    "    def __init__(self, output_size: int):\n",
    "        super(FineNet, self).__init__()\n",
    "        self.conv1 = conv_block(4, 64)\n",
    "        self.ca1 = ChannelAttention(64)\n",
    "        self.sa1 = SpatialAttention()\n",
    "        self.conv2 = conv_block(64, 64)\n",
    "        self.ca2 = ChannelAttention(64)\n",
    "        self.sa2 = SpatialAttention()\n",
    "        # Adding an extra conv layer for more depth\n",
    "        self.conv_extra = conv_block(64, 64)\n",
    "        # Residual connection from conv1 output\n",
    "        self.res_conv = nn.Conv2d(64, 32, kernel_size=1)\n",
    "        # Adjust channels for concatenation: 64 (from extra) + 32 (residual) = 96\n",
    "        self.conv3 = conv_block(96, 32)\n",
    "        self.out_conv = nn.Conv2d(32, 1, kernel_size=3, padding=1)\n",
    "        self.final_pool = nn.AdaptiveAvgPool2d((output_size, output_size))\n",
    "\n",
    "    def forward(self, coarse: torch.Tensor, rgb: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.cat([coarse, rgb], dim=1)\n",
    "        x1 = self.conv1(x)\n",
    "        x1 = x1 * self.ca1(x1)\n",
    "        x1 = x1 * self.sa1(x1)\n",
    "        x2 = self.conv2(x1)\n",
    "        x2 = x2 * self.ca2(x2)\n",
    "        x2 = x2 * self.sa2(x2)\n",
    "        # Extra layer for added capacity\n",
    "        x_extra = self.conv_extra(x2)\n",
    "        res = self.res_conv(x1)\n",
    "        x_cat = torch.cat([x_extra, res], dim=1)\n",
    "        x3 = self.conv3(x_cat)\n",
    "        refined = self.out_conv(x3)\n",
    "        return self.final_pool(refined)\n",
    "\n",
    "# Combined Model\n",
    "class DepthEstimationModel(nn.Module):\n",
    "    def __init__(self, output_size: int):\n",
    "        super(DepthEstimationModel, self).__init__()\n",
    "        # Upgraded denoising branch\n",
    "        self.denoising_branch = DenoisingNet()\n",
    "        self.coarse_net = CoarseNet(output_size)\n",
    "        self.fine_net = FineNet(output_size)\n",
    "        self.final_resize = nn.AdaptiveAvgPool2d((output_size, output_size))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        denoised = self.denoising_branch(x)\n",
    "        coarse = self.coarse_net(denoised)\n",
    "        # Resize denoised output for fine network\n",
    "        denoised_resized = F.interpolate(denoised, size=(config.output_size, config.output_size),\n",
    "                                         mode='bilinear', align_corners=False)\n",
    "        refined = self.fine_net(coarse, denoised_resized)\n",
    "        return self.final_resize(refined)\n",
    "\n",
    "# Improved Discriminator Architecture\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Extra layer for improved capacity\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, 4, 1, 0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.main(x).view(-1, 1)\n",
    "\n",
    "# ------------------- Loss Functions -------------------\n",
    "class DepthLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DepthLoss, self).__init__()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.huber_loss = nn.HuberLoss()\n",
    "        # Use SSIM for a single channel depth map\n",
    "        self.ssim = SSIM(data_range=1.0, size_average=True, channel=1)\n",
    "\n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor, lambda_weight: float = 0.5) -> torch.Tensor:\n",
    "        l1 = self.l1_loss(pred, target)\n",
    "        huber = self.huber_loss(pred, target)\n",
    "        combined_loss = lambda_weight * l1 + (1 - lambda_weight) * huber\n",
    "        ssim_loss = 1 - self.ssim(pred, target)\n",
    "        return 0.6 * combined_loss + 0.4 * ssim_loss\n",
    "\n",
    "# Edge-Aware Smoothness Loss\n",
    "import torch.nn.functional as F\n",
    "class EdgeAwareSmoothnessLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EdgeAwareSmoothnessLoss, self).__init__()\n",
    "\n",
    "    def forward(self, depth: torch.Tensor, image: torch.Tensor) -> torch.Tensor:\n",
    "        # Resize image to match depth dimensions if needed\n",
    "        if image.shape[2:] != depth.shape[2:]:\n",
    "            image = F.interpolate(image, size=depth.shape[2:], mode='bilinear', align_corners=False)\n",
    "        # Convert image to grayscale (averaging channels)\n",
    "        image_gray = torch.mean(image, dim=1, keepdim=True)\n",
    "        dx_depth = torch.abs(depth[:, :, :, :-1] - depth[:, :, :, 1:])\n",
    "        dy_depth = torch.abs(depth[:, :, :-1, :] - depth[:, :, 1:, :])\n",
    "        dx_image = torch.abs(image_gray[:, :, :, :-1] - image_gray[:, :, :, 1:])\n",
    "        dy_image = torch.abs(image_gray[:, :, :-1, :] - image_gray[:, :, 1:, :])\n",
    "        weight_x = torch.exp(-dx_image)\n",
    "        weight_y = torch.exp(-dy_image)\n",
    "        smoothness_x = dx_depth * weight_x\n",
    "        smoothness_y = dy_depth * weight_y\n",
    "        return torch.mean(smoothness_x) + torch.mean(smoothness_y)\n",
    "\n",
    "\n",
    "class GANLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, pred: torch.Tensor, target_is_real: bool) -> torch.Tensor:\n",
    "        target = torch.full_like(pred, 0.9) if target_is_real else torch.full_like(pred, 0.1)\n",
    "        return self.bce_loss(pred, target)\n",
    "\n",
    "def compute_rmse(pred: torch.Tensor, target: torch.Tensor) -> float:\n",
    "    pred_np = pred.detach().cpu().numpy().flatten()\n",
    "    target_np = target.detach().cpu().numpy().flatten()\n",
    "    return np.sqrt(np.mean((pred_np - target_np) ** 2))\n",
    "\n",
    "# ------------------- Trainer Class -------------------\n",
    "class DepthEstimationTrainer:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.device = config.device\n",
    "        set_global_seeds(config.seed)\n",
    "        self.train_transform, self.val_transform = get_transforms(config)\n",
    "        self._setup_data()\n",
    "        self._setup_model_and_optimizer()\n",
    "\n",
    "    def _setup_data(self):\n",
    "        self.train_dataset = DepthDataset(\n",
    "            self.config.train_rgb_dir,\n",
    "            self.config.train_depth_dir,\n",
    "            transform=self.train_transform\n",
    "        )\n",
    "        self.val_dataset = DepthDataset(\n",
    "            self.config.val_rgb_dir,\n",
    "            self.config.val_depth_dir,\n",
    "            transform=self.val_transform,\n",
    "            is_val=True\n",
    "        )\n",
    "        self.test_dataset = DepthDataset(\n",
    "            self.config.test_rgb_dir,\n",
    "            transform=self.val_transform\n",
    "        )\n",
    "        self.train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.config.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        self.val_loader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.config.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        self.test_loader = DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.config.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    def _setup_model_and_optimizer(self):\n",
    "        self.model = DepthEstimationModel(self.config.output_size).to(self.device)\n",
    "        self.discriminator = Discriminator().to(self.device)\n",
    "        self.depth_criterion = DepthLoss().to(self.device)\n",
    "        self.smoothness_criterion = EdgeAwareSmoothnessLoss().to(self.device)\n",
    "        self.gan_criterion = GANLoss().to(self.device)\n",
    "        self.optimizer_g = optim.AdamW(self.model.parameters(), lr=self.config.base_lr,\n",
    "                                       weight_decay=self.config.weight_decay)\n",
    "        self.optimizer_d = optim.AdamW(self.discriminator.parameters(), lr=self.config.base_lr,\n",
    "                                       weight_decay=self.config.weight_decay)\n",
    "        self.scheduler = CosineAnnealingWarmRestarts(self.optimizer_g, T_0=5, T_mult=2)\n",
    "        self.scaler = GradScaler(enabled=self.config.use_amp)\n",
    "        self.best_val_loss = float('inf')\n",
    "\n",
    "    def train_epoch(self, epoch: int):\n",
    "        self.model.train()\n",
    "        self.discriminator.train()\n",
    "        train_loss, rmse_sum = 0.0, 0.0\n",
    "        # Curriculum: gradually increase adversarial weight from 0 to 0.1\n",
    "        adv_weight = min(0.1 * ((epoch + 1) / self.config.num_epochs), 0.1)\n",
    "        for images, depths in tqdm(self.train_loader, desc=f\"Epoch {epoch + 1} Training\"):\n",
    "            images, depths = images.to(self.device), depths.to(self.device)\n",
    "\n",
    "            # Train Discriminator\n",
    "            self.optimizer_d.zero_grad()\n",
    "            with autocast('cuda' if torch.cuda.is_available() else 'cpu', enabled=self.config.use_amp):\n",
    "                fake_depth = self.model(images)\n",
    "                d_real = self.discriminator(depths)\n",
    "                d_fake = self.discriminator(fake_depth.detach())\n",
    "                d_loss = 0.5 * (self.gan_criterion(d_real, True) + self.gan_criterion(d_fake, False))\n",
    "            self.scaler.scale(d_loss).backward()\n",
    "            self.scaler.step(self.optimizer_d)\n",
    "            self.scaler.update()\n",
    "\n",
    "            # Train Generator\n",
    "            self.optimizer_g.zero_grad()\n",
    "            with autocast('cuda' if torch.cuda.is_available() else 'cpu', enabled=self.config.use_amp):\n",
    "                fake_depth = self.model(images)\n",
    "                g_loss_adv = self.gan_criterion(self.discriminator(fake_depth), True)\n",
    "                g_loss_depth = self.depth_criterion(fake_depth, depths)\n",
    "                smooth_loss = self.smoothness_criterion(fake_depth, images)\n",
    "                g_loss = g_loss_depth + 0.1 * adv_weight * g_loss_adv + 0.01 * smooth_loss\n",
    "            self.scaler.scale(g_loss).backward()\n",
    "            self.scaler.unscale_(self.optimizer_g)\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.clip_value)\n",
    "            self.scaler.step(self.optimizer_g)\n",
    "            self.scaler.update()\n",
    "\n",
    "            train_loss += g_loss.item()\n",
    "            rmse_sum += compute_rmse(fake_depth, depths)\n",
    "\n",
    "        avg_train_loss = train_loss / len(self.train_loader)\n",
    "        avg_train_rmse = rmse_sum / len(self.train_loader)\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{self.config.num_epochs} - Training Loss: {avg_train_loss:.4f}, RMSE: {avg_train_rmse:.4f}\")\n",
    "        return avg_train_loss\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        val_loss, rmse_sum = 0.0, 0.0\n",
    "        low_light_loss, noisy_loss = 0.0, 0.0\n",
    "        low_light_count, noisy_count = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for idx, (images, depths) in enumerate(tqdm(self.val_loader, desc=\"Validating\")):\n",
    "                images, depths = images.to(self.device), depths.to(self.device)\n",
    "                with autocast('cuda' if torch.cuda.is_available() else 'cpu', enabled=self.config.use_amp):\n",
    "                    outputs = self.model(images)\n",
    "                    loss = self.depth_criterion(outputs, depths)\n",
    "                val_loss += loss.item()\n",
    "                rmse_sum += compute_rmse(outputs, depths)\n",
    "                batch_size = images.size(0)\n",
    "                batch_indices = range(idx * batch_size, (idx + 1) * batch_size)\n",
    "                for i, global_idx in enumerate(batch_indices):\n",
    "                    if global_idx in self.val_dataset.low_light_indices:\n",
    "                        low_light_loss += loss.item()\n",
    "                        low_light_count += 1\n",
    "                    if global_idx in self.val_dataset.noisy_indices:\n",
    "                        noisy_loss += loss.item()\n",
    "                        noisy_count += 1\n",
    "\n",
    "        avg_val_loss = val_loss / len(self.val_loader)\n",
    "        avg_val_rmse = rmse_sum / len(self.val_loader)\n",
    "        print(f\"Validation - Loss: {avg_val_loss:.4f}, RMSE: {avg_val_rmse:.4f}\")\n",
    "        if low_light_count:\n",
    "            print(f\"Low-Light Loss: {low_light_loss / low_light_count:.4f}\")\n",
    "        if noisy_count:\n",
    "            print(f\"Noisy Loss: {noisy_loss / noisy_count:.4f}\")\n",
    "        return avg_val_loss\n",
    "\n",
    "    def generate_predictions(self):\n",
    "        os.makedirs(self.config.output_dir, exist_ok=True)\n",
    "        self.model.eval()\n",
    "        data = []\n",
    "        with torch.no_grad():\n",
    "            for images, filenames in tqdm(self.test_loader, desc=\"Generating Predictions\"):\n",
    "                images = images.to(self.device)\n",
    "                outputs = self.model(images).squeeze(1).cpu().numpy()\n",
    "                for i, fname in enumerate(filenames):\n",
    "                    output = outputs[i]\n",
    "                    output = output * self.config.global_std + self.config.global_mean\n",
    "                    output = np.clip(output, 0, 255).astype(np.uint8)\n",
    "                    cv2.imwrite(os.path.join(self.config.output_dir, f\"{fname}.png\"), output)\n",
    "                    image_resized = cv2.resize(output, (128, 128))\n",
    "                    if np.max(image_resized) > np.min(image_resized):\n",
    "                        image_norm = (image_resized - np.min(image_resized)) / (\n",
    "                                    np.max(image_resized) - np.min(image_resized) + 1e-6)\n",
    "                    else:\n",
    "                        image_norm = image_resized\n",
    "                    image_norm = np.uint8(image_norm * 255)\n",
    "                    row = [len(data), fname] + image_norm.flatten().tolist()\n",
    "                    data.append(row)\n",
    "        num_columns = len(data[0]) - 2 if data else 0\n",
    "        columns = [\"id\", \"ImageID\"] + [str(i) for i in range(num_columns)]\n",
    "        df = pd.DataFrame(data, columns=columns)\n",
    "        df.to_csv(self.config.csv_path, index=False)\n",
    "        print(f\"CSV file saved to {self.config.csv_path}\")\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            self.train_epoch(epoch)\n",
    "            avg_val_loss = self.validate()\n",
    "            self.scheduler.step()\n",
    "            if avg_val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = avg_val_loss\n",
    "                torch.save(self.model.state_dict(), \"best_model.pth\")\n",
    "                print(f\"Best model saved with val loss: {self.best_val_loss:.4f}\")\n",
    "        # Load best model and generate predictions\n",
    "        self.model.load_state_dict(torch.load(\"best_model.pth\", map_location=self.device))\n",
    "        self.generate_predictions()\n",
    "\n",
    "# ------------------- Main -------------------\n",
    "if __name__ == \"__main__\":\n",
    "    config = Config()\n",
    "    trainer = DepthEstimationTrainer(config)\n",
    "    trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11466546,
     "isSourceIdPinned": false,
     "sourceId": 96480,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10841.91644,
   "end_time": "2025-03-31T12:31:16.060350",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-31T09:30:34.143910",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
