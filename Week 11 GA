{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60613ef6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T19:03:24.180342Z",
     "iopub.status.busy": "2025-03-31T19:03:24.180081Z",
     "iopub.status.idle": "2025-03-31T19:03:30.248574Z",
     "shell.execute_reply": "2025-03-31T19:03:30.247589Z"
    },
    "papermill": {
     "duration": 6.07307,
     "end_time": "2025-03-31T19:03:30.250164",
     "exception": false,
     "start_time": "2025-03-31T19:03:24.177094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting piqa\r\n",
      "  Downloading piqa-1.3.2-py3-none-any.whl.metadata (5.8 kB)\r\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\r\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.5)\r\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\r\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\r\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\r\n",
      "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\r\n",
      "Collecting pytorch-msssim\r\n",
      "  Downloading pytorch_msssim-1.0.0-py3-none-any.whl.metadata (8.0 kB)\r\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\r\n",
      "Collecting torch_optimizer\r\n",
      "  Downloading torch_optimizer-0.3.0-py3-none-any.whl.metadata (55 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.9.0.post0)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\r\n",
      "Collecting pytorch-ranger>=0.1.1 (from torch_optimizer)\r\n",
      "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl.metadata (509 bytes)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\r\n",
      "Downloading piqa-1.3.2-py3-none-any.whl (32 kB)\r\n",
      "Downloading pytorch_msssim-1.0.0-py3-none-any.whl (7.7 kB)\r\n",
      "Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\r\n",
      "Installing collected packages: pytorch-ranger, pytorch-msssim, torch_optimizer, piqa\r\n",
      "Successfully installed piqa-1.3.2 pytorch-msssim-1.0.0 pytorch-ranger-0.1.1 torch_optimizer-0.3.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install piqa opencv-python matplotlib scikit-learn pillow tqdm torch torchvision opencv-contrib-python pytorch-msssim transformers torch_optimizer pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "575da82a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T19:03:30.257460Z",
     "iopub.status.busy": "2025-03-31T19:03:30.257221Z",
     "iopub.status.idle": "2025-03-31T19:03:35.467163Z",
     "shell.execute_reply": "2025-03-31T19:03:35.466108Z"
    },
    "papermill": {
     "duration": 5.215267,
     "end_time": "2025-03-31T19:03:35.468891",
     "exception": false,
     "start_time": "2025-03-31T19:03:30.253624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\r\n",
      "Collecting albumentations\r\n",
      "  Downloading albumentations-2.0.5-py3-none-any.whl.metadata (41 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\r\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\r\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.11.0a2)\r\n",
      "Collecting albucore==0.0.23 (from albumentations)\r\n",
      "  Downloading albucore-0.0.23-py3-none-any.whl.metadata (5.3 kB)\r\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\r\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.23->albumentations) (3.11.1)\r\n",
      "Collecting simsimd>=5.9.2 (from albucore==0.0.23->albumentations)\r\n",
      "  Downloading simsimd-6.2.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (66 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2.4.1)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (2.29.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (4.12.2)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24.4->albumentations) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24.4->albumentations) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.24.4->albumentations) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.24.4->albumentations) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.24.4->albumentations) (2024.2.0)\r\n",
      "Downloading albumentations-2.0.5-py3-none-any.whl (290 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.6/290.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading albucore-0.0.23-py3-none-any.whl (14 kB)\r\n",
      "Downloading simsimd-6.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (632 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m632.7/632.7 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: simsimd, albucore, albumentations\r\n",
      "  Attempting uninstall: albucore\r\n",
      "    Found existing installation: albucore 0.0.19\r\n",
      "    Uninstalling albucore-0.0.19:\r\n",
      "      Successfully uninstalled albucore-0.0.19\r\n",
      "  Attempting uninstall: albumentations\r\n",
      "    Found existing installation: albumentations 1.4.20\r\n",
      "    Uninstalling albumentations-1.4.20:\r\n",
      "      Successfully uninstalled albumentations-1.4.20\r\n",
      "Successfully installed albucore-0.0.23 albumentations-2.0.5 simsimd-6.2.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -U albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "199ead6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T19:03:35.478710Z",
     "iopub.status.busy": "2025-03-31T19:03:35.478456Z",
     "iopub.status.idle": "2025-03-31T23:34:46.086740Z",
     "shell.execute_reply": "2025-03-31T23:34:46.085568Z"
    },
    "papermill": {
     "duration": 16270.615379,
     "end_time": "2025-03-31T23:34:46.088376",
     "exception": false,
     "start_time": "2025-03-31T19:03:35.472997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-d3b854f3f2a5>:73: UserWarning: Argument(s) 'max_holes, max_height, max_width' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(max_holes=8, max_height=16, max_width=16, p=0.3),\n",
      "<ipython-input-3-d3b854f3f2a5>:75: UserWarning: Argument(s) 'std_limit' are not valid for transform GaussNoise\n",
      "  A.GaussNoise(std_limit=(10 / 255, 50 / 255), p=0.5),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a56c09a90443c1bfbba2c83a80d7fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/436 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35fae08c97934f119f4d89ffcab20608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/548 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8d3f2ba8f04805a98fb6049800c92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training: 100%|██████████| 418/418 [08:57<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Training Loss: 0.5790, RMSE: 0.6558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:17<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5155, RMSE: 0.5692\n",
      "Low-Light Loss: 0.5134\n",
      "Best model saved with val loss: 0.5155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training: 100%|██████████| 418/418 [08:56<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 - Training Loss: 0.5346, RMSE: 0.6057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:16<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.6330, RMSE: 0.7308\n",
      "Low-Light Loss: 0.6324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training: 100%|██████████| 418/418 [08:52<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 - Training Loss: 0.5633, RMSE: 0.6411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:17<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5318, RMSE: 0.5885\n",
      "Low-Light Loss: 0.5298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training: 100%|██████████| 418/418 [08:51<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 - Training Loss: 0.5286, RMSE: 0.6008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:17<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5504, RMSE: 0.6195\n",
      "Low-Light Loss: 0.5481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training: 100%|██████████| 418/418 [08:54<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 - Training Loss: 0.5398, RMSE: 0.6216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:17<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5641, RMSE: 0.6474\n",
      "Low-Light Loss: 0.5616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Training: 100%|██████████| 418/418 [08:54<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 - Training Loss: 0.6042, RMSE: 0.7096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:17<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.6170, RMSE: 0.7323\n",
      "Low-Light Loss: 0.6168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Training: 100%|██████████| 418/418 [08:47<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 - Training Loss: 0.6705, RMSE: 0.8489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:16<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.6527, RMSE: 0.8471\n",
      "Low-Light Loss: 0.6519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Training: 100%|██████████| 418/418 [08:40<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 - Training Loss: 0.7375, RMSE: 1.0271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:16<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.8434, RMSE: 1.1771\n",
      "Low-Light Loss: 0.8432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 Training: 100%|██████████| 418/418 [08:38<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 - Training Loss: 0.7667, RMSE: 1.0946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:16<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.7797, RMSE: 1.0495\n",
      "Low-Light Loss: 0.7789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 Training: 100%|██████████| 418/418 [08:39<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 - Training Loss: 0.7493, RMSE: 1.0448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:16<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.7822, RMSE: 1.0296\n",
      "Low-Light Loss: 0.7813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 Training: 100%|██████████| 418/418 [08:35<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 - Training Loss: 0.7522, RMSE: 1.0097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:16<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.7564, RMSE: 1.0528\n",
      "Low-Light Loss: 0.7540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 Training: 100%|██████████| 418/418 [08:39<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 - Training Loss: 0.7361, RMSE: 0.9594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:16<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.7101, RMSE: 0.9412\n",
      "Low-Light Loss: 0.7090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 Training: 100%|██████████| 418/418 [08:41<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 - Training Loss: 0.7516, RMSE: 0.9790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:16<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.7172, RMSE: 0.9561\n",
      "Low-Light Loss: 0.7165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 Training: 100%|██████████| 418/418 [08:44<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 - Training Loss: 0.7312, RMSE: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:16<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.7061, RMSE: 0.9507\n",
      "Low-Light Loss: 0.7050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 Training: 100%|██████████| 418/418 [08:49<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 - Training Loss: 0.7300, RMSE: 0.9639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:16<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.7031, RMSE: 0.9525\n",
      "Low-Light Loss: 0.7015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 Training: 100%|██████████| 418/418 [08:42<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 - Training Loss: 0.6144, RMSE: 0.7300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:16<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5763, RMSE: 0.6552\n",
      "Low-Light Loss: 0.5747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 Training: 100%|██████████| 418/418 [08:43<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 - Training Loss: 0.6267, RMSE: 0.7215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:16<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.6498, RMSE: 0.7754\n",
      "Low-Light Loss: 0.6503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 Training: 100%|██████████| 418/418 [08:41<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 - Training Loss: 0.6493, RMSE: 0.7694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:16<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.6337, RMSE: 0.7970\n",
      "Low-Light Loss: 0.6321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 Training: 100%|██████████| 418/418 [08:39<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 - Training Loss: 0.6526, RMSE: 0.7902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:16<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.6259, RMSE: 0.7861\n",
      "Low-Light Loss: 0.6247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 Training: 100%|██████████| 418/418 [08:39<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 - Training Loss: 0.6405, RMSE: 0.7686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:16<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5974, RMSE: 0.6991\n",
      "Low-Light Loss: 0.5966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21 Training: 100%|██████████| 418/418 [08:37<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 - Training Loss: 0.6668, RMSE: 0.8423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:16<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.6348, RMSE: 0.7805\n",
      "Low-Light Loss: 0.6337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22 Training: 100%|██████████| 418/418 [08:36<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 - Training Loss: 0.6603, RMSE: 0.8137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:16<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.6259, RMSE: 0.7892\n",
      "Low-Light Loss: 0.6244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23 Training: 100%|██████████| 418/418 [08:37<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 - Training Loss: 0.6405, RMSE: 0.7630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:16<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.6013, RMSE: 0.7035\n",
      "Low-Light Loss: 0.6001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24 Training: 100%|██████████| 418/418 [08:37<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 - Training Loss: 0.6316, RMSE: 0.7403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:16<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.6085, RMSE: 0.7391\n",
      "Low-Light Loss: 0.6068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25 Training: 100%|██████████| 418/418 [08:35<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 - Training Loss: 0.6326, RMSE: 0.7416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:16<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5996, RMSE: 0.7338\n",
      "Low-Light Loss: 0.5976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26 Training: 100%|██████████| 418/418 [08:35<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 - Training Loss: 0.6251, RMSE: 0.7319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:16<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5922, RMSE: 0.7073\n",
      "Low-Light Loss: 0.5900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27 Training: 100%|██████████| 418/418 [08:36<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 - Training Loss: 0.6146, RMSE: 0.7042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:16<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5626, RMSE: 0.6344\n",
      "Low-Light Loss: 0.5606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28 Training: 100%|██████████| 418/418 [08:38<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 - Training Loss: 0.6044, RMSE: 0.6779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:16<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5597, RMSE: 0.6304\n",
      "Low-Light Loss: 0.5573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29 Training: 100%|██████████| 418/418 [08:38<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 - Training Loss: 0.5993, RMSE: 0.6669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:16<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5549, RMSE: 0.6142\n",
      "Low-Light Loss: 0.5529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30 Training: 100%|██████████| 418/418 [08:37<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 - Training Loss: 0.5980, RMSE: 0.6657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:16<00:00,  3.16it/s]\n",
      "<ipython-input-3-d3b854f3f2a5>:646: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(\"best_model.pth\", map_location=self.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.5551, RMSE: 0.6127\n",
      "Low-Light Loss: 0.5528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Predictions: 100%|██████████| 53/53 [00:40<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved to submission.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from pytorch_msssim import SSIM\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# ------------------- Configuration & Reproducibility -------------------\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Reproducibility\n",
    "        self.seed = 42\n",
    "        # Device settings\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Data & Image settings\n",
    "        self.image_size = 256\n",
    "        self.output_size = 128\n",
    "        self.batch_size = 16\n",
    "        self.num_workers = 4\n",
    "        # Training settings\n",
    "        self.num_epochs = 30\n",
    "        self.base_lr = 2e-5\n",
    "        self.weight_decay = 1e-4\n",
    "        self.use_amp = True\n",
    "        self.clip_value = 1.0\n",
    "        # Paths (update these paths as needed)\n",
    "        self.train_rgb_dir = \"/kaggle/input/depth-estimation/competition-data/competition-data/training/images\"\n",
    "        self.train_depth_dir = \"/kaggle/input/depth-estimation/competition-data/competition-data/training/depths\"\n",
    "        self.val_rgb_dir = \"/kaggle/input/depth-estimation/competition-data/competition-data/validation/images\"\n",
    "        self.val_depth_dir = \"/kaggle/input/depth-estimation/competition-data/competition-data/validation/depths\"\n",
    "        self.test_rgb_dir = \"/kaggle/input/depth-estimation/competition-data/competition-data/testing/images\"\n",
    "        self.output_dir = \"predictions_folder\"\n",
    "        self.csv_path = \"submission.csv\"\n",
    "        # Depth normalization parameters\n",
    "        self.global_mean = 88.2930\n",
    "        self.global_std = 61.7739\n",
    "\n",
    "def set_global_seeds(seed: int):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ------------------- Data Augmentations & Preprocessing -------------------\n",
    "def apply_clahe(image: np.ndarray, **kwargs) -> np.ndarray:\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l)\n",
    "    merged = cv2.merge((cl, a, b))\n",
    "    return cv2.cvtColor(merged, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "def get_transforms(config: Config):\n",
    "    train_transform = A.Compose([\n",
    "        A.Lambda(image=apply_clahe),\n",
    "        A.Resize(config.image_size, config.image_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.CoarseDropout(max_holes=8, max_height=16, max_width=16, p=0.3),\n",
    "        A.RandomBrightnessContrast(p=0.3),\n",
    "        A.GaussNoise(std_limit=(10 / 255, 50 / 255), p=0.5),\n",
    "        A.MotionBlur(blur_limit=7, p=0.3),  # Add motion blur\n",
    "        A.ISONoise(p=0.5),\n",
    "        A.HueSaturationValue(p=0.3),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                    std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "    ], additional_targets={\"mask\": \"mask\"})\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Lambda(image=apply_clahe),\n",
    "        A.Resize(config.image_size, config.image_size),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                    std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "    ], additional_targets={\"mask\": \"mask\"})\n",
    "\n",
    "    return train_transform, val_transform\n",
    "\n",
    "\n",
    "# ------------------- Dataset -------------------\n",
    "class DepthDataset(Dataset):\n",
    "    def __init__(self, rgb_dir: str, depth_dir: Optional[str] = None,\n",
    "                 transform: Optional[A.Compose] = None, is_val: bool = False):\n",
    "        self.rgb_dir = rgb_dir\n",
    "        self.depth_dir = depth_dir\n",
    "        self.transform = transform\n",
    "        self.filenames = sorted(os.listdir(rgb_dir), key=lambda x: int(''.join(filter(str.isdigit, x)) or 0))\n",
    "        self.is_val = is_val\n",
    "        self.low_light_indices = []\n",
    "        self.noisy_indices = []\n",
    "        if is_val and depth_dir:\n",
    "            self._split_validation_set()\n",
    "\n",
    "    def _split_validation_set(self):\n",
    "        for idx, fname in enumerate(self.filenames):\n",
    "            rgb_path = os.path.join(self.rgb_dir, fname)\n",
    "            image = cv2.imread(rgb_path, cv2.IMREAD_COLOR)\n",
    "            brightness = np.mean(image)\n",
    "            noise_level = np.std(image)\n",
    "            if brightness < 50:\n",
    "                self.low_light_indices.append(idx)\n",
    "            if noise_level > 30:\n",
    "                self.noisy_indices.append(idx)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        fname = self.filenames[idx]\n",
    "        rgb_path = os.path.join(self.rgb_dir, fname)\n",
    "        image = cv2.imread(rgb_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        depth = None\n",
    "        if self.depth_dir:\n",
    "            depth_path = os.path.join(self.depth_dir, fname)\n",
    "            depth = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED).astype(np.float32)\n",
    "            depth = cv2.resize(depth, (config.image_size, config.image_size))\n",
    "            depth = (depth - config.global_mean) / (config.global_std + 1e-8)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=depth if depth is not None else image)\n",
    "            image = augmented[\"image\"]\n",
    "            if depth is not None:\n",
    "                depth = augmented[\"mask\"]\n",
    "        if depth is not None:\n",
    "            depth = torch.from_numpy(depth).float() if isinstance(depth, np.ndarray) else depth.clone().detach().float()\n",
    "            depth = depth.unsqueeze(0)\n",
    "            depth = F.interpolate(depth.unsqueeze(0), size=(config.output_size, config.output_size),\n",
    "                                  mode='bilinear', align_corners=False).squeeze(0)\n",
    "        return (image, depth) if depth is not None else (image, fname)\n",
    "\n",
    "# ------------------- Model Components -------------------\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoImageProcessor, Dinov2Model\n",
    "\n",
    "class DinoV2Backbone(nn.Module):\n",
    "    def __init__(self, output_dim=256):\n",
    "        super(DinoV2Backbone, self).__init__()\n",
    "        # Load the image processor and the Dinov2 model from Hugging Face\n",
    "        self.image_processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
    "        self.backbone = Dinov2Model.from_pretrained(\"facebook/dinov2-base\")\n",
    "        # Set return_dict to False for easier indexing\n",
    "        self.backbone.config.return_dict = False\n",
    "\n",
    "        # Store patch_size and image_size from the model config (or set manually)\n",
    "        # Note: For facebook/dinov2-base, defaults are usually patch_size=14, image_size=224.\n",
    "        self.patch_size = self.backbone.config.patch_size if hasattr(self.backbone.config, \"patch_size\") else 14\n",
    "        self.image_size = self.backbone.config.image_size if hasattr(self.backbone.config, \"image_size\") else 224\n",
    "\n",
    "        # Projection layer to map the hidden states to the desired output dimension.\n",
    "        # The hidden size is stored in the model config.\n",
    "        self.proj = nn.Conv2d(self.backbone.config.hidden_size, output_dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the backbone.\n",
    "        \n",
    "        Parameters:\n",
    "            x (torch.Tensor): Input tensor of shape (B, C, H, W). It is assumed that x is already preprocessed.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Projected features with shape (B, output_dim, H_out, W_out).\n",
    "        \"\"\"\n",
    "        # Get the backbone output: shape [B, seq_length, hidden_size]\n",
    "        outputs = self.backbone(pixel_values=x)[0]\n",
    "\n",
    "        # Remove the class token (first token) if present.\n",
    "        outputs = outputs[:, 1:, :]  # New shape: [B, seq_length-1, hidden_size]\n",
    "\n",
    "        # Determine grid size from the number of tokens.\n",
    "        B, L, C = outputs.shape\n",
    "        grid_size = int(math.sqrt(L))\n",
    "        if grid_size * grid_size != L:\n",
    "            raise ValueError(f\"Expected square grid, but got sequence length {L} which is not a perfect square.\")\n",
    "\n",
    "        # Reshape into [B, hidden_size, grid_size, grid_size]\n",
    "        outputs = outputs.transpose(1, 2).reshape(B, C, grid_size, grid_size)\n",
    "        features = self.proj(outputs)\n",
    "        return features\n",
    "        \n",
    "# Channel Attention (already provided)\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes: int, ratio: int = 16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        return self.sigmoid(avg_out + max_out)\n",
    "\n",
    "# Spatial Attention Module\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size=7, padding=3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
    "        return self.sigmoid(self.conv1(x_cat))\n",
    "\n",
    "# Residual Block used in the upgraded denoising branch\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        self.conv3 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(channels)\n",
    "        self.conv4 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.relu(self.bn3(self.conv3(out)))\n",
    "        out = self.bn4(self.conv4(out))\n",
    "        out += identity\n",
    "        return self.relu(out)\n",
    "\n",
    "# Upgraded Denoising Branch\n",
    "class DenoisingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenoisingNet, self).__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.resblock1 = ResidualBlock(64)\n",
    "        self.resblock2 = ResidualBlock(64)\n",
    "        self.resblock3 = ResidualBlock(64)\n",
    "        self.out_conv = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
    "        self.out_act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.initial(x)\n",
    "        x = self.resblock1(x)\n",
    "        x = self.resblock2(x)\n",
    "        x = self.resblock3(x)\n",
    "        x = self.out_conv(x)\n",
    "        return self.out_act(x)\n",
    "\n",
    "# Coarse Network\n",
    "def conv_block(in_channels: int, out_channels: int, dropout: float = 0.0):\n",
    "    layers = [\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    ]\n",
    "    if dropout > 0:\n",
    "        layers.append(nn.Dropout2d(dropout))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class CoarseNet(nn.Module):\n",
    "    def __init__(self, output_size: int, in_channels: int = 256):\n",
    "        super(CoarseNet, self).__init__()\n",
    "        # Now the encoder1 accepts in_channels (default 256) instead of 3.\n",
    "        self.encoder1 = conv_block(in_channels, 128, dropout=0.2)\n",
    "        self.encoder2 = conv_block(128, 256, dropout=0.2)\n",
    "        self.encoder3 = conv_block(256, 512, dropout=0.2)\n",
    "        self.encoder3_extra = conv_block(512, 512, dropout=0.2)\n",
    "        self.encoder4 = nn.Sequential(\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, dilation=2, padding=2),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.up4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.decoder4 = conv_block(1024, 512, dropout=0.2)\n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder3 = conv_block(512, 256, dropout=0.2)\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = conv_block(256, 128, dropout=0.2)\n",
    "        self.decoder1 = nn.Conv2d(128, 1, kernel_size=3, padding=1)\n",
    "        self.final_pool = nn.AdaptiveAvgPool2d((output_size, output_size))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(self.pool(e1))\n",
    "        e3 = self.encoder3(self.pool(e2))\n",
    "        e3 = self.encoder3_extra(e3)\n",
    "        e4 = self.encoder4(self.pool(e3))\n",
    "        \n",
    "        u4 = self.up4(e4)\n",
    "        # Ensure u4 and e3 have the same spatial dimensions before concatenation\n",
    "        if u4.shape[2:] != e3.shape[2:]:\n",
    "            u4 = F.interpolate(u4, size=e3.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d4 = self.decoder4(torch.cat([u4, e3], dim=1))\n",
    "        \n",
    "        u3 = self.up3(d4)\n",
    "        if u3.shape[2:] != e2.shape[2:]:\n",
    "            u3 = F.interpolate(u3, size=e2.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d3 = self.decoder3(torch.cat([u3, e2], dim=1))\n",
    "        \n",
    "        u2 = self.up2(d3)\n",
    "        if u2.shape[2:] != e1.shape[2:]:\n",
    "            u2 = F.interpolate(u2, size=e1.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d2 = self.decoder2(torch.cat([u2, e1], dim=1))\n",
    "        \n",
    "        coarse = self.decoder1(d2)\n",
    "        return self.final_pool(coarse)\n",
    "\n",
    "\n",
    "# Fine Network with Spatial Attention and Residual Connections\n",
    "class FineNet(nn.Module):\n",
    "    def __init__(self, output_size: int):\n",
    "        super(FineNet, self).__init__()\n",
    "        # Update input channels from 4 to 257 (1 from coarse + 256 from denoised branch)\n",
    "        self.conv1 = conv_block(257, 64)\n",
    "        self.ca1 = ChannelAttention(64)\n",
    "        self.sa1 = SpatialAttention()\n",
    "        self.conv2 = conv_block(64, 64)\n",
    "        self.ca2 = ChannelAttention(64)\n",
    "        self.sa2 = SpatialAttention()\n",
    "        # Extra conv layer for more capacity\n",
    "        self.conv_extra = conv_block(64, 64)\n",
    "        # Residual connection from conv1 output\n",
    "        self.res_conv = nn.Conv2d(64, 32, kernel_size=1)\n",
    "        # Adjust channels for concatenation: 64 (from extra) + 32 (from residual) = 96\n",
    "        self.conv3 = conv_block(96, 32)\n",
    "        self.out_conv = nn.Conv2d(32, 1, kernel_size=3, padding=1)\n",
    "        self.final_pool = nn.AdaptiveAvgPool2d((output_size, output_size))\n",
    "\n",
    "    def forward(self, coarse: torch.Tensor, rgb: torch.Tensor) -> torch.Tensor:\n",
    "        # coarse: [B, 1, H, W] and rgb: [B, 256, H, W] => concatenated: [B, 257, H, W]\n",
    "        x = torch.cat([coarse, rgb], dim=1)\n",
    "        x1 = self.conv1(x)\n",
    "        x1 = x1 * self.ca1(x1)\n",
    "        x1 = x1 * self.sa1(x1)\n",
    "        x2 = self.conv2(x1)\n",
    "        x2 = x2 * self.ca2(x2)\n",
    "        x2 = x2 * self.sa2(x2)\n",
    "        x_extra = self.conv_extra(x2)\n",
    "        res = self.res_conv(x1)\n",
    "        x_cat = torch.cat([x_extra, res], dim=1)\n",
    "        x3 = self.conv3(x_cat)\n",
    "        refined = self.out_conv(x3)\n",
    "        return self.final_pool(refined)\n",
    "\n",
    "# Combined Model\n",
    "class DepthEstimationModel(nn.Module):\n",
    "    def __init__(self, output_size: int):\n",
    "        super(DepthEstimationModel, self).__init__()\n",
    "        self.dino_backbone = DinoV2Backbone(output_dim=256)\n",
    "        self.denoising_branch = DenoisingNet()\n",
    "        self.denoised_proj = nn.Conv2d(3, 256, kernel_size=1)\n",
    "        # Now, pass in_channels=256 since fused features have 256 channels.\n",
    "        self.coarse_net = CoarseNet(output_size, in_channels=256)\n",
    "        self.fine_net = FineNet(output_size)\n",
    "        self.final_resize = nn.AdaptiveAvgPool2d((output_size, output_size))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        dino_features = self.dino_backbone(x)\n",
    "        denoised = self.denoising_branch(x)\n",
    "        denoised = self.denoised_proj(denoised)\n",
    "        denoised = F.interpolate(denoised, size=dino_features.shape[2:], mode='bilinear', align_corners=False)\n",
    "        fused = dino_features + denoised\n",
    "        coarse = self.coarse_net(fused)\n",
    "        denoised_resized = F.interpolate(denoised, size=(config.output_size, config.output_size),\n",
    "                                         mode='bilinear', align_corners=False)\n",
    "        refined = self.fine_net(coarse, denoised_resized)\n",
    "        return self.final_resize(refined)\n",
    "\n",
    "# Improved Discriminator Architecture\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Extra layer for improved capacity\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, 4, 1, 0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.main(x).view(-1, 1)\n",
    "\n",
    "# ------------------- Loss Functions -------------------\n",
    "class DepthLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DepthLoss, self).__init__()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.huber_loss = nn.HuberLoss()\n",
    "        # Use SSIM for a single channel depth map\n",
    "        self.ssim = SSIM(data_range=1.0, size_average=True, channel=1)\n",
    "\n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor, lambda_weight: float = 0.5) -> torch.Tensor:\n",
    "        l1 = self.l1_loss(pred, target)\n",
    "        huber = self.huber_loss(pred, target)\n",
    "        combined_loss = lambda_weight * l1 + (1 - lambda_weight) * huber\n",
    "        ssim_loss = 1 - self.ssim(pred, target)\n",
    "        return 0.6 * combined_loss + 0.4 * ssim_loss\n",
    "\n",
    "# Edge-Aware Smoothness Loss\n",
    "import torch.nn.functional as F\n",
    "class EdgeAwareSmoothnessLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EdgeAwareSmoothnessLoss, self).__init__()\n",
    "\n",
    "    def forward(self, depth: torch.Tensor, image: torch.Tensor) -> torch.Tensor:\n",
    "        # Resize image to match depth dimensions if needed\n",
    "        if image.shape[2:] != depth.shape[2:]:\n",
    "            image = F.interpolate(image, size=depth.shape[2:], mode='bilinear', align_corners=False)\n",
    "        # Convert image to grayscale (averaging channels)\n",
    "        image_gray = torch.mean(image, dim=1, keepdim=True)\n",
    "        dx_depth = torch.abs(depth[:, :, :, :-1] - depth[:, :, :, 1:])\n",
    "        dy_depth = torch.abs(depth[:, :, :-1, :] - depth[:, :, 1:, :])\n",
    "        dx_image = torch.abs(image_gray[:, :, :, :-1] - image_gray[:, :, :, 1:])\n",
    "        dy_image = torch.abs(image_gray[:, :, :-1, :] - image_gray[:, :, 1:, :])\n",
    "        weight_x = torch.exp(-dx_image)\n",
    "        weight_y = torch.exp(-dy_image)\n",
    "        smoothness_x = dx_depth * weight_x\n",
    "        smoothness_y = dy_depth * weight_y\n",
    "        return torch.mean(smoothness_x) + torch.mean(smoothness_y)\n",
    "\n",
    "\n",
    "class GANLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, pred: torch.Tensor, target_is_real: bool) -> torch.Tensor:\n",
    "        target = torch.full_like(pred, 0.9) if target_is_real else torch.full_like(pred, 0.1)\n",
    "        return self.bce_loss(pred, target)\n",
    "\n",
    "def compute_rmse(pred: torch.Tensor, target: torch.Tensor) -> float:\n",
    "    pred_np = pred.detach().cpu().numpy().flatten()\n",
    "    target_np = target.detach().cpu().numpy().flatten()\n",
    "    return np.sqrt(np.mean((pred_np - target_np) ** 2))\n",
    "\n",
    "# ------------------- Trainer Class -------------------\n",
    "class DepthEstimationTrainer:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.device = config.device\n",
    "        set_global_seeds(config.seed)\n",
    "        self.train_transform, self.val_transform = get_transforms(config)\n",
    "        self._setup_data()\n",
    "        self._setup_model_and_optimizer()\n",
    "\n",
    "    def _setup_data(self):\n",
    "        self.train_dataset = DepthDataset(\n",
    "            self.config.train_rgb_dir,\n",
    "            self.config.train_depth_dir,\n",
    "            transform=self.train_transform\n",
    "        )\n",
    "        self.val_dataset = DepthDataset(\n",
    "            self.config.val_rgb_dir,\n",
    "            self.config.val_depth_dir,\n",
    "            transform=self.val_transform,\n",
    "            is_val=True\n",
    "        )\n",
    "        self.test_dataset = DepthDataset(\n",
    "            self.config.test_rgb_dir,\n",
    "            transform=self.val_transform\n",
    "        )\n",
    "        self.train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.config.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        self.val_loader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.config.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        self.test_loader = DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.config.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    def _setup_model_and_optimizer(self):\n",
    "        self.model = DepthEstimationModel(self.config.output_size).to(self.device)\n",
    "        self.discriminator = Discriminator().to(self.device)\n",
    "        self.depth_criterion = DepthLoss().to(self.device)\n",
    "        self.smoothness_criterion = EdgeAwareSmoothnessLoss().to(self.device)\n",
    "        self.gan_criterion = GANLoss().to(self.device)\n",
    "        self.optimizer_g = optim.AdamW(self.model.parameters(), lr=self.config.base_lr,\n",
    "                                       weight_decay=self.config.weight_decay)\n",
    "        self.optimizer_d = optim.AdamW(self.discriminator.parameters(), lr=self.config.base_lr,\n",
    "                                       weight_decay=self.config.weight_decay)\n",
    "        self.scheduler = CosineAnnealingWarmRestarts(self.optimizer_g, T_0=5, T_mult=2)\n",
    "        self.scaler = GradScaler(enabled=self.config.use_amp)\n",
    "        self.best_val_loss = float('inf')\n",
    "\n",
    "    def train_epoch(self, epoch: int):\n",
    "        self.model.train()\n",
    "        self.discriminator.train()\n",
    "        train_loss, rmse_sum = 0.0, 0.0\n",
    "        # Curriculum: gradually increase adversarial weight from 0 to 0.1\n",
    "        adv_weight = min(0.1 * ((epoch + 1) / self.config.num_epochs), 0.1)\n",
    "        for images, depths in tqdm(self.train_loader, desc=f\"Epoch {epoch + 1} Training\"):\n",
    "            images, depths = images.to(self.device), depths.to(self.device)\n",
    "\n",
    "            # Train Discriminator\n",
    "            self.optimizer_d.zero_grad()\n",
    "            with autocast('cuda' if torch.cuda.is_available() else 'cpu', enabled=self.config.use_amp):\n",
    "                fake_depth = self.model(images)\n",
    "                d_real = self.discriminator(depths)\n",
    "                d_fake = self.discriminator(fake_depth.detach())\n",
    "                d_loss = 0.5 * (self.gan_criterion(d_real, True) + self.gan_criterion(d_fake, False))\n",
    "            self.scaler.scale(d_loss).backward()\n",
    "            self.scaler.step(self.optimizer_d)\n",
    "            self.scaler.update()\n",
    "\n",
    "            # Train Generator\n",
    "            self.optimizer_g.zero_grad()\n",
    "            with autocast('cuda' if torch.cuda.is_available() else 'cpu', enabled=self.config.use_amp):\n",
    "                fake_depth = self.model(images)\n",
    "                g_loss_adv = self.gan_criterion(self.discriminator(fake_depth), True)\n",
    "                g_loss_depth = self.depth_criterion(fake_depth, depths)\n",
    "                smooth_loss = self.smoothness_criterion(fake_depth, images)\n",
    "                g_loss = g_loss_depth + 0.1 * adv_weight * g_loss_adv + 0.01 * smooth_loss\n",
    "            self.scaler.scale(g_loss).backward()\n",
    "            self.scaler.unscale_(self.optimizer_g)\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.clip_value)\n",
    "            self.scaler.step(self.optimizer_g)\n",
    "            self.scaler.update()\n",
    "\n",
    "            train_loss += g_loss.item()\n",
    "            rmse_sum += compute_rmse(fake_depth, depths)\n",
    "\n",
    "        avg_train_loss = train_loss / len(self.train_loader)\n",
    "        avg_train_rmse = rmse_sum / len(self.train_loader)\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{self.config.num_epochs} - Training Loss: {avg_train_loss:.4f}, RMSE: {avg_train_rmse:.4f}\")\n",
    "        return avg_train_loss\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        val_loss, rmse_sum = 0.0, 0.0\n",
    "        low_light_loss, noisy_loss = 0.0, 0.0\n",
    "        low_light_count, noisy_count = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for idx, (images, depths) in enumerate(tqdm(self.val_loader, desc=\"Validating\")):\n",
    "                images, depths = images.to(self.device), depths.to(self.device)\n",
    "                with autocast('cuda' if torch.cuda.is_available() else 'cpu', enabled=self.config.use_amp):\n",
    "                    outputs = self.model(images)\n",
    "                    loss = self.depth_criterion(outputs, depths)\n",
    "                val_loss += loss.item()\n",
    "                rmse_sum += compute_rmse(outputs, depths)\n",
    "                batch_size = images.size(0)\n",
    "                batch_indices = range(idx * batch_size, (idx + 1) * batch_size)\n",
    "                for i, global_idx in enumerate(batch_indices):\n",
    "                    if global_idx in self.val_dataset.low_light_indices:\n",
    "                        low_light_loss += loss.item()\n",
    "                        low_light_count += 1\n",
    "                    if global_idx in self.val_dataset.noisy_indices:\n",
    "                        noisy_loss += loss.item()\n",
    "                        noisy_count += 1\n",
    "\n",
    "        avg_val_loss = val_loss / len(self.val_loader)\n",
    "        avg_val_rmse = rmse_sum / len(self.val_loader)\n",
    "        print(f\"Validation - Loss: {avg_val_loss:.4f}, RMSE: {avg_val_rmse:.4f}\")\n",
    "        if low_light_count:\n",
    "            print(f\"Low-Light Loss: {low_light_loss / low_light_count:.4f}\")\n",
    "        if noisy_count:\n",
    "            print(f\"Noisy Loss: {noisy_loss / noisy_count:.4f}\")\n",
    "        return avg_val_loss\n",
    "\n",
    "    def generate_predictions(self):\n",
    "        os.makedirs(self.config.output_dir, exist_ok=True)\n",
    "        self.model.eval()\n",
    "        data = []\n",
    "        with torch.no_grad():\n",
    "            for images, filenames in tqdm(self.test_loader, desc=\"Generating Predictions\"):\n",
    "                images = images.to(self.device)\n",
    "                outputs = self.model(images).squeeze(1).cpu().numpy()\n",
    "                for i, fname in enumerate(filenames):\n",
    "                    output = outputs[i]\n",
    "                    output = output * self.config.global_std + self.config.global_mean\n",
    "                    output = np.clip(output, 0, 255).astype(np.uint8)\n",
    "                    cv2.imwrite(os.path.join(self.config.output_dir, f\"{fname}.png\"), output)\n",
    "                    image_resized = cv2.resize(output, (128, 128))\n",
    "                    if np.max(image_resized) > np.min(image_resized):\n",
    "                        image_norm = (image_resized - np.min(image_resized)) / (\n",
    "                                    np.max(image_resized) - np.min(image_resized) + 1e-6)\n",
    "                    else:\n",
    "                        image_norm = image_resized\n",
    "                    image_norm = np.uint8(image_norm * 255)\n",
    "                    row = [len(data), fname] + image_norm.flatten().tolist()\n",
    "                    data.append(row)\n",
    "        num_columns = len(data[0]) - 2 if data else 0\n",
    "        columns = [\"id\", \"ImageID\"] + [str(i) for i in range(num_columns)]\n",
    "        df = pd.DataFrame(data, columns=columns)\n",
    "        df.to_csv(self.config.csv_path, index=False)\n",
    "        print(f\"CSV file saved to {self.config.csv_path}\")\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            self.train_epoch(epoch)\n",
    "            avg_val_loss = self.validate()\n",
    "            self.scheduler.step()\n",
    "            if avg_val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = avg_val_loss\n",
    "                torch.save(self.model.state_dict(), \"best_model.pth\")\n",
    "                print(f\"Best model saved with val loss: {self.best_val_loss:.4f}\")\n",
    "        # Load best model and generate predictions\n",
    "        self.model.load_state_dict(torch.load(\"best_model.pth\", map_location=self.device))\n",
    "        self.generate_predictions()\n",
    "\n",
    "# ------------------- Main -------------------\n",
    "if __name__ == \"__main__\":\n",
    "    config = Config()\n",
    "    trainer = DepthEstimationTrainer(config)\n",
    "    trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11466546,
     "isSourceIdPinned": false,
     "sourceId": 96480,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16288.950226,
   "end_time": "2025-03-31T23:34:50.541694",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-31T19:03:21.591468",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03da5713bbe24d7cb62d51d54538eaf4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "047758ee2a92412aa28632838f757e8c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "078d35baded94376b880bada5a97f67e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0ee8db7a3ae64722b16fc4a5430a51d4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1c78a06fb8ae472c84984b8bfc4f23d4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f2b557e0f94e48d4b6cee136f4662575",
       "max": 436.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8a5a6200c2ee40aea0ce14223cf6177c",
       "tabbable": null,
       "tooltip": null,
       "value": 436.0
      }
     },
     "1dbcf28124854457a3328acbedf56253": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "26cf3251f0b849b8a22c798403579e29": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2c6387141b934e71b0dc1910c0e1614e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0ee8db7a3ae64722b16fc4a5430a51d4",
       "placeholder": "​",
       "style": "IPY_MODEL_40331b02d80241c29992ec64a6e498bb",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors: 100%"
      }
     },
     "35fae08c97934f119f4d89ffcab20608": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_dde294308b3d41f2b2ed3b876b0f2715",
        "IPY_MODEL_3fa6fe23d7c448dcb51859087fea1621",
        "IPY_MODEL_c5c103a720e049419bdaeb0208cabb19"
       ],
       "layout": "IPY_MODEL_baec96c3f777496abc18b66461095d7e",
       "tabbable": null,
       "tooltip": null
      }
     },
     "3fa6fe23d7c448dcb51859087fea1621": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d46d80c953064e1b8ecf1429d8b1bc92",
       "max": 548.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_62bca30a64f24a7485c7404e987c8f8e",
       "tabbable": null,
       "tooltip": null,
       "value": 548.0
      }
     },
     "40331b02d80241c29992ec64a6e498bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "49cd7373abf440389256fef7b1cf92db": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "551687c113b24175a827f553db626644": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "62bca30a64f24a7485c7404e987c8f8e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "76e927655ddb472caf365841b0ccb579": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_49cd7373abf440389256fef7b1cf92db",
       "max": 346345912.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8c75dd74b37743b7ba3b54b76a82924c",
       "tabbable": null,
       "tooltip": null,
       "value": 346345912.0
      }
     },
     "8746f6430cb340cc81adc4620aed9e8a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_078d35baded94376b880bada5a97f67e",
       "placeholder": "​",
       "style": "IPY_MODEL_047758ee2a92412aa28632838f757e8c",
       "tabbable": null,
       "tooltip": null,
       "value": " 436/436 [00:00&lt;00:00, 33.7kB/s]"
      }
     },
     "8a5a6200c2ee40aea0ce14223cf6177c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "8b1bc533b8e44a78b932c33c2b26615d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8c75dd74b37743b7ba3b54b76a82924c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "92a56c09a90443c1bfbba2c83a80d7fb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c2a7f44ea149467fbcc1d281f4c38cb0",
        "IPY_MODEL_1c78a06fb8ae472c84984b8bfc4f23d4",
        "IPY_MODEL_8746f6430cb340cc81adc4620aed9e8a"
       ],
       "layout": "IPY_MODEL_b98ef7d237ae499c81bc8904fd8438c4",
       "tabbable": null,
       "tooltip": null
      }
     },
     "9a89a89bbd9945969018ac6433dd146e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a4940622c90b49c58066a3fbd9dec550": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b98ef7d237ae499c81bc8904fd8438c4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "baec96c3f777496abc18b66461095d7e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bbea506fc1af49f98d5e31165a2eba92": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c2a7f44ea149467fbcc1d281f4c38cb0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a4940622c90b49c58066a3fbd9dec550",
       "placeholder": "​",
       "style": "IPY_MODEL_551687c113b24175a827f553db626644",
       "tabbable": null,
       "tooltip": null,
       "value": "preprocessor_config.json: 100%"
      }
     },
     "c5c103a720e049419bdaeb0208cabb19": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_26cf3251f0b849b8a22c798403579e29",
       "placeholder": "​",
       "style": "IPY_MODEL_e81c948d04244620b1c600df66bb9024",
       "tabbable": null,
       "tooltip": null,
       "value": " 548/548 [00:00&lt;00:00, 55.9kB/s]"
      }
     },
     "d46d80c953064e1b8ecf1429d8b1bc92": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dd8d3f2ba8f04805a98fb6049800c92b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2c6387141b934e71b0dc1910c0e1614e",
        "IPY_MODEL_76e927655ddb472caf365841b0ccb579",
        "IPY_MODEL_ffdf9b930356418788a3c1a322a08ff9"
       ],
       "layout": "IPY_MODEL_8b1bc533b8e44a78b932c33c2b26615d",
       "tabbable": null,
       "tooltip": null
      }
     },
     "dde294308b3d41f2b2ed3b876b0f2715": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9a89a89bbd9945969018ac6433dd146e",
       "placeholder": "​",
       "style": "IPY_MODEL_1dbcf28124854457a3328acbedf56253",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json: 100%"
      }
     },
     "e81c948d04244620b1c600df66bb9024": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f2b557e0f94e48d4b6cee136f4662575": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ffdf9b930356418788a3c1a322a08ff9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_bbea506fc1af49f98d5e31165a2eba92",
       "placeholder": "​",
       "style": "IPY_MODEL_03da5713bbe24d7cb62d51d54538eaf4",
       "tabbable": null,
       "tooltip": null,
       "value": " 346M/346M [00:00&lt;00:00, 393MB/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
