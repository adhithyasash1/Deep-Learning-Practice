{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d2a6027",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T10:49:24.737493Z",
     "iopub.status.busy": "2025-03-30T10:49:24.737163Z",
     "iopub.status.idle": "2025-03-30T10:49:29.645287Z",
     "shell.execute_reply": "2025-03-30T10:49:29.644431Z"
    },
    "papermill": {
     "duration": 4.912619,
     "end_time": "2025-03-30T10:49:29.646982",
     "exception": false,
     "start_time": "2025-03-30T10:49:24.734363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\r\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.5)\r\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\r\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\r\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\r\n",
      "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\r\n",
      "Collecting pytorch-msssim\r\n",
      "  Downloading pytorch_msssim-1.0.0-py3-none-any.whl.metadata (8.0 kB)\r\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\r\n",
      "Collecting torch_optimizer\r\n",
      "  Downloading torch_optimizer-0.3.0-py3-none-any.whl.metadata (55 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.9.0.post0)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\r\n",
      "Collecting pytorch-ranger>=0.1.1 (from torch_optimizer)\r\n",
      "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl.metadata (509 bytes)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\r\n",
      "Downloading pytorch_msssim-1.0.0-py3-none-any.whl (7.7 kB)\r\n",
      "Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\r\n",
      "Installing collected packages: pytorch-ranger, pytorch-msssim, torch_optimizer\r\n",
      "Successfully installed pytorch-msssim-1.0.0 pytorch-ranger-0.1.1 torch_optimizer-0.3.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python matplotlib scikit-learn pillow tqdm torch torchvision opencv-contrib-python pytorch-msssim transformers torch_optimizer pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70883114",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T10:49:29.653653Z",
     "iopub.status.busy": "2025-03-30T10:49:29.653393Z",
     "iopub.status.idle": "2025-03-30T10:49:34.817424Z",
     "shell.execute_reply": "2025-03-30T10:49:34.816391Z"
    },
    "papermill": {
     "duration": 5.168804,
     "end_time": "2025-03-30T10:49:34.818912",
     "exception": false,
     "start_time": "2025-03-30T10:49:29.650108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\r\n",
      "Collecting albumentations\r\n",
      "  Downloading albumentations-2.0.5-py3-none-any.whl.metadata (41 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\r\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\r\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.11.0a2)\r\n",
      "Collecting albucore==0.0.23 (from albumentations)\r\n",
      "  Downloading albucore-0.0.23-py3-none-any.whl.metadata (5.3 kB)\r\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\r\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.23->albumentations) (3.11.1)\r\n",
      "Collecting simsimd>=5.9.2 (from albucore==0.0.23->albumentations)\r\n",
      "  Downloading simsimd-6.2.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (66 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2.4.1)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (2.29.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (4.12.2)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24.4->albumentations) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24.4->albumentations) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.24.4->albumentations) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.24.4->albumentations) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.24.4->albumentations) (2024.2.0)\r\n",
      "Downloading albumentations-2.0.5-py3-none-any.whl (290 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.6/290.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading albucore-0.0.23-py3-none-any.whl (14 kB)\r\n",
      "Downloading simsimd-6.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (632 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m632.7/632.7 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: simsimd, albucore, albumentations\r\n",
      "  Attempting uninstall: albucore\r\n",
      "    Found existing installation: albucore 0.0.19\r\n",
      "    Uninstalling albucore-0.0.19:\r\n",
      "      Successfully uninstalled albucore-0.0.19\r\n",
      "  Attempting uninstall: albumentations\r\n",
      "    Found existing installation: albumentations 1.4.20\r\n",
      "    Uninstalling albumentations-1.4.20:\r\n",
      "      Successfully uninstalled albumentations-1.4.20\r\n",
      "Successfully installed albucore-0.0.23 albumentations-2.0.5 simsimd-6.2.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -U albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e6924a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T10:49:34.827682Z",
     "iopub.status.busy": "2025-03-30T10:49:34.827435Z",
     "iopub.status.idle": "2025-03-30T12:41:00.965833Z",
     "shell.execute_reply": "2025-03-30T12:41:00.964681Z"
    },
    "papermill": {
     "duration": 6686.144458,
     "end_time": "2025-03-30T12:41:00.967146",
     "exception": false,
     "start_time": "2025-03-30T10:49:34.822688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training: 100%|██████████| 418/418 [03:34<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Training Loss: 0.7825, RMSE: 0.8280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:09<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Overall Loss: 0.6930, RMSE: 0.8765\n",
      "Low-Light Loss: 0.6952, Noisy Loss: 0.0000\n",
      "Best model saved with val loss: 0.6930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training: 100%|██████████| 418/418 [03:32<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30, Training Loss: 0.8007, RMSE: 0.8704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:09<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Overall Loss: 0.7268, RMSE: 0.9479\n",
      "Low-Light Loss: 0.7302, Noisy Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training: 100%|██████████| 418/418 [03:32<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30, Training Loss: 0.8273, RMSE: 0.9070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:09<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Overall Loss: 0.6195, RMSE: 0.7697\n",
      "Low-Light Loss: 0.6171, Noisy Loss: 0.0000\n",
      "Best model saved with val loss: 0.6195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training: 100%|██████████| 418/418 [03:32<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30, Training Loss: 0.8440, RMSE: 0.9011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:09<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Overall Loss: 0.8044, RMSE: 1.0993\n",
      "Low-Light Loss: 0.8080, Noisy Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training: 100%|██████████| 418/418 [03:32<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30, Training Loss: 0.8512, RMSE: 0.8868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:09<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Overall Loss: 0.6331, RMSE: 0.7836\n",
      "Low-Light Loss: 0.6346, Noisy Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Training: 100%|██████████| 418/418 [03:32<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30, Training Loss: 0.8311, RMSE: 0.8644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:09<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Overall Loss: 0.6874, RMSE: 0.8695\n",
      "Low-Light Loss: 0.6900, Noisy Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Training: 100%|██████████| 418/418 [03:32<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30, Training Loss: 0.8445, RMSE: 0.8759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:09<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Overall Loss: 0.6753, RMSE: 0.8514\n",
      "Low-Light Loss: 0.6779, Noisy Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Training: 100%|██████████| 418/418 [03:32<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30, Training Loss: 0.8442, RMSE: 0.8683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:09<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Overall Loss: 0.6757, RMSE: 0.8455\n",
      "Low-Light Loss: 0.6775, Noisy Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 Training: 100%|██████████| 418/418 [03:32<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30, Training Loss: 0.8413, RMSE: 0.8526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:09<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Overall Loss: 0.6244, RMSE: 0.7530\n",
      "Low-Light Loss: 0.6253, Noisy Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 Training: 100%|██████████| 418/418 [03:32<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30, Training Loss: 0.8565, RMSE: 0.8636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:09<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Overall Loss: 0.7090, RMSE: 0.9100\n",
      "Low-Light Loss: 0.7121, Noisy Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 Training: 100%|██████████| 418/418 [03:32<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30, Training Loss: 0.8510, RMSE: 0.8532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:09<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Overall Loss: 0.6095, RMSE: 0.7362\n",
      "Low-Light Loss: 0.6110, Noisy Loss: 0.0000\n",
      "Best model saved with val loss: 0.6095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 Training: 100%|██████████| 418/418 [03:32<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30, Training Loss: 0.8488, RMSE: 0.8345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:09<00:00,  5.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Overall Loss: 0.5834, RMSE: 0.6889\n",
      "Low-Light Loss: 0.5825, Noisy Loss: 0.0000\n",
      "Best model saved with val loss: 0.5834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 Training: 100%|██████████| 418/418 [03:32<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30, Training Loss: 0.8518, RMSE: 0.8280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:09<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Overall Loss: 0.5993, RMSE: 0.7116\n",
      "Low-Light Loss: 0.5990, Noisy Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 Training: 100%|██████████| 418/418 [03:31<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30, Training Loss: 0.8495, RMSE: 0.8158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:09<00:00,  5.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Overall Loss: 0.5960, RMSE: 0.7062\n",
      "Low-Light Loss: 0.5961, Noisy Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 Training: 100%|██████████| 418/418 [03:31<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30, Training Loss: 0.8456, RMSE: 0.8012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:09<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Overall Loss: 0.5866, RMSE: 0.6933\n",
      "Low-Light Loss: 0.5869, Noisy Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 Training: 100%|██████████| 418/418 [03:31<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30, Training Loss: 0.8524, RMSE: 0.8434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:09<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Overall Loss: 0.7561, RMSE: 1.0117\n",
      "Low-Light Loss: 0.7587, Noisy Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 Training: 100%|██████████| 418/418 [03:30<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30, Training Loss: 0.8588, RMSE: 0.8512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:09<00:00,  5.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Overall Loss: 0.6467, RMSE: 0.7976\n",
      "Low-Light Loss: 0.6489, Noisy Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 Training: 100%|██████████| 418/418 [03:31<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30, Training Loss: 0.8558, RMSE: 0.8393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:09<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Overall Loss: 0.6338, RMSE: 0.7624\n",
      "Low-Light Loss: 0.6357, Noisy Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 Training: 100%|██████████| 418/418 [03:31<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30, Training Loss: 0.8528, RMSE: 0.8395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:09<00:00,  5.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Overall Loss: 0.6830, RMSE: 0.8513\n",
      "Low-Light Loss: 0.6858, Noisy Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 Training: 100%|██████████| 418/418 [03:31<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30, Training Loss: 0.8481, RMSE: 0.8267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:09<00:00,  5.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Overall Loss: 0.6286, RMSE: 0.7670\n",
      "Low-Light Loss: 0.6304, Noisy Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21 Training: 100%|██████████| 418/418 [03:31<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30, Training Loss: 0.8446, RMSE: 0.8181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:09<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Overall Loss: 0.6397, RMSE: 0.7834\n",
      "Low-Light Loss: 0.6417, Noisy Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22 Training: 100%|██████████| 418/418 [03:31<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30, Training Loss: 0.8444, RMSE: 0.8121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:09<00:00,  5.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Overall Loss: 0.6606, RMSE: 0.8209\n",
      "Low-Light Loss: 0.6630, Noisy Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23 Training: 100%|██████████| 418/418 [03:30<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30, Training Loss: 0.8474, RMSE: 0.8188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:09<00:00,  5.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Overall Loss: 0.7208, RMSE: 0.9285\n",
      "Low-Light Loss: 0.7244, Noisy Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24 Training: 100%|██████████| 418/418 [03:31<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30, Training Loss: 0.8512, RMSE: 0.8237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:09<00:00,  5.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Overall Loss: 0.6514, RMSE: 0.8003\n",
      "Low-Light Loss: 0.6541, Noisy Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25 Training: 100%|██████████| 418/418 [03:32<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30, Training Loss: 0.8464, RMSE: 0.8187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:09<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Overall Loss: 0.6228, RMSE: 0.7554\n",
      "Low-Light Loss: 0.6244, Noisy Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26 Training: 100%|██████████| 418/418 [03:32<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30, Training Loss: 0.8446, RMSE: 0.8245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:09<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Overall Loss: 0.5832, RMSE: 0.7078\n",
      "Low-Light Loss: 0.5824, Noisy Loss: 0.0000\n",
      "Best model saved with val loss: 0.5832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27 Training: 100%|██████████| 418/418 [03:31<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30, Training Loss: 0.8434, RMSE: 0.8174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:09<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Overall Loss: 0.6437, RMSE: 0.7968\n",
      "Low-Light Loss: 0.6460, Noisy Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28 Training: 100%|██████████| 418/418 [03:31<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30, Training Loss: 0.8400, RMSE: 0.8114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:09<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Overall Loss: 0.5855, RMSE: 0.6896\n",
      "Low-Light Loss: 0.5856, Noisy Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29 Training: 100%|██████████| 418/418 [03:31<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30, Training Loss: 0.8407, RMSE: 0.8085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:09<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Overall Loss: 0.6177, RMSE: 0.7496\n",
      "Low-Light Loss: 0.6193, Noisy Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30 Training: 100%|██████████| 418/418 [03:31<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30, Training Loss: 0.8303, RMSE: 0.7866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 53/53 [00:09<00:00,  5.64it/s]\n",
      "<ipython-input-3-9fcb6bff1872>:428: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Overall Loss: 0.6386, RMSE: 0.7837\n",
      "Low-Light Loss: 0.6410, Noisy Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Predictions: 100%|██████████| 53/53 [00:15<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved to submission.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Note: Warning about Albumentations version being outdated (1.4.20 vs 2.0.5)\n",
    "# pip install -U albumentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import GradScaler, autocast\n",
    "from pytorch_msssim import SSIM\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, Tuple\n",
    "import torch.backends.cudnn as cudnn\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# ------------------- Reproducibility -------------------\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "cudnn.deterministic = True\n",
    "cudnn.benchmark = False\n",
    "\n",
    "# ------------------- Configuration ---------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = 256\n",
    "OUTPUT_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 30\n",
    "BASE_LR = 2e-5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "NUM_WORKERS = 4\n",
    "USE_AMP = True\n",
    "CLIP_VALUE = 1.0\n",
    "\n",
    "\n",
    "# ------------------- Data Augmentations & Preprocessing -------------------\n",
    "def apply_clahe(image: np.ndarray, **kwargs) -> np.ndarray:\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l)\n",
    "    merged = cv2.merge((cl, a, b))\n",
    "    return cv2.cvtColor(merged, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Lambda(image=apply_clahe),\n",
    "    A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.GaussNoise(std_range=(10/255, 50/255), p=0.5),\n",
    "    A.ISONoise(p=0.5),\n",
    "    A.HueSaturationValue(p=0.3),  # Added for better color variation.\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "], additional_targets={\"mask\": \"mask\"})\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Lambda(image=apply_clahe),\n",
    "    A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "], additional_targets={\"mask\": \"mask\"})\n",
    "\n",
    "\n",
    "# ------------------- Dataset -------------------\n",
    "class DepthDataset(Dataset):\n",
    "    def __init__(self, rgb_dir: str, depth_dir: Optional[str] = None, transform: Optional[A.Compose] = None,\n",
    "                 is_val: bool = False):\n",
    "        self.rgb_dir = rgb_dir\n",
    "        self.depth_dir = depth_dir\n",
    "        self.transform = transform\n",
    "        self.filenames = sorted(os.listdir(rgb_dir), key=lambda x: int(''.join(filter(str.isdigit, x)) or 0))\n",
    "        self.is_val = is_val\n",
    "        self.low_light_indices = []\n",
    "        self.noisy_indices = []\n",
    "        if is_val and depth_dir:\n",
    "            self._split_validation_set()\n",
    "\n",
    "    def _split_validation_set(self):\n",
    "        for idx, fname in enumerate(self.filenames):\n",
    "            rgb_path = os.path.join(self.rgb_dir, fname)\n",
    "            image = cv2.imread(rgb_path, cv2.IMREAD_COLOR)\n",
    "            brightness = np.mean(image)\n",
    "            noise_level = np.std(image)\n",
    "            if brightness < 50:\n",
    "                self.low_light_indices.append(idx)\n",
    "            if noise_level > 30:\n",
    "                self.noisy_indices.append(idx)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        fname = self.filenames[idx]\n",
    "        rgb_path = os.path.join(self.rgb_dir, fname)\n",
    "        image = cv2.imread(rgb_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        depth = None\n",
    "        if self.depth_dir:\n",
    "            depth_path = os.path.join(self.depth_dir, fname)\n",
    "            depth = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED).astype(np.float32)\n",
    "            depth = cv2.resize(depth, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "            global_mean, global_std = 88.2930, 61.7739\n",
    "            depth = (depth - global_mean) / (global_std + 1e-8)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=depth if depth is not None else image)\n",
    "            image = augmented[\"image\"]\n",
    "            if depth is not None:\n",
    "                depth = augmented[\"mask\"]\n",
    "\n",
    "        if depth is not None:\n",
    "            depth = torch.from_numpy(depth).float() if isinstance(depth, np.ndarray) else depth.clone().detach().float()\n",
    "            depth = depth.unsqueeze(0)  # check Shape (1, H, W)\n",
    "            # Resizing to OUTPUT_SIZE\n",
    "            depth = torch.nn.functional.interpolate(depth.unsqueeze(0), size=(OUTPUT_SIZE, OUTPUT_SIZE),\n",
    "                                                    mode='bilinear', align_corners=False).squeeze(0)\n",
    "\n",
    "        return (image, depth) if depth is not None else (image, fname)\n",
    "\n",
    "\n",
    "# ------------------- Model Components -------------------\n",
    "def conv_block(in_channels: int, out_channels: int, use_se: bool = False):\n",
    "    layers = [\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    ]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes: int, ratio: int = 16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "\n",
    "# ------------------- Coarse Network with Skip Connections and Dilated Convolution -------------------\n",
    "class CoarseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CoarseNet, self).__init__()\n",
    "        # Encoder layers\n",
    "        self.encoder1 = conv_block(3, 64)\n",
    "        self.encoder2 = conv_block(64, 128)\n",
    "        self.encoder3 = conv_block(128, 256)\n",
    "        self.encoder4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, dilation=2, padding=2),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Decoder layers with skip connections\n",
    "        self.up4 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder4 = conv_block(512, 256)\n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder3 = conv_block(256, 128)\n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder2 = conv_block(128, 64)\n",
    "        self.decoder1 = nn.Conv2d(64, 1, kernel_size=3, padding=1)\n",
    "        self.final_pool = nn.AdaptiveAvgPool2d((OUTPUT_SIZE, OUTPUT_SIZE))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        e1 = self.encoder1(x)  # (B, 64, H, W)\n",
    "        e2 = self.encoder2(self.pool(e1))  # (B, 128, H/2, W/2)\n",
    "        e3 = self.encoder3(self.pool(e2))  # (B, 256, H/4, W/4)\n",
    "        e4 = self.encoder4(self.pool(e3))  # (B, 512, H/8, W/8)\n",
    "        u4 = self.up4(e4)  # (B, 256, H/4, W/4)\n",
    "        # Skip connection: concatenating encoder3 features with upsampled e4\n",
    "        d4 = self.decoder4(torch.cat([u4, e3], dim=1))\n",
    "        u3 = self.up3(d4)  # (B, 128, H/2, W/2)\n",
    "        d3 = self.decoder3(torch.cat([u3, e2], dim=1))\n",
    "        u2 = self.up2(d3)  # (B, 64, H, W)\n",
    "        d2 = self.decoder2(torch.cat([u2, e1], dim=1))\n",
    "        coarse = self.decoder1(d2)\n",
    "        return self.final_pool(coarse)\n",
    "\n",
    "\n",
    "# ------------------- Fine Network with Channel Attention -------------------\n",
    "class FineNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FineNet, self).__init__()\n",
    "        # Input channels: 1 (coarse) + 3 (denoised RGB) = 4\n",
    "        self.conv1 = conv_block(4, 64, use_se=True)\n",
    "        self.ca1 = ChannelAttention(64)\n",
    "        self.conv2 = conv_block(64, 64, use_se=True)\n",
    "        self.ca2 = ChannelAttention(64)\n",
    "        self.conv3 = conv_block(64, 32, use_se=True)\n",
    "        self.out_conv = nn.Conv2d(32, 1, kernel_size=3, padding=1)\n",
    "        self.final_pool = nn.AdaptiveAvgPool2d((OUTPUT_SIZE, OUTPUT_SIZE))\n",
    "\n",
    "    def forward(self, coarse: torch.Tensor, rgb: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.cat([coarse, rgb], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = x * self.ca1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x * self.ca2(x)\n",
    "        x = self.conv3(x)\n",
    "        refined = self.out_conv(x)\n",
    "        return self.final_pool(refined)\n",
    "\n",
    "\n",
    "# ------------------- Combined Depth Estimation Model -------------------\n",
    "class DepthEstimationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DepthEstimationModel, self).__init__()\n",
    "        self.denoising_branch = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 3, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.coarse_net = CoarseNet()\n",
    "        self.fine_net = FineNet()\n",
    "        # Adding a final resize layer to ensure output size is correct\n",
    "        self.final_resize = nn.AdaptiveAvgPool2d((OUTPUT_SIZE, OUTPUT_SIZE))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        denoised = self.denoising_branch(x)\n",
    "        coarse = self.coarse_net(denoised)\n",
    "\n",
    "        # Resizing denoised to match coarse output size\n",
    "        denoised_resized = torch.nn.functional.interpolate(\n",
    "            denoised,\n",
    "            size=(OUTPUT_SIZE, OUTPUT_SIZE),\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        )\n",
    "\n",
    "        refined = self.fine_net(coarse, denoised_resized)\n",
    "        return self.final_resize(refined)\n",
    "\n",
    "\n",
    "# ------------------- Discriminator with Label Smoothing -------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 1, 4, 2, 1)\n",
    "            # Removed the Sigmoid layer here\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.main(x).view(-1, 1)\n",
    "\n",
    "\n",
    "# ------------------- Loss Functions -------------------\n",
    "class DepthLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.huber_loss = nn.HuberLoss()\n",
    "        self.ssim = SSIM(data_range=1.0, size_average=True, channel=1)\n",
    "\n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor, lambda_weight: float = 0.5) -> torch.Tensor:\n",
    "        l1 = self.l1_loss(pred, target)\n",
    "        huber = self.huber_loss(pred, target)\n",
    "        combined_loss = lambda_weight * l1 + (1 - lambda_weight) * huber\n",
    "        ssim_loss = 1 - self.ssim(pred, target)\n",
    "        return 0.6 * combined_loss + 0.4 * ssim_loss\n",
    "\n",
    "\n",
    "class GANLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()  # Changed from BCELoss to BCEWithLogitsLoss\n",
    "\n",
    "    def forward(self, pred: torch.Tensor, target_is_real: bool) -> torch.Tensor:\n",
    "        # Label smoothing: using 0.9 for real and 0.1 for fake.\n",
    "        target = torch.full_like(pred, 0.9) if target_is_real else torch.full_like(pred, 0.1)\n",
    "        return self.bce_loss(pred, target)\n",
    "\n",
    "\n",
    "# ------------------- RMSE Metric -------------------\n",
    "def compute_rmse(pred: torch.Tensor, target: torch.Tensor) -> float:\n",
    "    # Using .detach() before .cpu().numpy() to detach from computational graph\n",
    "    pred_np = pred.detach().cpu().numpy().flatten()\n",
    "    target_np = target.detach().cpu().numpy().flatten()\n",
    "    return np.sqrt(np.mean((pred_np - target_np) ** 2))\n",
    "\n",
    "\n",
    "# ------------------- Main Function -------------------\n",
    "def main():\n",
    "    # Dataset paths\n",
    "    train_rgb_dir = \"/kaggle/input/depth-estimation/competition-data/competition-data/training/images\"\n",
    "    train_depth_dir = \"/kaggle/input/depth-estimation/competition-data/competition-data/training/depths\"\n",
    "    val_rgb_dir = \"/kaggle/input/depth-estimation/competition-data/competition-data/validation/images\"\n",
    "    val_depth_dir = \"/kaggle/input/depth-estimation/competition-data/competition-data/validation/depths\"\n",
    "    test_rgb_dir = \"/kaggle/input/depth-estimation/competition-data/competition-data/testing/images\"\n",
    "    output_dir = \"predictions_folder\"\n",
    "    csv_path = \"submission.csv\"\n",
    "\n",
    "    # Initializing datasets\n",
    "    train_dataset = DepthDataset(train_rgb_dir, train_depth_dir, transform=train_transform)\n",
    "    val_dataset = DepthDataset(val_rgb_dir, val_depth_dir, transform=val_transform, is_val=True)\n",
    "    test_dataset = DepthDataset(test_rgb_dir, transform=val_transform)\n",
    "\n",
    "    # Initializing data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS,\n",
    "                              pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS,\n",
    "                             pin_memory=True)\n",
    "\n",
    "    # Initializing model, discriminator, optimizers, scheduler, and loss functions\n",
    "    model = DepthEstimationModel().to(DEVICE)\n",
    "    discriminator = Discriminator().to(DEVICE)\n",
    "    depth_criterion = DepthLoss().to(DEVICE)\n",
    "    gan_criterion = GANLoss().to(DEVICE)\n",
    "    optimizer_g = optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
    "    optimizer_d = optim.AdamW(discriminator.parameters(), lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer_g, T_0=5, T_mult=2)\n",
    "    scaler = GradScaler('cuda' if torch.cuda.is_available() else 'cpu', enabled=USE_AMP)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        discriminator.train()\n",
    "        train_loss = 0.0\n",
    "        rmse_sum = 0.0\n",
    "        for images, depths in tqdm(train_loader, desc=f\"Epoch {epoch + 1} Training\"):\n",
    "            images, depths = images.to(DEVICE), depths.to(DEVICE)\n",
    "\n",
    "            # --- Train Discriminator ---\n",
    "            optimizer_d.zero_grad()\n",
    "            with autocast('cuda' if torch.cuda.is_available() else 'cpu', enabled=USE_AMP):\n",
    "                fake_depth = model(images)\n",
    "                d_real = discriminator(depths)\n",
    "                d_fake = discriminator(fake_depth.detach())\n",
    "                d_loss_real = gan_criterion(d_real, True)\n",
    "                d_loss_fake = gan_criterion(d_fake, False)\n",
    "                d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
    "            scaler.scale(d_loss).backward()\n",
    "            scaler.step(optimizer_d)\n",
    "            scaler.update()\n",
    "\n",
    "            # --- Train Generator ---\n",
    "            optimizer_g.zero_grad()\n",
    "            with autocast('cuda' if torch.cuda.is_available() else 'cpu', enabled=USE_AMP):\n",
    "                fake_depth = model(images)\n",
    "                d_fake = discriminator(fake_depth)\n",
    "                g_loss_adv = gan_criterion(d_fake, True)\n",
    "                g_loss_depth = depth_criterion(fake_depth, depths)\n",
    "                g_loss = g_loss_depth + 0.1 * g_loss_adv\n",
    "            scaler.scale(g_loss).backward()\n",
    "            scaler.unscale_(optimizer_g)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_VALUE)\n",
    "            scaler.step(optimizer_g)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss += g_loss.item()\n",
    "            rmse_sum += compute_rmse(fake_depth, depths)\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_train_rmse = rmse_sum / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Training Loss: {avg_train_loss:.4f}, RMSE: {avg_train_rmse:.4f}\")\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        rmse_val_sum = 0.0\n",
    "        low_light_loss = 0.0\n",
    "        noisy_loss = 0.0\n",
    "        low_light_count = 0\n",
    "        noisy_count = 0\n",
    "        with torch.no_grad():\n",
    "            for idx, (images, depths) in enumerate(tqdm(val_loader, desc=\"Validating\")):\n",
    "                images, depths = images.to(DEVICE), depths.to(DEVICE)\n",
    "                with autocast('cuda' if torch.cuda.is_available() else 'cpu', enabled=USE_AMP):\n",
    "                    outputs = model(images)\n",
    "                    loss = depth_criterion(outputs, depths)\n",
    "                val_loss += loss.item()\n",
    "                rmse_val_sum += compute_rmse(outputs, depths)\n",
    "\n",
    "                batch_size = images.size(0)\n",
    "                batch_indices = range(idx * batch_size, (idx + 1) * batch_size)\n",
    "                for i, batch_idx in enumerate(batch_indices):\n",
    "                    if batch_idx in val_dataset.low_light_indices:\n",
    "                        low_light_loss += loss.item()\n",
    "                        low_light_count += 1\n",
    "                    if batch_idx in val_dataset.noisy_indices:\n",
    "                        noisy_loss += loss.item()\n",
    "                        noisy_count += 1\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_val_rmse = rmse_val_sum / len(val_loader)\n",
    "        avg_low_light_loss = low_light_loss / max(low_light_count, 1)\n",
    "        avg_noisy_loss = noisy_loss / max(noisy_count, 1)\n",
    "        print(f\"Validation - Overall Loss: {avg_val_loss:.4f}, RMSE: {avg_val_rmse:.4f}\")\n",
    "        print(f\"Low-Light Loss: {avg_low_light_loss:.4f}, Noisy Loss: {avg_noisy_loss:.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(f\"Best model saved with val loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    # Loading the best model\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "    # --- Prediction and CSV Generation ---\n",
    "    model.eval()\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    global_mean, global_std = 88.2930, 61.7739\n",
    "    data = []\n",
    "    with torch.no_grad():\n",
    "        for images, filenames in tqdm(test_loader, desc=\"Generating Predictions\"):\n",
    "            images = images.to(DEVICE)\n",
    "            outputs = model(images).squeeze(1).cpu().numpy()\n",
    "            for i, fname in enumerate(filenames):\n",
    "                output = outputs[i]\n",
    "                # Converting normalized output back to original scale\n",
    "                output = output * global_std + global_mean\n",
    "                output = np.clip(output, 0, 255).astype(np.uint8)\n",
    "                cv2.imwrite(os.path.join(output_dir, f\"{fname}.png\"), output)\n",
    "\n",
    "                # Resizing to fixed 128x128 for CSV and normalize robustly.\n",
    "                image_resized = cv2.resize(output, (128, 128))\n",
    "                if np.max(image_resized) > np.min(image_resized):\n",
    "                    image_norm = (image_resized - np.min(image_resized)) / (\n",
    "                            np.max(image_resized) - np.min(image_resized) + 1e-6)\n",
    "                else:\n",
    "                    image_norm = image_resized  # avoiding division by zero if image is constant.\n",
    "                image_norm = np.uint8(image_norm * 255)\n",
    "                image_flat = image_norm.flatten().tolist()\n",
    "                row = [len(data), fname] + image_flat\n",
    "                data.append(row)\n",
    "\n",
    "    num_columns = len(data[0]) - 2 if data else 0\n",
    "    column_names = [\"id\", \"ImageID\"] + [str(indx) for indx in range(num_columns)]\n",
    "    df = pd.DataFrame(data, columns=column_names)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"CSV file saved to {csv_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11466546,
     "isSourceIdPinned": false,
     "sourceId": 96480,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6701.336426,
   "end_time": "2025-03-30T12:41:03.534773",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-30T10:49:22.198347",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
