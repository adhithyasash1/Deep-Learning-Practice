{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea985b29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T11:06:25.074408Z",
     "iopub.status.busy": "2025-03-29T11:06:25.074120Z",
     "iopub.status.idle": "2025-03-29T11:06:29.912046Z",
     "shell.execute_reply": "2025-03-29T11:06:29.910959Z"
    },
    "papermill": {
     "duration": 4.842639,
     "end_time": "2025-03-29T11:06:29.913824",
     "exception": false,
     "start_time": "2025-03-29T11:06:25.071185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\r\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.5)\r\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\r\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\r\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\r\n",
      "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\r\n",
      "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\r\n",
      "Collecting pytorch-msssim\r\n",
      "  Downloading pytorch_msssim-1.0.0-py3-none-any.whl.metadata (8.0 kB)\r\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\r\n",
      "Collecting torch_optimizer\r\n",
      "  Downloading torch_optimizer-0.3.0-py3-none-any.whl.metadata (55 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.9.0.post0)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\r\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\r\n",
      "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.11.0a2)\r\n",
      "Requirement already satisfied: albucore==0.0.19 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.19)\r\n",
      "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\r\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\r\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.19->albumentations) (3.11.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\r\n",
      "Collecting pytorch-ranger>=0.1.1 (from torch_optimizer)\r\n",
      "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl.metadata (509 bytes)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.29.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\r\n",
      "Downloading pytorch_msssim-1.0.0-py3-none-any.whl (7.7 kB)\r\n",
      "Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\r\n",
      "Installing collected packages: pytorch-ranger, pytorch-msssim, torch_optimizer\r\n",
      "Successfully installed pytorch-msssim-1.0.0 pytorch-ranger-0.1.1 torch_optimizer-0.3.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python matplotlib scikit-learn pillow tqdm torch torchvision opencv-contrib-python albumentations pytorch-msssim transformers torch_optimizer pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd3b2fa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T11:06:29.921356Z",
     "iopub.status.busy": "2025-03-29T11:06:29.921113Z",
     "iopub.status.idle": "2025-03-29T12:29:49.664221Z",
     "shell.execute_reply": "2025-03-29T12:29:49.663079Z"
    },
    "papermill": {
     "duration": 4999.748556,
     "end_time": "2025-03-29T12:29:49.665730",
     "exception": false,
     "start_time": "2025-03-29T11:06:29.917174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Dataset initialized with 6686 valid samples\n",
      "Dataset initialized with 836 valid samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee14009a31942e380cc516ad33946cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/548 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating model with a sample batch...\n",
      "Feature shapes before FPN:\n",
      "Feature 0: shape torch.Size([1, 4, 730, 768])\n",
      "Feature 1: shape torch.Size([1, 4, 730, 768])\n",
      "Feature 2: shape torch.Size([1, 4, 730, 768])\n",
      "Feature 3: shape torch.Size([1, 4, 730, 768])\n",
      "Input feature shapes before processing:\n",
      "Feature 0: shape torch.Size([1, 4, 730, 768])\n",
      "Feature 1: shape torch.Size([1, 4, 730, 768])\n",
      "Feature 2: shape torch.Size([1, 4, 730, 768])\n",
      "Feature 3: shape torch.Size([1, 4, 730, 768])\n",
      "Initializing FPN conv layers with actual feature dimensions:\n",
      "Feature 0: shape torch.Size([1, 4, 730, 768]), channels 4\n",
      "Feature 1: shape torch.Size([1, 4, 730, 768]), channels 4\n",
      "Feature 2: shape torch.Size([1, 4, 730, 768]), channels 4\n",
      "Feature 3: shape torch.Size([1, 4, 730, 768]), channels 4\n",
      "Sample prediction shape: torch.Size([4, 1, 384, 384])\n",
      "Sample prediction stats: min=-1.1109, max=0.7268, mean=-0.0214\n",
      "Model validation successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-27fbb532e1d9>:623: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "<ipython-input-2-27fbb532e1d9>:493: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Batch 10/1672, Loss: 0.4863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pytorch_ranger/ranger.py:172: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value = 1) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1642.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Batch 20/1672, Loss: 0.4842\n",
      "Epoch 1/5, Batch 30/1672, Loss: 0.4966\n",
      "Epoch 1/5, Batch 40/1672, Loss: 0.5149\n",
      "Epoch 1/5, Batch 50/1672, Loss: 0.5275\n",
      "Epoch 1/5, Batch 60/1672, Loss: 0.4934\n",
      "Epoch 1/5, Batch 70/1672, Loss: 0.4896\n",
      "Epoch 1/5, Batch 80/1672, Loss: 0.5002\n",
      "Epoch 1/5, Batch 90/1672, Loss: 0.5162\n",
      "Epoch 1/5, Batch 100/1672, Loss: 0.4902\n",
      "Epoch 1/5, Batch 110/1672, Loss: 0.4963\n",
      "Epoch 1/5, Batch 120/1672, Loss: 0.4915\n",
      "Epoch 1/5, Batch 130/1672, Loss: 0.5046\n",
      "Epoch 1/5, Batch 140/1672, Loss: 0.4766\n",
      "Epoch 1/5, Batch 150/1672, Loss: 0.4932\n",
      "Epoch 1/5, Batch 160/1672, Loss: 0.4832\n",
      "Epoch 1/5, Batch 170/1672, Loss: 0.4518\n",
      "Epoch 1/5, Batch 180/1672, Loss: 0.4326\n",
      "Epoch 1/5, Batch 190/1672, Loss: 0.6175\n",
      "Epoch 1/5, Batch 200/1672, Loss: 0.4771\n",
      "Epoch 1/5, Batch 210/1672, Loss: 0.4679\n",
      "Epoch 1/5, Batch 220/1672, Loss: 0.4665\n",
      "Epoch 1/5, Batch 230/1672, Loss: 0.4613\n",
      "Epoch 1/5, Batch 240/1672, Loss: 0.4382\n",
      "Epoch 1/5, Batch 250/1672, Loss: 0.5357\n",
      "Epoch 1/5, Batch 260/1672, Loss: 0.4298\n",
      "Epoch 1/5, Batch 270/1672, Loss: 0.4973\n",
      "Epoch 1/5, Batch 280/1672, Loss: 0.4439\n",
      "Epoch 1/5, Batch 290/1672, Loss: 0.4676\n",
      "Epoch 1/5, Batch 300/1672, Loss: 0.4928\n",
      "Epoch 1/5, Batch 310/1672, Loss: 0.4706\n",
      "Epoch 1/5, Batch 320/1672, Loss: 0.4259\n",
      "Epoch 1/5, Batch 330/1672, Loss: 0.4419\n",
      "Epoch 1/5, Batch 340/1672, Loss: 0.4554\n",
      "Epoch 1/5, Batch 350/1672, Loss: 0.4900\n",
      "Epoch 1/5, Batch 360/1672, Loss: 0.4240\n",
      "Epoch 1/5, Batch 370/1672, Loss: 0.4223\n",
      "Epoch 1/5, Batch 380/1672, Loss: 0.5116\n",
      "Epoch 1/5, Batch 390/1672, Loss: 0.4790\n",
      "Epoch 1/5, Batch 400/1672, Loss: 0.4286\n",
      "Epoch 1/5, Batch 410/1672, Loss: 0.5040\n",
      "Epoch 1/5, Batch 420/1672, Loss: 0.4532\n",
      "Epoch 1/5, Batch 430/1672, Loss: 0.4374\n",
      "Epoch 1/5, Batch 440/1672, Loss: 0.4486\n",
      "Epoch 1/5, Batch 450/1672, Loss: 0.4299\n",
      "Epoch 1/5, Batch 460/1672, Loss: 0.4564\n",
      "Epoch 1/5, Batch 470/1672, Loss: 0.4399\n",
      "Epoch 1/5, Batch 480/1672, Loss: 0.4223\n",
      "Epoch 1/5, Batch 490/1672, Loss: 0.4264\n",
      "Epoch 1/5, Batch 500/1672, Loss: 0.3885\n",
      "Epoch 1/5, Batch 510/1672, Loss: 0.4000\n",
      "Epoch 1/5, Batch 520/1672, Loss: 0.4165\n",
      "Epoch 1/5, Batch 530/1672, Loss: 0.4707\n",
      "Epoch 1/5, Batch 540/1672, Loss: 0.4026\n",
      "Epoch 1/5, Batch 550/1672, Loss: 0.4037\n",
      "Epoch 1/5, Batch 560/1672, Loss: 0.3896\n",
      "Epoch 1/5, Batch 570/1672, Loss: 0.3981\n",
      "Epoch 1/5, Batch 580/1672, Loss: 0.3930\n",
      "Epoch 1/5, Batch 590/1672, Loss: 0.3758\n",
      "Epoch 1/5, Batch 600/1672, Loss: 0.4007\n",
      "Epoch 1/5, Batch 610/1672, Loss: 0.3872\n",
      "Epoch 1/5, Batch 620/1672, Loss: 0.3683\n",
      "Epoch 1/5, Batch 630/1672, Loss: 0.4192\n",
      "Epoch 1/5, Batch 640/1672, Loss: 0.3939\n",
      "Epoch 1/5, Batch 650/1672, Loss: 0.4105\n",
      "Epoch 1/5, Batch 660/1672, Loss: 0.3799\n",
      "Epoch 1/5, Batch 670/1672, Loss: 0.3517\n",
      "Epoch 1/5, Batch 680/1672, Loss: 0.3791\n",
      "Epoch 1/5, Batch 690/1672, Loss: 0.3781\n",
      "Epoch 1/5, Batch 700/1672, Loss: 0.3669\n",
      "Epoch 1/5, Batch 710/1672, Loss: 0.3552\n",
      "Epoch 1/5, Batch 720/1672, Loss: 0.3817\n",
      "Epoch 1/5, Batch 730/1672, Loss: 0.3431\n",
      "Epoch 1/5, Batch 740/1672, Loss: 0.3591\n",
      "Epoch 1/5, Batch 750/1672, Loss: 0.3272\n",
      "Epoch 1/5, Batch 760/1672, Loss: 0.3384\n",
      "Epoch 1/5, Batch 770/1672, Loss: 0.3358\n",
      "Epoch 1/5, Batch 780/1672, Loss: 0.3248\n",
      "Epoch 1/5, Batch 790/1672, Loss: 0.3146\n",
      "Epoch 1/5, Batch 800/1672, Loss: 0.2929\n",
      "Epoch 1/5, Batch 810/1672, Loss: 0.3433\n",
      "Epoch 1/5, Batch 820/1672, Loss: 0.3315\n",
      "Epoch 1/5, Batch 830/1672, Loss: 0.3420\n",
      "Epoch 1/5, Batch 840/1672, Loss: 0.3348\n",
      "Epoch 1/5, Batch 850/1672, Loss: 0.3345\n",
      "Epoch 1/5, Batch 860/1672, Loss: 0.3201\n",
      "Epoch 1/5, Batch 870/1672, Loss: 0.2985\n",
      "Epoch 1/5, Batch 880/1672, Loss: 0.3242\n",
      "Epoch 1/5, Batch 890/1672, Loss: 0.3435\n",
      "Epoch 1/5, Batch 900/1672, Loss: 0.3619\n",
      "Epoch 1/5, Batch 910/1672, Loss: 0.3323\n",
      "Epoch 1/5, Batch 920/1672, Loss: 0.3327\n",
      "Epoch 1/5, Batch 930/1672, Loss: 0.3084\n",
      "Epoch 1/5, Batch 940/1672, Loss: 0.3650\n",
      "Epoch 1/5, Batch 950/1672, Loss: 0.3198\n",
      "Epoch 1/5, Batch 960/1672, Loss: 0.3290\n",
      "Epoch 1/5, Batch 970/1672, Loss: 0.3154\n",
      "Epoch 1/5, Batch 980/1672, Loss: 0.3299\n",
      "Epoch 1/5, Batch 990/1672, Loss: 0.3252\n",
      "Epoch 1/5, Batch 1000/1672, Loss: 0.3454\n",
      "Epoch 1/5, Batch 1010/1672, Loss: 0.3335\n",
      "Epoch 1/5, Batch 1020/1672, Loss: 0.2924\n",
      "Epoch 1/5, Batch 1030/1672, Loss: 0.3454\n",
      "Epoch 1/5, Batch 1040/1672, Loss: 0.3261\n",
      "Epoch 1/5, Batch 1050/1672, Loss: 0.2704\n",
      "Epoch 1/5, Batch 1060/1672, Loss: 0.3174\n",
      "Epoch 1/5, Batch 1070/1672, Loss: 0.3113\n",
      "Epoch 1/5, Batch 1080/1672, Loss: 0.3643\n",
      "Epoch 1/5, Batch 1090/1672, Loss: 0.3082\n",
      "Epoch 1/5, Batch 1100/1672, Loss: 0.3140\n",
      "Epoch 1/5, Batch 1110/1672, Loss: 0.3652\n",
      "Epoch 1/5, Batch 1120/1672, Loss: 0.3317\n",
      "Epoch 1/5, Batch 1130/1672, Loss: 0.3008\n",
      "Epoch 1/5, Batch 1140/1672, Loss: 0.3176\n",
      "Epoch 1/5, Batch 1150/1672, Loss: 0.3651\n",
      "Epoch 1/5, Batch 1160/1672, Loss: 0.3055\n",
      "Epoch 1/5, Batch 1170/1672, Loss: 0.3278\n",
      "Epoch 1/5, Batch 1180/1672, Loss: 0.2913\n",
      "Epoch 1/5, Batch 1190/1672, Loss: 0.2732\n",
      "Epoch 1/5, Batch 1200/1672, Loss: 0.3474\n",
      "Epoch 1/5, Batch 1210/1672, Loss: 0.3277\n",
      "Epoch 1/5, Batch 1220/1672, Loss: 0.3157\n",
      "Epoch 1/5, Batch 1230/1672, Loss: 0.3042\n",
      "Epoch 1/5, Batch 1240/1672, Loss: 0.3280\n",
      "Epoch 1/5, Batch 1250/1672, Loss: 0.2736\n",
      "Epoch 1/5, Batch 1260/1672, Loss: 0.3035\n",
      "Epoch 1/5, Batch 1270/1672, Loss: 0.2900\n",
      "Epoch 1/5, Batch 1280/1672, Loss: 0.3276\n",
      "Epoch 1/5, Batch 1290/1672, Loss: 0.3733\n",
      "Epoch 1/5, Batch 1300/1672, Loss: 0.2675\n",
      "Epoch 1/5, Batch 1310/1672, Loss: 0.3066\n",
      "Epoch 1/5, Batch 1320/1672, Loss: 0.3124\n",
      "Epoch 1/5, Batch 1330/1672, Loss: 0.3239\n",
      "Epoch 1/5, Batch 1340/1672, Loss: 0.3456\n",
      "Epoch 1/5, Batch 1350/1672, Loss: 0.3346\n",
      "Epoch 1/5, Batch 1360/1672, Loss: 0.2717\n",
      "Epoch 1/5, Batch 1370/1672, Loss: 0.2875\n",
      "Epoch 1/5, Batch 1380/1672, Loss: 0.3155\n",
      "Epoch 1/5, Batch 1390/1672, Loss: 0.3037\n",
      "Epoch 1/5, Batch 1400/1672, Loss: 0.3205\n",
      "Epoch 1/5, Batch 1410/1672, Loss: 0.3057\n",
      "Epoch 1/5, Batch 1420/1672, Loss: 0.3498\n",
      "Epoch 1/5, Batch 1430/1672, Loss: 0.3315\n",
      "Epoch 1/5, Batch 1440/1672, Loss: 0.3030\n",
      "Epoch 1/5, Batch 1450/1672, Loss: 0.3120\n",
      "Epoch 1/5, Batch 1460/1672, Loss: 0.2890\n",
      "Epoch 1/5, Batch 1470/1672, Loss: 0.3177\n",
      "Epoch 1/5, Batch 1480/1672, Loss: 0.2738\n",
      "Epoch 1/5, Batch 1490/1672, Loss: 0.2770\n",
      "Epoch 1/5, Batch 1500/1672, Loss: 0.3043\n",
      "Epoch 1/5, Batch 1510/1672, Loss: 0.3722\n",
      "Epoch 1/5, Batch 1520/1672, Loss: 0.3191\n",
      "Epoch 1/5, Batch 1530/1672, Loss: 0.3249\n",
      "Epoch 1/5, Batch 1540/1672, Loss: 0.2783\n",
      "Epoch 1/5, Batch 1550/1672, Loss: 0.3437\n",
      "Epoch 1/5, Batch 1560/1672, Loss: 0.3140\n",
      "Epoch 1/5, Batch 1570/1672, Loss: 0.2703\n",
      "Epoch 1/5, Batch 1580/1672, Loss: 0.2717\n",
      "Epoch 1/5, Batch 1590/1672, Loss: 0.2718\n",
      "Epoch 1/5, Batch 1600/1672, Loss: 0.2932\n",
      "Epoch 1/5, Batch 1610/1672, Loss: 0.3236\n",
      "Epoch 1/5, Batch 1620/1672, Loss: 0.2934\n",
      "Epoch 1/5, Batch 1630/1672, Loss: 0.2932\n",
      "Epoch 1/5, Batch 1640/1672, Loss: 0.2692\n",
      "Epoch 1/5, Batch 1650/1672, Loss: 0.2984\n",
      "Epoch 1/5, Batch 1660/1672, Loss: 0.2691\n",
      "Epoch 1/5, Batch 1670/1672, Loss: 0.3050\n",
      "Warning: Feature 0 has 2 channels but expected 4. Reshaping...\n",
      "Warning: Feature 1 has 2 channels but expected 4. Reshaping...\n",
      "Warning: Feature 2 has 2 channels but expected 4. Reshaping...\n",
      "Warning: Feature 3 has 2 channels but expected 4. Reshaping...\n",
      "Batch statistics for epoch 1:\n",
      "  Avg Loss: 0.3766, Avg Pred Mean: 0.1405, Avg Target Mean: 0.3607\n",
      "Epoch 1/5, Training Loss: 0.3766\n",
      "Validation Loss: 0.2776\n",
      "New best model with validation loss: 0.2776\n",
      "Epoch 2/5, Batch 10/1672, Loss: 0.3199\n",
      "Epoch 2/5, Batch 20/1672, Loss: 0.2644\n",
      "Epoch 2/5, Batch 30/1672, Loss: 0.2691\n",
      "Epoch 2/5, Batch 40/1672, Loss: 0.2990\n",
      "Epoch 2/5, Batch 50/1672, Loss: 0.2704\n",
      "Epoch 2/5, Batch 60/1672, Loss: 0.2552\n",
      "Epoch 2/5, Batch 70/1672, Loss: 0.2976\n",
      "Epoch 2/5, Batch 80/1672, Loss: 0.3200\n",
      "Epoch 2/5, Batch 90/1672, Loss: 0.2607\n",
      "Epoch 2/5, Batch 100/1672, Loss: 0.2842\n",
      "Epoch 2/5, Batch 110/1672, Loss: 0.2884\n",
      "Epoch 2/5, Batch 120/1672, Loss: 0.3208\n",
      "Epoch 2/5, Batch 130/1672, Loss: 0.3413\n",
      "Epoch 2/5, Batch 140/1672, Loss: 0.2937\n",
      "Epoch 2/5, Batch 150/1672, Loss: 0.2954\n",
      "Epoch 2/5, Batch 160/1672, Loss: 0.3130\n",
      "Epoch 2/5, Batch 170/1672, Loss: 0.2806\n",
      "Epoch 2/5, Batch 180/1672, Loss: 0.3420\n",
      "Epoch 2/5, Batch 190/1672, Loss: 0.3026\n",
      "Epoch 2/5, Batch 200/1672, Loss: 0.3143\n",
      "Epoch 2/5, Batch 210/1672, Loss: 0.2662\n",
      "Epoch 2/5, Batch 220/1672, Loss: 0.2811\n",
      "Epoch 2/5, Batch 230/1672, Loss: 0.2545\n",
      "Epoch 2/5, Batch 240/1672, Loss: 0.3895\n",
      "Epoch 2/5, Batch 250/1672, Loss: 0.2866\n",
      "Epoch 2/5, Batch 260/1672, Loss: 0.2581\n",
      "Epoch 2/5, Batch 270/1672, Loss: 0.2532\n",
      "Epoch 2/5, Batch 280/1672, Loss: 0.2963\n",
      "Epoch 2/5, Batch 290/1672, Loss: 0.2456\n",
      "Epoch 2/5, Batch 300/1672, Loss: 0.3055\n",
      "Epoch 2/5, Batch 310/1672, Loss: 0.2425\n",
      "Epoch 2/5, Batch 320/1672, Loss: 0.2641\n",
      "Epoch 2/5, Batch 330/1672, Loss: 0.2603\n",
      "Epoch 2/5, Batch 340/1672, Loss: 0.2703\n",
      "Epoch 2/5, Batch 350/1672, Loss: 0.2569\n",
      "Epoch 2/5, Batch 360/1672, Loss: 0.2712\n",
      "Epoch 2/5, Batch 370/1672, Loss: 0.3044\n",
      "Epoch 2/5, Batch 380/1672, Loss: 0.2648\n",
      "Epoch 2/5, Batch 390/1672, Loss: 0.3148\n",
      "Epoch 2/5, Batch 400/1672, Loss: 0.2721\n",
      "Epoch 2/5, Batch 410/1672, Loss: 0.2666\n",
      "Epoch 2/5, Batch 420/1672, Loss: 0.2678\n",
      "Epoch 2/5, Batch 430/1672, Loss: 0.2489\n",
      "Epoch 2/5, Batch 440/1672, Loss: 0.2612\n",
      "Epoch 2/5, Batch 450/1672, Loss: 0.2922\n",
      "Epoch 2/5, Batch 460/1672, Loss: 0.2741\n",
      "Epoch 2/5, Batch 470/1672, Loss: 0.2931\n",
      "Epoch 2/5, Batch 480/1672, Loss: 0.2667\n",
      "Epoch 2/5, Batch 490/1672, Loss: 0.2376\n",
      "Epoch 2/5, Batch 500/1672, Loss: 0.2649\n",
      "Epoch 2/5, Batch 510/1672, Loss: 0.2386\n",
      "Epoch 2/5, Batch 520/1672, Loss: 0.2794\n",
      "Epoch 2/5, Batch 530/1672, Loss: 0.2585\n",
      "Epoch 2/5, Batch 540/1672, Loss: 0.2851\n",
      "Epoch 2/5, Batch 550/1672, Loss: 0.2648\n",
      "Epoch 2/5, Batch 560/1672, Loss: 0.2503\n",
      "Epoch 2/5, Batch 570/1672, Loss: 0.2371\n",
      "Epoch 2/5, Batch 580/1672, Loss: 0.2626\n",
      "Epoch 2/5, Batch 590/1672, Loss: 0.2331\n",
      "Epoch 2/5, Batch 600/1672, Loss: 0.2207\n",
      "Epoch 2/5, Batch 610/1672, Loss: 0.2682\n",
      "Epoch 2/5, Batch 620/1672, Loss: 0.2474\n",
      "Epoch 2/5, Batch 630/1672, Loss: 0.3002\n",
      "Epoch 2/5, Batch 640/1672, Loss: 0.2635\n",
      "Epoch 2/5, Batch 650/1672, Loss: 0.2578\n",
      "Epoch 2/5, Batch 660/1672, Loss: 0.2196\n",
      "Epoch 2/5, Batch 670/1672, Loss: 0.2377\n",
      "Epoch 2/5, Batch 680/1672, Loss: 0.2074\n",
      "Epoch 2/5, Batch 690/1672, Loss: 0.2237\n",
      "Epoch 2/5, Batch 700/1672, Loss: 0.2528\n",
      "Epoch 2/5, Batch 710/1672, Loss: 0.2058\n",
      "Epoch 2/5, Batch 720/1672, Loss: 0.2418\n",
      "Epoch 2/5, Batch 730/1672, Loss: 0.2562\n",
      "Epoch 2/5, Batch 740/1672, Loss: 0.2514\n",
      "Epoch 2/5, Batch 750/1672, Loss: 0.2398\n",
      "Epoch 2/5, Batch 760/1672, Loss: 0.2727\n",
      "Epoch 2/5, Batch 770/1672, Loss: 0.2663\n",
      "Epoch 2/5, Batch 780/1672, Loss: 0.2432\n",
      "Epoch 2/5, Batch 790/1672, Loss: 0.2353\n",
      "Epoch 2/5, Batch 800/1672, Loss: 0.2297\n",
      "Epoch 2/5, Batch 810/1672, Loss: 0.2830\n",
      "Epoch 2/5, Batch 820/1672, Loss: 0.2799\n",
      "Epoch 2/5, Batch 830/1672, Loss: 0.2172\n",
      "Epoch 2/5, Batch 840/1672, Loss: 0.2882\n",
      "Epoch 2/5, Batch 850/1672, Loss: 0.2227\n",
      "Epoch 2/5, Batch 860/1672, Loss: 0.2074\n",
      "Epoch 2/5, Batch 870/1672, Loss: 0.2562\n",
      "Epoch 2/5, Batch 880/1672, Loss: 0.2786\n",
      "Epoch 2/5, Batch 890/1672, Loss: 0.2455\n",
      "Epoch 2/5, Batch 900/1672, Loss: 0.2348\n",
      "Epoch 2/5, Batch 910/1672, Loss: 0.2292\n",
      "Epoch 2/5, Batch 920/1672, Loss: 0.2618\n",
      "Epoch 2/5, Batch 930/1672, Loss: 0.2348\n",
      "Epoch 2/5, Batch 940/1672, Loss: 0.2338\n",
      "Epoch 2/5, Batch 950/1672, Loss: 0.2373\n",
      "Epoch 2/5, Batch 960/1672, Loss: 0.2756\n",
      "Epoch 2/5, Batch 970/1672, Loss: 0.2642\n",
      "Epoch 2/5, Batch 980/1672, Loss: 0.2622\n",
      "Epoch 2/5, Batch 990/1672, Loss: 0.3097\n",
      "Epoch 2/5, Batch 1000/1672, Loss: 0.2162\n",
      "Epoch 2/5, Batch 1010/1672, Loss: 0.2708\n",
      "Epoch 2/5, Batch 1020/1672, Loss: 0.2763\n",
      "Epoch 2/5, Batch 1030/1672, Loss: 0.2547\n",
      "Epoch 2/5, Batch 1040/1672, Loss: 0.2818\n",
      "Epoch 2/5, Batch 1050/1672, Loss: 0.3158\n",
      "Epoch 2/5, Batch 1060/1672, Loss: 0.2775\n",
      "Epoch 2/5, Batch 1070/1672, Loss: 0.2808\n",
      "Epoch 2/5, Batch 1080/1672, Loss: 0.3364\n",
      "Epoch 2/5, Batch 1090/1672, Loss: 0.2582\n",
      "Epoch 2/5, Batch 1100/1672, Loss: 0.2873\n",
      "Epoch 2/5, Batch 1110/1672, Loss: 0.3017\n",
      "Epoch 2/5, Batch 1120/1672, Loss: 0.2962\n",
      "Epoch 2/5, Batch 1130/1672, Loss: 0.2565\n",
      "Epoch 2/5, Batch 1140/1672, Loss: 0.3209\n",
      "Epoch 2/5, Batch 1150/1672, Loss: 0.2547\n",
      "Epoch 2/5, Batch 1160/1672, Loss: 0.3315\n",
      "Epoch 2/5, Batch 1170/1672, Loss: 0.3273\n",
      "Epoch 2/5, Batch 1180/1672, Loss: 0.3269\n",
      "Epoch 2/5, Batch 1190/1672, Loss: 0.3428\n",
      "Epoch 2/5, Batch 1200/1672, Loss: 0.3552\n",
      "Epoch 2/5, Batch 1210/1672, Loss: 0.3125\n",
      "Epoch 2/5, Batch 1220/1672, Loss: 0.3451\n",
      "Epoch 2/5, Batch 1230/1672, Loss: 0.3001\n",
      "Epoch 2/5, Batch 1240/1672, Loss: 0.3774\n",
      "Epoch 2/5, Batch 1250/1672, Loss: 0.3847\n",
      "Epoch 2/5, Batch 1260/1672, Loss: 0.3098\n",
      "Epoch 2/5, Batch 1270/1672, Loss: 0.4187\n",
      "Epoch 2/5, Batch 1280/1672, Loss: 0.3808\n",
      "Epoch 2/5, Batch 1290/1672, Loss: 0.4017\n",
      "Epoch 2/5, Batch 1300/1672, Loss: 0.3697\n",
      "Epoch 2/5, Batch 1310/1672, Loss: 0.3777\n",
      "Epoch 2/5, Batch 1320/1672, Loss: 0.4707\n",
      "Epoch 2/5, Batch 1330/1672, Loss: 0.3593\n",
      "Epoch 2/5, Batch 1340/1672, Loss: 0.4316\n",
      "Epoch 2/5, Batch 1350/1672, Loss: 0.4241\n",
      "Epoch 2/5, Batch 1360/1672, Loss: 0.4059\n",
      "Epoch 2/5, Batch 1370/1672, Loss: 0.4593\n",
      "Epoch 2/5, Batch 1380/1672, Loss: 0.4566\n",
      "Epoch 2/5, Batch 1390/1672, Loss: 0.4688\n",
      "Epoch 2/5, Batch 1400/1672, Loss: 0.6570\n",
      "Epoch 2/5, Batch 1410/1672, Loss: 0.4910\n",
      "Epoch 2/5, Batch 1420/1672, Loss: 0.4041\n",
      "Epoch 2/5, Batch 1430/1672, Loss: 0.5133\n",
      "Epoch 2/5, Batch 1440/1672, Loss: 0.4927\n",
      "Epoch 2/5, Batch 1450/1672, Loss: 0.4964\n",
      "Epoch 2/5, Batch 1460/1672, Loss: 0.5286\n",
      "Epoch 2/5, Batch 1470/1672, Loss: 0.4688\n",
      "Epoch 2/5, Batch 1480/1672, Loss: 0.4967\n",
      "Epoch 2/5, Batch 1490/1672, Loss: 0.4984\n",
      "Epoch 2/5, Batch 1500/1672, Loss: 0.5561\n",
      "Epoch 2/5, Batch 1510/1672, Loss: 0.5931\n",
      "Epoch 2/5, Batch 1520/1672, Loss: 0.5372\n",
      "Epoch 2/5, Batch 1530/1672, Loss: 0.5225\n",
      "Epoch 2/5, Batch 1540/1672, Loss: 0.5178\n",
      "Epoch 2/5, Batch 1550/1672, Loss: 0.5462\n",
      "Epoch 2/5, Batch 1560/1672, Loss: 0.4746\n",
      "Epoch 2/5, Batch 1570/1672, Loss: 0.5501\n",
      "Epoch 2/5, Batch 1580/1672, Loss: 0.5383\n",
      "Epoch 2/5, Batch 1590/1672, Loss: 0.5664\n",
      "Epoch 2/5, Batch 1600/1672, Loss: 0.5548\n",
      "Epoch 2/5, Batch 1610/1672, Loss: 0.5893\n",
      "Epoch 2/5, Batch 1620/1672, Loss: 0.5621\n",
      "Epoch 2/5, Batch 1630/1672, Loss: 0.5465\n",
      "Epoch 2/5, Batch 1640/1672, Loss: 0.6003\n",
      "Epoch 2/5, Batch 1650/1672, Loss: 0.6541\n",
      "Epoch 2/5, Batch 1660/1672, Loss: 0.6070\n",
      "Epoch 2/5, Batch 1670/1672, Loss: 0.6011\n",
      "Warning: Feature 0 has 2 channels but expected 4. Reshaping...\n",
      "Warning: Feature 1 has 2 channels but expected 4. Reshaping...\n",
      "Warning: Feature 2 has 2 channels but expected 4. Reshaping...\n",
      "Warning: Feature 3 has 2 channels but expected 4. Reshaping...\n",
      "Batch statistics for epoch 2:\n",
      "  Avg Loss: 0.3268, Avg Pred Mean: 0.5057, Avg Target Mean: 0.3607\n",
      "Epoch 2/5, Training Loss: 0.3268\n",
      "Validation Loss: 0.6378\n",
      "Epoch 3/5, Batch 10/1672, Loss: 0.6505\n",
      "Epoch 3/5, Batch 20/1672, Loss: 0.6040\n",
      "Epoch 3/5, Batch 30/1672, Loss: 0.6726\n",
      "Epoch 3/5, Batch 40/1672, Loss: 0.6544\n",
      "Epoch 3/5, Batch 50/1672, Loss: 0.5894\n",
      "Epoch 3/5, Batch 60/1672, Loss: 0.5978\n",
      "Epoch 3/5, Batch 70/1672, Loss: 0.5659\n",
      "Epoch 3/5, Batch 80/1672, Loss: 0.6492\n",
      "Epoch 3/5, Batch 90/1672, Loss: 0.6050\n",
      "Epoch 3/5, Batch 100/1672, Loss: 0.5490\n",
      "Epoch 3/5, Batch 110/1672, Loss: 0.5971\n",
      "Epoch 3/5, Batch 120/1672, Loss: 0.6528\n",
      "Epoch 3/5, Batch 130/1672, Loss: 0.5928\n",
      "Epoch 3/5, Batch 140/1672, Loss: 0.6300\n",
      "Epoch 3/5, Batch 150/1672, Loss: 0.6349\n",
      "Epoch 3/5, Batch 160/1672, Loss: 0.6775\n",
      "Epoch 3/5, Batch 170/1672, Loss: 0.5907\n",
      "Epoch 3/5, Batch 180/1672, Loss: 0.6252\n",
      "Epoch 3/5, Batch 190/1672, Loss: 0.6478\n",
      "Epoch 3/5, Batch 200/1672, Loss: 0.6220\n",
      "Epoch 3/5, Batch 210/1672, Loss: 0.6302\n",
      "Epoch 3/5, Batch 220/1672, Loss: 0.6420\n",
      "Epoch 3/5, Batch 230/1672, Loss: 0.6571\n",
      "Epoch 3/5, Batch 240/1672, Loss: 0.6137\n",
      "Epoch 3/5, Batch 250/1672, Loss: 0.6374\n",
      "Epoch 3/5, Batch 260/1672, Loss: 0.6645\n",
      "Epoch 3/5, Batch 270/1672, Loss: 0.6946\n",
      "Epoch 3/5, Batch 280/1672, Loss: 0.6245\n",
      "Epoch 3/5, Batch 290/1672, Loss: 0.6297\n",
      "Epoch 3/5, Batch 300/1672, Loss: 0.6201\n",
      "Epoch 3/5, Batch 310/1672, Loss: 0.5868\n",
      "Epoch 3/5, Batch 320/1672, Loss: 0.6480\n",
      "Epoch 3/5, Batch 330/1672, Loss: 0.7034\n",
      "Epoch 3/5, Batch 340/1672, Loss: 0.6509\n",
      "Epoch 3/5, Batch 350/1672, Loss: 0.6664\n",
      "Epoch 3/5, Batch 360/1672, Loss: 0.6112\n",
      "Epoch 3/5, Batch 370/1672, Loss: 0.8275\n",
      "Epoch 3/5, Batch 380/1672, Loss: 0.6854\n",
      "Epoch 3/5, Batch 390/1672, Loss: 0.6753\n",
      "Epoch 3/5, Batch 400/1672, Loss: 0.6626\n",
      "Epoch 3/5, Batch 410/1672, Loss: 0.6778\n",
      "Epoch 3/5, Batch 420/1672, Loss: 0.6785\n",
      "Epoch 3/5, Batch 430/1672, Loss: 0.7270\n",
      "Epoch 3/5, Batch 440/1672, Loss: 0.6430\n",
      "Epoch 3/5, Batch 450/1672, Loss: 0.6793\n",
      "Epoch 3/5, Batch 460/1672, Loss: 0.6968\n",
      "Epoch 3/5, Batch 470/1672, Loss: 0.6412\n",
      "Epoch 3/5, Batch 480/1672, Loss: 0.6739\n",
      "Epoch 3/5, Batch 490/1672, Loss: 0.7775\n",
      "Epoch 3/5, Batch 500/1672, Loss: 0.7222\n",
      "Epoch 3/5, Batch 510/1672, Loss: 0.5722\n",
      "Epoch 3/5, Batch 520/1672, Loss: 0.6501\n",
      "Epoch 3/5, Batch 530/1672, Loss: 0.7161\n",
      "Epoch 3/5, Batch 540/1672, Loss: 0.6789\n",
      "Epoch 3/5, Batch 550/1672, Loss: 0.7193\n",
      "Epoch 3/5, Batch 560/1672, Loss: 0.6561\n",
      "Epoch 3/5, Batch 570/1672, Loss: 0.7367\n",
      "Epoch 3/5, Batch 580/1672, Loss: 0.6985\n",
      "Epoch 3/5, Batch 590/1672, Loss: 0.6958\n",
      "Epoch 3/5, Batch 600/1672, Loss: 0.6930\n",
      "Epoch 3/5, Batch 610/1672, Loss: 0.6975\n",
      "Epoch 3/5, Batch 620/1672, Loss: 0.7295\n",
      "Epoch 3/5, Batch 630/1672, Loss: 0.6997\n",
      "Epoch 3/5, Batch 640/1672, Loss: 0.7088\n",
      "Epoch 3/5, Batch 650/1672, Loss: 0.7139\n",
      "Epoch 3/5, Batch 660/1672, Loss: 0.7334\n",
      "Epoch 3/5, Batch 670/1672, Loss: 0.7024\n",
      "Epoch 3/5, Batch 680/1672, Loss: 0.6840\n",
      "Epoch 3/5, Batch 690/1672, Loss: 0.7131\n",
      "Epoch 3/5, Batch 700/1672, Loss: 0.6529\n",
      "Epoch 3/5, Batch 710/1672, Loss: 0.6902\n",
      "Epoch 3/5, Batch 720/1672, Loss: 0.7026\n",
      "Epoch 3/5, Batch 730/1672, Loss: 0.7024\n",
      "Epoch 3/5, Batch 740/1672, Loss: 0.7494\n",
      "Epoch 3/5, Batch 750/1672, Loss: 0.7415\n",
      "Epoch 3/5, Batch 760/1672, Loss: 0.7162\n",
      "Epoch 3/5, Batch 770/1672, Loss: 0.7240\n",
      "Epoch 3/5, Batch 780/1672, Loss: 0.7195\n",
      "Epoch 3/5, Batch 790/1672, Loss: 0.7144\n",
      "Epoch 3/5, Batch 800/1672, Loss: 0.7312\n",
      "Epoch 3/5, Batch 810/1672, Loss: 0.6230\n",
      "Epoch 3/5, Batch 820/1672, Loss: 0.6688\n",
      "Epoch 3/5, Batch 830/1672, Loss: 0.7213\n",
      "Epoch 3/5, Batch 840/1672, Loss: 0.7212\n",
      "Epoch 3/5, Batch 850/1672, Loss: 0.7822\n",
      "Epoch 3/5, Batch 860/1672, Loss: 0.6942\n",
      "Epoch 3/5, Batch 870/1672, Loss: 0.8038\n",
      "Epoch 3/5, Batch 880/1672, Loss: 0.7312\n",
      "Epoch 3/5, Batch 890/1672, Loss: 0.7377\n",
      "Epoch 3/5, Batch 900/1672, Loss: 0.6898\n",
      "Epoch 3/5, Batch 910/1672, Loss: 0.7195\n",
      "Epoch 3/5, Batch 920/1672, Loss: 0.7419\n",
      "Epoch 3/5, Batch 930/1672, Loss: 0.7004\n",
      "Epoch 3/5, Batch 940/1672, Loss: 0.6627\n",
      "Epoch 3/5, Batch 950/1672, Loss: 0.7406\n",
      "Epoch 3/5, Batch 960/1672, Loss: 0.7795\n",
      "Epoch 3/5, Batch 970/1672, Loss: 0.7601\n",
      "Epoch 3/5, Batch 980/1672, Loss: 0.7051\n",
      "Epoch 3/5, Batch 990/1672, Loss: 0.7947\n",
      "Epoch 3/5, Batch 1000/1672, Loss: 0.7279\n",
      "Epoch 3/5, Batch 1010/1672, Loss: 0.7248\n",
      "Epoch 3/5, Batch 1020/1672, Loss: 0.7258\n",
      "Epoch 3/5, Batch 1030/1672, Loss: 0.6134\n",
      "Epoch 3/5, Batch 1040/1672, Loss: 0.7760\n",
      "Epoch 3/5, Batch 1050/1672, Loss: 0.7763\n",
      "Epoch 3/5, Batch 1060/1672, Loss: 0.7562\n",
      "Epoch 3/5, Batch 1070/1672, Loss: 0.7494\n",
      "Epoch 3/5, Batch 1080/1672, Loss: 0.7515\n",
      "Epoch 3/5, Batch 1090/1672, Loss: 0.6502\n",
      "Epoch 3/5, Batch 1100/1672, Loss: 0.7122\n",
      "Epoch 3/5, Batch 1110/1672, Loss: 0.7905\n",
      "Epoch 3/5, Batch 1120/1672, Loss: 0.6809\n",
      "Epoch 3/5, Batch 1130/1672, Loss: 0.7307\n",
      "Epoch 3/5, Batch 1140/1672, Loss: 0.7209\n",
      "Epoch 3/5, Batch 1150/1672, Loss: 0.7986\n",
      "Epoch 3/5, Batch 1160/1672, Loss: 0.7700\n",
      "Epoch 3/5, Batch 1170/1672, Loss: 0.8112\n",
      "Epoch 3/5, Batch 1180/1672, Loss: 0.7162\n",
      "Epoch 3/5, Batch 1190/1672, Loss: 0.8058\n",
      "Epoch 3/5, Batch 1200/1672, Loss: 0.7942\n",
      "Epoch 3/5, Batch 1210/1672, Loss: 0.7368\n",
      "Epoch 3/5, Batch 1220/1672, Loss: 0.7788\n",
      "Epoch 3/5, Batch 1230/1672, Loss: 0.7839\n",
      "Epoch 3/5, Batch 1240/1672, Loss: 0.7475\n",
      "Epoch 3/5, Batch 1250/1672, Loss: 0.7575\n",
      "Epoch 3/5, Batch 1260/1672, Loss: 0.7054\n",
      "Epoch 3/5, Batch 1270/1672, Loss: 0.8018\n",
      "Epoch 3/5, Batch 1280/1672, Loss: 0.6847\n",
      "Epoch 3/5, Batch 1290/1672, Loss: 0.8375\n",
      "Epoch 3/5, Batch 1300/1672, Loss: 0.6901\n",
      "Epoch 3/5, Batch 1310/1672, Loss: 0.7734\n",
      "Epoch 3/5, Batch 1320/1672, Loss: 0.7988\n",
      "Epoch 3/5, Batch 1330/1672, Loss: 0.6934\n",
      "Epoch 3/5, Batch 1340/1672, Loss: 0.7280\n",
      "Epoch 3/5, Batch 1350/1672, Loss: 0.7503\n",
      "Epoch 3/5, Batch 1360/1672, Loss: 0.7786\n",
      "Epoch 3/5, Batch 1370/1672, Loss: 0.6781\n",
      "Epoch 3/5, Batch 1380/1672, Loss: 0.7789\n",
      "Epoch 3/5, Batch 1390/1672, Loss: 0.7339\n",
      "Epoch 3/5, Batch 1400/1672, Loss: 0.6815\n",
      "Epoch 3/5, Batch 1410/1672, Loss: 0.8286\n",
      "Epoch 3/5, Batch 1420/1672, Loss: 0.8227\n",
      "Epoch 3/5, Batch 1430/1672, Loss: 0.8123\n",
      "Epoch 3/5, Batch 1440/1672, Loss: 0.8003\n",
      "Epoch 3/5, Batch 1450/1672, Loss: 0.7352\n",
      "Epoch 3/5, Batch 1460/1672, Loss: 0.7413\n",
      "Epoch 3/5, Batch 1470/1672, Loss: 0.7611\n",
      "Epoch 3/5, Batch 1480/1672, Loss: 0.8221\n",
      "Epoch 3/5, Batch 1490/1672, Loss: 0.7785\n",
      "Epoch 3/5, Batch 1500/1672, Loss: 0.7517\n",
      "Epoch 3/5, Batch 1510/1672, Loss: 0.7965\n",
      "Epoch 3/5, Batch 1520/1672, Loss: 0.8164\n",
      "Epoch 3/5, Batch 1530/1672, Loss: 0.7356\n",
      "Epoch 3/5, Batch 1540/1672, Loss: 0.7992\n",
      "Epoch 3/5, Batch 1550/1672, Loss: 0.7412\n",
      "Epoch 3/5, Batch 1560/1672, Loss: 0.8245\n",
      "Epoch 3/5, Batch 1570/1672, Loss: 0.8175\n",
      "Epoch 3/5, Batch 1580/1672, Loss: 0.7182\n",
      "Epoch 3/5, Batch 1590/1672, Loss: 0.7840\n",
      "Epoch 3/5, Batch 1600/1672, Loss: 0.7697\n",
      "Epoch 3/5, Batch 1610/1672, Loss: 0.7550\n",
      "Epoch 3/5, Batch 1620/1672, Loss: 0.7256\n",
      "Epoch 3/5, Batch 1630/1672, Loss: 0.8593\n",
      "Epoch 3/5, Batch 1640/1672, Loss: 0.7069\n",
      "Epoch 3/5, Batch 1650/1672, Loss: 0.7291\n",
      "Epoch 3/5, Batch 1660/1672, Loss: 0.7961\n",
      "Epoch 3/5, Batch 1670/1672, Loss: 0.8359\n",
      "Warning: Feature 0 has 2 channels but expected 4. Reshaping...\n",
      "Warning: Feature 1 has 2 channels but expected 4. Reshaping...\n",
      "Warning: Feature 2 has 2 channels but expected 4. Reshaping...\n",
      "Warning: Feature 3 has 2 channels but expected 4. Reshaping...\n",
      "Batch statistics for epoch 3:\n",
      "  Avg Loss: 0.7113, Avg Pred Mean: 1.5199, Avg Target Mean: 0.3607\n",
      "Epoch 3/5, Training Loss: 0.7113\n",
      "Validation Loss: 0.7900\n",
      "Epoch 4/5, Batch 10/1672, Loss: 0.7212\n",
      "Epoch 4/5, Batch 20/1672, Loss: 0.7514\n",
      "Epoch 4/5, Batch 30/1672, Loss: 0.7291\n",
      "Epoch 4/5, Batch 40/1672, Loss: 0.7429\n",
      "Epoch 4/5, Batch 50/1672, Loss: 0.7665\n",
      "Epoch 4/5, Batch 60/1672, Loss: 0.7871\n",
      "Epoch 4/5, Batch 70/1672, Loss: 0.6926\n",
      "Epoch 4/5, Batch 80/1672, Loss: 0.7831\n",
      "Epoch 4/5, Batch 90/1672, Loss: 0.8388\n",
      "Epoch 4/5, Batch 100/1672, Loss: 0.7632\n",
      "Epoch 4/5, Batch 110/1672, Loss: 0.7480\n",
      "Epoch 4/5, Batch 120/1672, Loss: 0.8083\n",
      "Epoch 4/5, Batch 130/1672, Loss: 0.7938\n",
      "Epoch 4/5, Batch 140/1672, Loss: 0.8425\n",
      "Epoch 4/5, Batch 150/1672, Loss: 0.7515\n",
      "Epoch 4/5, Batch 160/1672, Loss: 0.7699\n",
      "Epoch 4/5, Batch 170/1672, Loss: 0.7200\n",
      "Epoch 4/5, Batch 180/1672, Loss: 0.7924\n",
      "Epoch 4/5, Batch 190/1672, Loss: 0.7889\n",
      "Epoch 4/5, Batch 200/1672, Loss: 0.7370\n",
      "Epoch 4/5, Batch 210/1672, Loss: 0.7203\n",
      "Epoch 4/5, Batch 220/1672, Loss: 0.7381\n",
      "Epoch 4/5, Batch 230/1672, Loss: 0.7629\n",
      "Epoch 4/5, Batch 240/1672, Loss: 0.7350\n",
      "Epoch 4/5, Batch 250/1672, Loss: 0.7675\n",
      "Epoch 4/5, Batch 260/1672, Loss: 0.7671\n",
      "Epoch 4/5, Batch 270/1672, Loss: 0.6952\n",
      "Epoch 4/5, Batch 280/1672, Loss: 0.7780\n",
      "Epoch 4/5, Batch 290/1672, Loss: 0.8368\n",
      "Epoch 4/5, Batch 300/1672, Loss: 0.7454\n",
      "Epoch 4/5, Batch 310/1672, Loss: 0.6909\n",
      "Epoch 4/5, Batch 320/1672, Loss: 0.8259\n",
      "Epoch 4/5, Batch 330/1672, Loss: 0.7483\n",
      "Epoch 4/5, Batch 340/1672, Loss: 0.7802\n",
      "Epoch 4/5, Batch 350/1672, Loss: 0.7758\n",
      "Epoch 4/5, Batch 360/1672, Loss: 0.8160\n",
      "Epoch 4/5, Batch 370/1672, Loss: 0.8126\n",
      "Epoch 4/5, Batch 380/1672, Loss: 0.7457\n",
      "Epoch 4/5, Batch 390/1672, Loss: 0.7954\n",
      "Epoch 4/5, Batch 400/1672, Loss: 0.8025\n",
      "Epoch 4/5, Batch 410/1672, Loss: 0.7916\n",
      "Epoch 4/5, Batch 420/1672, Loss: 0.7919\n",
      "Epoch 4/5, Batch 430/1672, Loss: 0.7475\n",
      "Epoch 4/5, Batch 440/1672, Loss: 0.7424\n",
      "Epoch 4/5, Batch 450/1672, Loss: 0.7598\n",
      "Epoch 4/5, Batch 460/1672, Loss: 0.6985\n",
      "Epoch 4/5, Batch 470/1672, Loss: 0.7704\n",
      "Epoch 4/5, Batch 480/1672, Loss: 0.7958\n",
      "Epoch 4/5, Batch 490/1672, Loss: 0.7006\n",
      "Epoch 4/5, Batch 500/1672, Loss: 0.7952\n",
      "Epoch 4/5, Batch 510/1672, Loss: 0.7443\n",
      "Epoch 4/5, Batch 520/1672, Loss: 0.7809\n",
      "Epoch 4/5, Batch 530/1672, Loss: 0.7743\n",
      "Epoch 4/5, Batch 540/1672, Loss: 0.8007\n",
      "Epoch 4/5, Batch 550/1672, Loss: 0.7454\n",
      "Epoch 4/5, Batch 560/1672, Loss: 0.7281\n",
      "Epoch 4/5, Batch 570/1672, Loss: 0.7031\n",
      "Epoch 4/5, Batch 580/1672, Loss: 0.8381\n",
      "Epoch 4/5, Batch 590/1672, Loss: 0.8099\n",
      "Epoch 4/5, Batch 600/1672, Loss: 0.7559\n",
      "Epoch 4/5, Batch 610/1672, Loss: 0.8660\n",
      "Epoch 4/5, Batch 620/1672, Loss: 0.8206\n",
      "Epoch 4/5, Batch 630/1672, Loss: 0.7983\n",
      "Epoch 4/5, Batch 640/1672, Loss: 0.7256\n",
      "Epoch 4/5, Batch 650/1672, Loss: 0.6730\n",
      "Epoch 4/5, Batch 660/1672, Loss: 0.7817\n",
      "Epoch 4/5, Batch 670/1672, Loss: 0.8037\n",
      "Epoch 4/5, Batch 680/1672, Loss: 0.8793\n",
      "Epoch 4/5, Batch 690/1672, Loss: 0.8105\n",
      "Epoch 4/5, Batch 700/1672, Loss: 0.7931\n",
      "Epoch 4/5, Batch 710/1672, Loss: 0.7231\n",
      "Epoch 4/5, Batch 720/1672, Loss: 0.7432\n",
      "Epoch 4/5, Batch 730/1672, Loss: 0.7347\n",
      "Epoch 4/5, Batch 740/1672, Loss: 0.7916\n",
      "Epoch 4/5, Batch 750/1672, Loss: 0.7417\n",
      "Epoch 4/5, Batch 760/1672, Loss: 0.7505\n",
      "Epoch 4/5, Batch 770/1672, Loss: 0.7887\n",
      "Epoch 4/5, Batch 780/1672, Loss: 0.7688\n",
      "Epoch 4/5, Batch 790/1672, Loss: 0.8264\n",
      "Epoch 4/5, Batch 800/1672, Loss: 0.7503\n",
      "Epoch 4/5, Batch 810/1672, Loss: 0.7187\n",
      "Epoch 4/5, Batch 820/1672, Loss: 0.8393\n",
      "Epoch 4/5, Batch 830/1672, Loss: 0.7789\n",
      "Epoch 4/5, Batch 840/1672, Loss: 0.6780\n",
      "Epoch 4/5, Batch 850/1672, Loss: 0.8406\n",
      "Epoch 4/5, Batch 860/1672, Loss: 0.7007\n",
      "Epoch 4/5, Batch 870/1672, Loss: 0.7852\n",
      "Epoch 4/5, Batch 880/1672, Loss: 0.7615\n",
      "Epoch 4/5, Batch 890/1672, Loss: 0.7587\n",
      "Epoch 4/5, Batch 900/1672, Loss: 0.7204\n",
      "Epoch 4/5, Batch 910/1672, Loss: 0.7984\n",
      "Epoch 4/5, Batch 920/1672, Loss: 0.7077\n",
      "Epoch 4/5, Batch 930/1672, Loss: 0.8450\n",
      "Epoch 4/5, Batch 940/1672, Loss: 0.7489\n",
      "Epoch 4/5, Batch 950/1672, Loss: 0.8036\n",
      "Epoch 4/5, Batch 960/1672, Loss: 0.7534\n",
      "Epoch 4/5, Batch 970/1672, Loss: 0.7472\n",
      "Epoch 4/5, Batch 980/1672, Loss: 0.7308\n",
      "Epoch 4/5, Batch 990/1672, Loss: 0.8597\n",
      "Epoch 4/5, Batch 1000/1672, Loss: 0.7801\n",
      "Epoch 4/5, Batch 1010/1672, Loss: 0.8101\n",
      "Epoch 4/5, Batch 1020/1672, Loss: 0.8089\n",
      "Epoch 4/5, Batch 1030/1672, Loss: 0.7504\n",
      "Epoch 4/5, Batch 1040/1672, Loss: 0.7722\n",
      "Epoch 4/5, Batch 1050/1672, Loss: 0.7429\n",
      "Epoch 4/5, Batch 1060/1672, Loss: 0.7607\n",
      "Epoch 4/5, Batch 1070/1672, Loss: 0.8037\n",
      "Epoch 4/5, Batch 1080/1672, Loss: 0.7480\n",
      "Epoch 4/5, Batch 1090/1672, Loss: 0.7405\n",
      "Epoch 4/5, Batch 1100/1672, Loss: 0.7736\n",
      "Epoch 4/5, Batch 1110/1672, Loss: 0.7778\n",
      "Epoch 4/5, Batch 1120/1672, Loss: 0.8246\n",
      "Epoch 4/5, Batch 1130/1672, Loss: 0.7906\n",
      "Epoch 4/5, Batch 1140/1672, Loss: 0.7249\n",
      "Epoch 4/5, Batch 1150/1672, Loss: 0.7729\n",
      "Epoch 4/5, Batch 1160/1672, Loss: 0.8079\n",
      "Epoch 4/5, Batch 1170/1672, Loss: 0.8200\n",
      "Epoch 4/5, Batch 1180/1672, Loss: 0.7820\n",
      "Epoch 4/5, Batch 1190/1672, Loss: 0.7733\n",
      "Epoch 4/5, Batch 1200/1672, Loss: 0.8533\n",
      "Epoch 4/5, Batch 1210/1672, Loss: 0.8301\n",
      "Epoch 4/5, Batch 1220/1672, Loss: 0.7489\n",
      "Epoch 4/5, Batch 1230/1672, Loss: 0.7692\n",
      "Epoch 4/5, Batch 1240/1672, Loss: 0.7589\n",
      "Epoch 4/5, Batch 1250/1672, Loss: 0.8008\n",
      "Epoch 4/5, Batch 1260/1672, Loss: 0.7610\n",
      "Epoch 4/5, Batch 1270/1672, Loss: 0.7835\n",
      "Epoch 4/5, Batch 1280/1672, Loss: 0.8247\n",
      "Epoch 4/5, Batch 1290/1672, Loss: 0.8089\n",
      "Epoch 4/5, Batch 1300/1672, Loss: 0.7818\n",
      "Epoch 4/5, Batch 1310/1672, Loss: 0.7558\n",
      "Epoch 4/5, Batch 1320/1672, Loss: 0.6271\n",
      "Epoch 4/5, Batch 1330/1672, Loss: 0.7496\n",
      "Epoch 4/5, Batch 1340/1672, Loss: 0.7455\n",
      "Epoch 4/5, Batch 1350/1672, Loss: 0.6909\n",
      "Epoch 4/5, Batch 1360/1672, Loss: 0.7371\n",
      "Epoch 4/5, Batch 1370/1672, Loss: 0.8469\n",
      "Epoch 4/5, Batch 1380/1672, Loss: 0.6866\n",
      "Epoch 4/5, Batch 1390/1672, Loss: 0.7214\n",
      "Epoch 4/5, Batch 1400/1672, Loss: 0.8214\n",
      "Epoch 4/5, Batch 1410/1672, Loss: 0.7997\n",
      "Epoch 4/5, Batch 1420/1672, Loss: 0.7760\n",
      "Epoch 4/5, Batch 1430/1672, Loss: 0.8257\n",
      "Epoch 4/5, Batch 1440/1672, Loss: 0.7538\n",
      "Epoch 4/5, Batch 1450/1672, Loss: 0.8074\n",
      "Epoch 4/5, Batch 1460/1672, Loss: 0.8005\n",
      "Epoch 4/5, Batch 1470/1672, Loss: 0.8270\n",
      "Epoch 4/5, Batch 1480/1672, Loss: 0.8045\n",
      "Epoch 4/5, Batch 1490/1672, Loss: 0.8221\n",
      "Epoch 4/5, Batch 1500/1672, Loss: 0.7600\n",
      "Epoch 4/5, Batch 1510/1672, Loss: 0.6825\n",
      "Epoch 4/5, Batch 1520/1672, Loss: 0.7435\n",
      "Epoch 4/5, Batch 1530/1672, Loss: 0.7951\n",
      "Epoch 4/5, Batch 1540/1672, Loss: 0.7379\n",
      "Epoch 4/5, Batch 1550/1672, Loss: 0.8364\n",
      "Epoch 4/5, Batch 1560/1672, Loss: 0.7453\n",
      "Epoch 4/5, Batch 1570/1672, Loss: 0.7559\n",
      "Epoch 4/5, Batch 1580/1672, Loss: 0.8834\n",
      "Epoch 4/5, Batch 1590/1672, Loss: 0.7857\n",
      "Epoch 4/5, Batch 1600/1672, Loss: 0.7790\n",
      "Epoch 4/5, Batch 1610/1672, Loss: 0.7789\n",
      "Epoch 4/5, Batch 1620/1672, Loss: 0.7495\n",
      "Epoch 4/5, Batch 1630/1672, Loss: 0.7405\n",
      "Epoch 4/5, Batch 1640/1672, Loss: 0.7403\n",
      "Epoch 4/5, Batch 1650/1672, Loss: 0.7708\n",
      "Epoch 4/5, Batch 1660/1672, Loss: 0.9728\n",
      "Epoch 4/5, Batch 1670/1672, Loss: 0.7933\n",
      "Warning: Feature 0 has 2 channels but expected 4. Reshaping...\n",
      "Warning: Feature 1 has 2 channels but expected 4. Reshaping...\n",
      "Warning: Feature 2 has 2 channels but expected 4. Reshaping...\n",
      "Warning: Feature 3 has 2 channels but expected 4. Reshaping...\n",
      "Batch statistics for epoch 4:\n",
      "  Avg Loss: 0.7716, Avg Pred Mean: 1.6388, Avg Target Mean: 0.3607\n",
      "Epoch 4/5, Training Loss: 0.7716\n",
      "Validation Loss: 0.8035\n",
      "Epoch 5/5, Batch 10/1672, Loss: 0.7686\n",
      "Epoch 5/5, Batch 20/1672, Loss: 0.8321\n",
      "Epoch 5/5, Batch 30/1672, Loss: 0.8013\n",
      "Epoch 5/5, Batch 40/1672, Loss: 0.7661\n",
      "Epoch 5/5, Batch 50/1672, Loss: 0.7640\n",
      "Epoch 5/5, Batch 60/1672, Loss: 0.8340\n",
      "Epoch 5/5, Batch 70/1672, Loss: 0.7848\n",
      "Epoch 5/5, Batch 80/1672, Loss: 0.7602\n",
      "Epoch 5/5, Batch 90/1672, Loss: 0.8021\n",
      "Epoch 5/5, Batch 100/1672, Loss: 0.8133\n",
      "Epoch 5/5, Batch 110/1672, Loss: 0.7400\n",
      "Epoch 5/5, Batch 120/1672, Loss: 0.8465\n",
      "Epoch 5/5, Batch 130/1672, Loss: 0.8184\n",
      "Epoch 5/5, Batch 140/1672, Loss: 0.7822\n",
      "Epoch 5/5, Batch 150/1672, Loss: 0.7569\n",
      "Epoch 5/5, Batch 160/1672, Loss: 0.7651\n",
      "Epoch 5/5, Batch 170/1672, Loss: 0.7801\n",
      "Epoch 5/5, Batch 180/1672, Loss: 0.7952\n",
      "Epoch 5/5, Batch 190/1672, Loss: 0.7409\n",
      "Epoch 5/5, Batch 200/1672, Loss: 0.8229\n",
      "Epoch 5/5, Batch 210/1672, Loss: 0.7985\n",
      "Epoch 5/5, Batch 220/1672, Loss: 0.8668\n",
      "Epoch 5/5, Batch 230/1672, Loss: 0.7867\n",
      "Epoch 5/5, Batch 240/1672, Loss: 0.8462\n",
      "Epoch 5/5, Batch 250/1672, Loss: 0.7915\n",
      "Epoch 5/5, Batch 260/1672, Loss: 0.7743\n",
      "Epoch 5/5, Batch 270/1672, Loss: 0.7904\n",
      "Epoch 5/5, Batch 280/1672, Loss: 0.7441\n",
      "Epoch 5/5, Batch 290/1672, Loss: 0.7521\n",
      "Epoch 5/5, Batch 300/1672, Loss: 0.7551\n",
      "Epoch 5/5, Batch 310/1672, Loss: 0.7498\n",
      "Epoch 5/5, Batch 320/1672, Loss: 0.7645\n",
      "Epoch 5/5, Batch 330/1672, Loss: 0.7826\n",
      "Epoch 5/5, Batch 340/1672, Loss: 0.7798\n",
      "Epoch 5/5, Batch 350/1672, Loss: 0.7298\n",
      "Epoch 5/5, Batch 360/1672, Loss: 0.7961\n",
      "Epoch 5/5, Batch 370/1672, Loss: 0.8053\n",
      "Epoch 5/5, Batch 380/1672, Loss: 0.8356\n",
      "Epoch 5/5, Batch 390/1672, Loss: 0.7629\n",
      "Epoch 5/5, Batch 400/1672, Loss: 0.7870\n",
      "Epoch 5/5, Batch 410/1672, Loss: 0.7952\n",
      "Epoch 5/5, Batch 420/1672, Loss: 0.7980\n",
      "Epoch 5/5, Batch 430/1672, Loss: 0.7322\n",
      "Epoch 5/5, Batch 440/1672, Loss: 0.7333\n",
      "Epoch 5/5, Batch 450/1672, Loss: 0.8020\n",
      "Epoch 5/5, Batch 460/1672, Loss: 0.7437\n",
      "Epoch 5/5, Batch 470/1672, Loss: 0.6944\n",
      "Epoch 5/5, Batch 480/1672, Loss: 0.7452\n",
      "Epoch 5/5, Batch 490/1672, Loss: 0.6412\n",
      "Epoch 5/5, Batch 500/1672, Loss: 0.7662\n",
      "Epoch 5/5, Batch 510/1672, Loss: 0.7567\n",
      "Epoch 5/5, Batch 520/1672, Loss: 0.7322\n",
      "Epoch 5/5, Batch 530/1672, Loss: 0.7738\n",
      "Epoch 5/5, Batch 540/1672, Loss: 0.7591\n",
      "Epoch 5/5, Batch 550/1672, Loss: 0.7773\n",
      "Epoch 5/5, Batch 560/1672, Loss: 0.7991\n",
      "Epoch 5/5, Batch 570/1672, Loss: 0.8095\n",
      "Epoch 5/5, Batch 580/1672, Loss: 0.7891\n",
      "Epoch 5/5, Batch 590/1672, Loss: 0.7586\n",
      "Epoch 5/5, Batch 600/1672, Loss: 0.7158\n",
      "Epoch 5/5, Batch 610/1672, Loss: 0.7522\n",
      "Epoch 5/5, Batch 620/1672, Loss: 0.8258\n",
      "Epoch 5/5, Batch 630/1672, Loss: 0.8168\n",
      "Epoch 5/5, Batch 640/1672, Loss: 0.6981\n",
      "Epoch 5/5, Batch 650/1672, Loss: 0.7531\n",
      "Epoch 5/5, Batch 660/1672, Loss: 0.8280\n",
      "Epoch 5/5, Batch 670/1672, Loss: 0.8127\n",
      "Epoch 5/5, Batch 680/1672, Loss: 0.7652\n",
      "Epoch 5/5, Batch 690/1672, Loss: 0.7843\n",
      "Epoch 5/5, Batch 700/1672, Loss: 0.7672\n",
      "Epoch 5/5, Batch 710/1672, Loss: 0.7975\n",
      "Epoch 5/5, Batch 720/1672, Loss: 0.8279\n",
      "Epoch 5/5, Batch 730/1672, Loss: 0.7830\n",
      "Epoch 5/5, Batch 740/1672, Loss: 0.7946\n",
      "Epoch 5/5, Batch 750/1672, Loss: 0.8155\n",
      "Epoch 5/5, Batch 760/1672, Loss: 0.7322\n",
      "Epoch 5/5, Batch 770/1672, Loss: 0.8154\n",
      "Epoch 5/5, Batch 780/1672, Loss: 0.7714\n",
      "Epoch 5/5, Batch 790/1672, Loss: 0.7383\n",
      "Epoch 5/5, Batch 800/1672, Loss: 0.7753\n",
      "Epoch 5/5, Batch 810/1672, Loss: 0.7662\n",
      "Epoch 5/5, Batch 820/1672, Loss: 0.7544\n",
      "Epoch 5/5, Batch 830/1672, Loss: 0.7884\n",
      "Epoch 5/5, Batch 840/1672, Loss: 0.7676\n",
      "Epoch 5/5, Batch 850/1672, Loss: 0.7730\n",
      "Epoch 5/5, Batch 860/1672, Loss: 0.7552\n",
      "Epoch 5/5, Batch 870/1672, Loss: 0.7653\n",
      "Epoch 5/5, Batch 880/1672, Loss: 0.7816\n",
      "Epoch 5/5, Batch 890/1672, Loss: 0.8292\n",
      "Epoch 5/5, Batch 900/1672, Loss: 0.8395\n",
      "Epoch 5/5, Batch 910/1672, Loss: 0.7794\n",
      "Epoch 5/5, Batch 920/1672, Loss: 0.7696\n",
      "Epoch 5/5, Batch 930/1672, Loss: 0.7140\n",
      "Epoch 5/5, Batch 940/1672, Loss: 0.7793\n",
      "Epoch 5/5, Batch 950/1672, Loss: 0.7119\n",
      "Epoch 5/5, Batch 960/1672, Loss: 0.7685\n",
      "Epoch 5/5, Batch 970/1672, Loss: 0.6934\n",
      "Epoch 5/5, Batch 980/1672, Loss: 0.8334\n",
      "Epoch 5/5, Batch 990/1672, Loss: 0.7949\n",
      "Epoch 5/5, Batch 1000/1672, Loss: 0.7543\n",
      "Epoch 5/5, Batch 1010/1672, Loss: 0.7790\n",
      "Epoch 5/5, Batch 1020/1672, Loss: 0.7655\n",
      "Epoch 5/5, Batch 1030/1672, Loss: 0.7978\n",
      "Epoch 5/5, Batch 1040/1672, Loss: 0.8472\n",
      "Epoch 5/5, Batch 1050/1672, Loss: 0.8330\n",
      "Epoch 5/5, Batch 1060/1672, Loss: 0.7737\n",
      "Epoch 5/5, Batch 1070/1672, Loss: 0.7416\n",
      "Epoch 5/5, Batch 1080/1672, Loss: 0.8630\n",
      "Epoch 5/5, Batch 1090/1672, Loss: 0.8083\n",
      "Epoch 5/5, Batch 1100/1672, Loss: 0.8266\n",
      "Epoch 5/5, Batch 1110/1672, Loss: 0.7814\n",
      "Epoch 5/5, Batch 1120/1672, Loss: 0.7843\n",
      "Epoch 5/5, Batch 1130/1672, Loss: 0.8558\n",
      "Epoch 5/5, Batch 1140/1672, Loss: 0.7383\n",
      "Epoch 5/5, Batch 1150/1672, Loss: 0.7741\n",
      "Epoch 5/5, Batch 1160/1672, Loss: 0.7804\n",
      "Epoch 5/5, Batch 1170/1672, Loss: 0.7568\n",
      "Epoch 5/5, Batch 1180/1672, Loss: 0.7472\n",
      "Epoch 5/5, Batch 1190/1672, Loss: 0.7257\n",
      "Epoch 5/5, Batch 1200/1672, Loss: 0.7767\n",
      "Epoch 5/5, Batch 1210/1672, Loss: 0.8939\n",
      "Epoch 5/5, Batch 1220/1672, Loss: 0.8031\n",
      "Epoch 5/5, Batch 1230/1672, Loss: 0.7081\n",
      "Epoch 5/5, Batch 1240/1672, Loss: 0.8280\n",
      "Epoch 5/5, Batch 1250/1672, Loss: 0.7213\n",
      "Epoch 5/5, Batch 1260/1672, Loss: 0.7075\n",
      "Epoch 5/5, Batch 1270/1672, Loss: 0.7635\n",
      "Epoch 5/5, Batch 1280/1672, Loss: 0.7914\n",
      "Epoch 5/5, Batch 1290/1672, Loss: 0.8029\n",
      "Epoch 5/5, Batch 1300/1672, Loss: 0.8577\n",
      "Epoch 5/5, Batch 1310/1672, Loss: 0.7903\n",
      "Epoch 5/5, Batch 1320/1672, Loss: 0.7779\n",
      "Epoch 5/5, Batch 1330/1672, Loss: 0.7508\n",
      "Epoch 5/5, Batch 1340/1672, Loss: 0.7960\n",
      "Epoch 5/5, Batch 1350/1672, Loss: 0.8216\n",
      "Epoch 5/5, Batch 1360/1672, Loss: 0.7911\n",
      "Epoch 5/5, Batch 1370/1672, Loss: 0.7635\n",
      "Epoch 5/5, Batch 1380/1672, Loss: 0.7391\n",
      "Epoch 5/5, Batch 1390/1672, Loss: 0.7797\n",
      "Epoch 5/5, Batch 1400/1672, Loss: 0.7529\n",
      "Epoch 5/5, Batch 1410/1672, Loss: 0.7862\n",
      "Epoch 5/5, Batch 1420/1672, Loss: 0.7333\n",
      "Epoch 5/5, Batch 1430/1672, Loss: 0.8273\n",
      "Epoch 5/5, Batch 1440/1672, Loss: 0.7362\n",
      "Epoch 5/5, Batch 1450/1672, Loss: 0.8366\n",
      "Epoch 5/5, Batch 1460/1672, Loss: 0.8320\n",
      "Epoch 5/5, Batch 1470/1672, Loss: 0.8370\n",
      "Epoch 5/5, Batch 1480/1672, Loss: 0.7666\n",
      "Epoch 5/5, Batch 1490/1672, Loss: 0.7754\n",
      "Epoch 5/5, Batch 1500/1672, Loss: 0.7523\n",
      "Epoch 5/5, Batch 1510/1672, Loss: 0.8082\n",
      "Epoch 5/5, Batch 1520/1672, Loss: 0.8064\n",
      "Epoch 5/5, Batch 1530/1672, Loss: 0.7867\n",
      "Epoch 5/5, Batch 1540/1672, Loss: 0.7228\n",
      "Epoch 5/5, Batch 1550/1672, Loss: 0.7501\n",
      "Epoch 5/5, Batch 1560/1672, Loss: 0.7444\n",
      "Epoch 5/5, Batch 1570/1672, Loss: 0.8280\n",
      "Epoch 5/5, Batch 1580/1672, Loss: 0.7713\n",
      "Epoch 5/5, Batch 1590/1672, Loss: 0.9009\n",
      "Epoch 5/5, Batch 1600/1672, Loss: 0.8641\n",
      "Epoch 5/5, Batch 1610/1672, Loss: 0.8128\n",
      "Epoch 5/5, Batch 1620/1672, Loss: 0.7792\n",
      "Epoch 5/5, Batch 1630/1672, Loss: 0.7283\n",
      "Epoch 5/5, Batch 1640/1672, Loss: 0.7949\n",
      "Epoch 5/5, Batch 1650/1672, Loss: 0.7877\n",
      "Epoch 5/5, Batch 1660/1672, Loss: 0.7518\n",
      "Epoch 5/5, Batch 1670/1672, Loss: 0.7685\n",
      "Warning: Feature 0 has 2 channels but expected 4. Reshaping...\n",
      "Warning: Feature 1 has 2 channels but expected 4. Reshaping...\n",
      "Warning: Feature 2 has 2 channels but expected 4. Reshaping...\n",
      "Warning: Feature 3 has 2 channels but expected 4. Reshaping...\n",
      "Batch statistics for epoch 5:\n",
      "  Avg Loss: 0.7769, Avg Pred Mean: 1.6493, Avg Target Mean: 0.3607\n",
      "Epoch 5/5, Training Loss: 0.7769\n",
      "Validation Loss: 0.7989\n",
      "Training completed!\n",
      "Model saved to fine_tuned_dpt_refined\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from pytorch_msssim import ssim\n",
    "from transformers import Dinov2Config, DPTConfig, DPTForDepthEstimation\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import torch_optimizer as topt  # Ranger from torch_optimizer\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION & PATHS\n",
    "# ============================================================\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "TRAIN_RGB_DIR = \"/kaggle/input/depth-estimation/competition-data/competition-data/training/images\"\n",
    "TRAIN_DEPTH_DIR = \"/kaggle/input/depth-estimation/competition-data/competition-data/training/depths\"\n",
    "VAL_RGB_DIR = \"/kaggle/input/depth-estimation/competition-data/competition-data/validation/images\"\n",
    "VAL_DEPTH_DIR = \"/kaggle/input/depth-estimation/competition-data/competition-data/validation/depths\"\n",
    "TEST_RGB_DIR = \"/kaggle/input/depth-estimation/competition-data/competition-data/testing/images\"\n",
    "PREDICTIONS_FOLDER = \"predictions\"\n",
    "PREDICTIONS_CSV = \"predictions.csv\"\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 5\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "INIT_LR = 1e-4\n",
    "MAX_GRAD_NORM = 1.0  # For gradient clipping\n",
    "\n",
    "# Loss hyperparameters (tune these based on validation)\n",
    "L1_WEIGHT = 0.5\n",
    "SSIM_WEIGHT = 0.3\n",
    "EDGE_WEIGHT = 0.2\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# DATA AUGMENTATION & PREPROCESSING WITH CONSISTENCY CHECKS\n",
    "# ============================================================\n",
    "def histogram_equalization(image, **kwargs):\n",
    "    # Convert to YUV and equalize the Y channel\n",
    "    img_yuv = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
    "    img_yuv[:, :, 0] = cv2.equalizeHist(img_yuv[:, :, 0])\n",
    "    return cv2.cvtColor(img_yuv, cv2.COLOR_YUV2RGB)\n",
    "\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(384, 384),\n",
    "    A.Lambda(image=histogram_equalization),\n",
    "    A.RandomBrightnessContrast(p=0.7),\n",
    "    A.MotionBlur(blur_limit=7, p=0.5),\n",
    "    A.RandomGamma(gamma_limit=(70, 130), p=0.5),\n",
    "    A.Rotate(limit=15, p=0.5),\n",
    "    A.RandomScale(scale_limit=0.2, p=0.5),\n",
    "    A.Perspective(scale=(0.05, 0.1), p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(384, 384),\n",
    "    A.Lambda(image=histogram_equalization),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "\n",
    "def adaptive_depth_normalize(depth):\n",
    "    \"\"\"Normalize depth consistently to [0,1] range\"\"\"\n",
    "    depth = cv2.resize(depth, (384, 384))\n",
    "    d_min, d_max = np.min(depth), np.max(depth)\n",
    "    normalized = (depth - d_min) / (d_max - d_min + 1e-6)\n",
    "    return normalized\n",
    "\n",
    "\n",
    "class DepthDataset(Dataset):\n",
    "    def __init__(self, rgb_dir, depth_dir, transform):\n",
    "        self.rgb_dir = rgb_dir\n",
    "        self.depth_dir = depth_dir\n",
    "        self.transform = transform\n",
    "        self.filenames = sorted(os.listdir(rgb_dir))\n",
    "\n",
    "        # Validate data consistency\n",
    "        self._validate_data()\n",
    "\n",
    "    def _validate_data(self):\n",
    "        \"\"\"Validate that image and depth files match and are available\"\"\"\n",
    "        depth_files = set(os.listdir(self.depth_dir))\n",
    "        missing_depths = []\n",
    "\n",
    "        for fname in self.filenames:\n",
    "            if fname not in depth_files:\n",
    "                missing_depths.append(fname)\n",
    "\n",
    "        if missing_depths:\n",
    "            print(f\"Warning: {len(missing_depths)} image files don't have matching depth files.\")\n",
    "            print(f\"First 5 missing: {missing_depths[:5]}\")\n",
    "            # Remove files without depth data\n",
    "            self.filenames = [f for f in self.filenames if f in depth_files]\n",
    "\n",
    "        print(f\"Dataset initialized with {len(self.filenames)} valid samples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.filenames[idx]\n",
    "        rgb_path = os.path.join(self.rgb_dir, fname)\n",
    "        depth_path = os.path.join(self.depth_dir, fname)\n",
    "\n",
    "        image = cv2.imread(rgb_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        depth = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED).astype(np.float32)\n",
    "        depth = adaptive_depth_normalize(depth)\n",
    "\n",
    "        augmented = self.transform(image=image)\n",
    "        image = augmented[\"image\"]\n",
    "\n",
    "        # Ensure image tensor is 384x384x3 and in right format\n",
    "        if image.shape[-2:] != (384, 384):\n",
    "            image = F.interpolate(image.unsqueeze(0), size=(384, 384), mode='bilinear', align_corners=False).squeeze(0)\n",
    "\n",
    "        # Check for NaN values\n",
    "        if torch.isnan(image).any():\n",
    "            print(f\"Warning: NaN values found in image tensor for file {fname}\")\n",
    "            # Replace NaNs with zeros\n",
    "            image = torch.nan_to_num(image, nan=0.0)\n",
    "\n",
    "        depth = torch.tensor(depth, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        # Check for NaN values in depth\n",
    "        if torch.isnan(depth).any():\n",
    "            print(f\"Warning: NaN values found in depth tensor for file {fname}\")\n",
    "            depth = torch.nan_to_num(depth, nan=0.0)\n",
    "\n",
    "        return image, depth\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MODEL: FPN HEAD WITH CONSISTENT CHANNEL HANDLING\n",
    "# ============================================================\n",
    "class FPNHead(nn.Module):\n",
    "    def __init__(self, out_channels=64):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.out_channels = out_channels\n",
    "        self.initialized = False\n",
    "        self.expected_channels = None\n",
    "\n",
    "    def forward(self, features):\n",
    "        # Debug feature shapes\n",
    "        if not self.initialized:\n",
    "            print(\"Input feature shapes before processing:\")\n",
    "            for i, feat in enumerate(features):\n",
    "                print(f\"Feature {i}: shape {feat.shape}\")\n",
    "\n",
    "        # Preprocessing: ensure consistent dimensions for all features\n",
    "        processed_features = []\n",
    "        for feat in features:\n",
    "            # Ensure feature has batch dimension\n",
    "            if feat.ndim == 3:\n",
    "                feat = feat.unsqueeze(0)\n",
    "\n",
    "            # Fix NaN or Inf values\n",
    "            if torch.isnan(feat).any() or torch.isinf(feat).any():\n",
    "                print(f\"Warning: NaN or Inf values found in feature. Replacing with zeros.\")\n",
    "                feat = torch.nan_to_num(feat, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "            processed_features.append(feat)\n",
    "\n",
    "        features = processed_features\n",
    "\n",
    "        # Initialize conv layers on first call with the first batch\n",
    "        if not self.initialized:\n",
    "            print(\"Initializing FPN conv layers with actual feature dimensions:\")\n",
    "            self.expected_channels = []\n",
    "            for idx, feat in enumerate(features):\n",
    "                in_channels = feat.shape[1]\n",
    "                self.expected_channels.append(in_channels)\n",
    "                print(f\"Feature {idx}: shape {feat.shape}, channels {in_channels}\")\n",
    "                self.conv_layers.append(\n",
    "                    nn.Sequential(\n",
    "                        nn.Conv2d(in_channels, self.out_channels, kernel_size=1).to(feat.device),\n",
    "                        nn.BatchNorm2d(self.out_channels).to(feat.device),\n",
    "                        nn.ReLU(inplace=True)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # Create fusion conv on first pass\n",
    "            self.fusion_conv = nn.Sequential(\n",
    "                nn.Conv2d(len(features) * self.out_channels, self.out_channels, kernel_size=3, padding=1).to(\n",
    "                    features[0].device),\n",
    "                nn.Dropout2d(0.2),\n",
    "                nn.BatchNorm2d(self.out_channels).to(features[0].device),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "            self.initialized = True\n",
    "\n",
    "        # Process features\n",
    "        outs = []\n",
    "        for idx, (feat, conv) in enumerate(zip(features, self.conv_layers)):\n",
    "            # Check for channel mismatch and fix it consistently\n",
    "            if feat.shape[1] != self.expected_channels[idx]:\n",
    "                if self.training:  # Only log during training to reduce verbosity\n",
    "                    print(\n",
    "                        f\"Warning: Feature {idx} has {feat.shape[1]} channels but expected {self.expected_channels[idx]}. Reshaping...\")\n",
    "\n",
    "                # Use a consistent method for handling channel mismatch\n",
    "                feat = self._fix_channels(feat, self.expected_channels[idx])\n",
    "\n",
    "            # Apply convolution\n",
    "            try:\n",
    "                out = conv(feat)\n",
    "                outs.append(out)\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error processing feature {idx}: {e}\")\n",
    "                print(f\"Feature shape: {feat.shape}, Expected channels: {self.expected_channels[idx]}\")\n",
    "                # Create a placeholder output of the expected shape\n",
    "                placeholder = torch.zeros(\n",
    "                    feat.shape[0], self.out_channels, feat.shape[2], feat.shape[3],\n",
    "                    device=feat.device\n",
    "                )\n",
    "                outs.append(placeholder)\n",
    "\n",
    "        # Resize all features to the size of the first feature\n",
    "        target_size = outs[0].shape[-2:]\n",
    "        ups = [F.interpolate(f, size=target_size, mode='bilinear', align_corners=False) for f in outs]\n",
    "        fused = torch.cat(ups, dim=1)\n",
    "\n",
    "        return self.fusion_conv(fused)\n",
    "\n",
    "    def _fix_channels(self, tensor, target_channels):\n",
    "        \"\"\"Helper method to fix channel dimension mismatches consistently\"\"\"\n",
    "        current_channels = tensor.shape[1]\n",
    "\n",
    "        if current_channels < target_channels:\n",
    "            # Pad with zeros to match expected channels\n",
    "            padding = torch.zeros(tensor.shape[0], target_channels - current_channels,\n",
    "                                  tensor.shape[2], tensor.shape[3], device=tensor.device)\n",
    "            return torch.cat([tensor, padding], dim=1)\n",
    "        elif current_channels > target_channels:\n",
    "            # Take the first target_channels channels\n",
    "            return tensor[:, :target_channels, :, :]\n",
    "        else:\n",
    "            return tensor  # No change needed\n",
    "\n",
    "\n",
    "# Simple depth refinement (super-resolution style) module with skip connections.\n",
    "class DepthRefinementNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, base_channels=32):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, base_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(base_channels, base_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(base_channels, in_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        return x + skip\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# IMPROVED MODEL WITH CONSISTENT FEATURE HANDLING\n",
    "# ============================================================\n",
    "class DepthEstimationWithRefinement(nn.Module):\n",
    "    def __init__(self, dpt_model, fpn_head, refinement_net):\n",
    "        super().__init__()\n",
    "        self.dpt = dpt_model\n",
    "        self.fpn = fpn_head\n",
    "        self.refine = refinement_net\n",
    "        # Add a 1x1 conv to reduce channels from 64 to 1 before refinement\n",
    "        self.reduce_channels = nn.Conv2d(64, 1, kernel_size=1)\n",
    "\n",
    "        # Create feature consistency check flag\n",
    "        self.feature_check_done = False\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        # Track the forward pass for debugging\n",
    "        try:\n",
    "            # Get multi-scale features with hidden states\n",
    "            outputs = self.dpt.base_model(pixel_values, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states\n",
    "\n",
    "            # Use the last four hidden states as features and ensure consistency\n",
    "            features = []\n",
    "            for i in range(4):\n",
    "                feature = hidden_states[-(i + 1)]\n",
    "                # Ensure feature has proper batch dimension\n",
    "                if feature.ndim == 3:\n",
    "                    feature = feature.unsqueeze(0)\n",
    "                features.append(feature)\n",
    "\n",
    "            # Log feature shapes for first batch during training\n",
    "            if self.training and not self.feature_check_done:\n",
    "                print(\"Feature shapes before FPN:\")\n",
    "                for i, feat in enumerate(features):\n",
    "                    print(f\"Feature {i}: shape {feat.shape}\")\n",
    "                self.feature_check_done = True\n",
    "\n",
    "            # Process through FPN\n",
    "            fpn_out = self.fpn(features)\n",
    "\n",
    "            # Get DPT output\n",
    "            with torch.no_grad():\n",
    "                # We capture DPT output in a separate try block to isolate errors\n",
    "                dpt_out = self.dpt(pixel_values).predicted_depth\n",
    "\n",
    "                # Check for NaN values\n",
    "                if torch.isnan(dpt_out).any():\n",
    "                    print(\"Warning: NaN values detected in DPT output. Replacing with zeros.\")\n",
    "                    dpt_out = torch.nan_to_num(dpt_out, nan=0.0)\n",
    "\n",
    "                # Add batch dimension if missing\n",
    "                if dpt_out.ndim == 2:\n",
    "                    dpt_out = dpt_out.unsqueeze(0)\n",
    "\n",
    "                # Add channel dimension if missing\n",
    "                if dpt_out.ndim == 3:\n",
    "                    dpt_out = dpt_out.unsqueeze(1)  # (B,1,H,W)\n",
    "\n",
    "                # Scale DPT output to be in a reasonable range\n",
    "                if dpt_out.max() > 100:\n",
    "                    dpt_out = dpt_out / dpt_out.max()\n",
    "\n",
    "            # First reduce the channels of fpn_out to match dpt_out\n",
    "            fpn_out_reduced = self.reduce_channels(fpn_out)\n",
    "\n",
    "            # Safety check for fpn output\n",
    "            if torch.isnan(fpn_out_reduced).any():\n",
    "                print(\"Warning: NaN values in FPN output. Replacing with zeros.\")\n",
    "                fpn_out_reduced = torch.nan_to_num(fpn_out_reduced, nan=0.0)\n",
    "\n",
    "            # Then upsample to match dpt_out dimensions\n",
    "            fpn_out_upsampled = F.interpolate(fpn_out_reduced, size=dpt_out.shape[-2:], mode='bilinear',\n",
    "                                              align_corners=False)\n",
    "\n",
    "            # Now both have shape (B,1,H,W) and can be fused\n",
    "            fused = dpt_out + fpn_out_upsampled\n",
    "\n",
    "            # Final check for NaN values before refinement\n",
    "            if torch.isnan(fused).any():\n",
    "                print(\"Warning: NaN values in fused output. Replacing with zeros.\")\n",
    "                fused = torch.nan_to_num(fused, nan=0.0)\n",
    "\n",
    "            refined = self.refine(fused)\n",
    "\n",
    "            # Ensure refined has the expected output shape (B,1,384,384)\n",
    "            if refined.shape[-2:] != (384, 384):\n",
    "                refined = F.interpolate(refined, size=(384, 384), mode='bilinear', align_corners=False)\n",
    "\n",
    "            # Final NaN check\n",
    "            if torch.isnan(refined).any():\n",
    "                print(\"Warning: NaN values in refined output. Replacing with zeros.\")\n",
    "                refined = torch.nan_to_num(refined, nan=0.0)\n",
    "\n",
    "            return refined\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in model forward pass: {e}\")\n",
    "            # If an error occurs, return a zero tensor of expected shape\n",
    "            return torch.zeros(pixel_values.shape[0], 1, 384, 384, device=pixel_values.device)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# LOSS FUNCTIONS WITH NAN CHECKING\n",
    "# ============================================================\n",
    "def edge_aware_smoothness_loss(depth, image):\n",
    "    # Check for NaN or Inf values\n",
    "    if torch.isnan(depth).any() or torch.isnan(image).any():\n",
    "        print(\"Warning: NaN values detected in edge loss inputs. Using safe calculations.\")\n",
    "        depth = torch.nan_to_num(depth, nan=0.0)\n",
    "        image = torch.nan_to_num(image, nan=0.0)\n",
    "\n",
    "    dy_depth, dx_depth = torch.gradient(depth, dim=[2, 3])\n",
    "    dy_img, dx_img = torch.gradient(image, dim=[2, 3])\n",
    "\n",
    "    # Avoid division by zero or large values\n",
    "    weight_x = torch.exp(-torch.mean(torch.abs(dx_img).clamp(min=1e-6, max=10), dim=1, keepdim=True))\n",
    "    weight_y = torch.exp(-torch.mean(torch.abs(dy_img).clamp(min=1e-6, max=10), dim=1, keepdim=True))\n",
    "\n",
    "    loss_x = weight_x * torch.abs(dx_depth)\n",
    "    loss_y = weight_y * torch.abs(dy_depth)\n",
    "\n",
    "    return torch.mean(loss_x + loss_y)\n",
    "\n",
    "\n",
    "def combined_loss(pred, target, image):\n",
    "    # Check for NaN values\n",
    "    if torch.isnan(pred).any() or torch.isnan(target).any() or torch.isnan(image).any():\n",
    "        print(\"Warning: NaN values detected in loss inputs. Replacing with zeros.\")\n",
    "        pred = torch.nan_to_num(pred, nan=0.0)\n",
    "        target = torch.nan_to_num(target, nan=0.0)\n",
    "        image = torch.nan_to_num(image, nan=0.0)\n",
    "\n",
    "    # Ensure pred and target have the same shape\n",
    "    if pred.shape != target.shape:\n",
    "        pred = F.interpolate(pred, size=target.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "    # Calculate individual losses with safe operations\n",
    "    try:\n",
    "        l1 = F.l1_loss(pred, target)\n",
    "        ssim_loss = 1 - ssim(pred.clamp(0, 1), target.clamp(0, 1), data_range=1, size_average=True)\n",
    "        edge_loss = edge_aware_smoothness_loss(pred, image)\n",
    "\n",
    "        # Check for extreme values in individual losses\n",
    "        if torch.isnan(l1) or torch.isinf(l1) or l1 > 100:\n",
    "            print(f\"Warning: L1 loss is problematic: {l1}. Using safe value.\")\n",
    "            l1 = torch.tensor(1.0, device=pred.device)\n",
    "\n",
    "        if torch.isnan(ssim_loss) or torch.isinf(ssim_loss) or ssim_loss > 100:\n",
    "            print(f\"Warning: SSIM loss is problematic: {ssim_loss}. Using safe value.\")\n",
    "            ssim_loss = torch.tensor(1.0, device=pred.device)\n",
    "\n",
    "        if torch.isnan(edge_loss) or torch.isinf(edge_loss) or edge_loss > 100:\n",
    "            print(f\"Warning: Edge loss is problematic: {edge_loss}. Using safe value.\")\n",
    "            edge_loss = torch.tensor(1.0, device=pred.device)\n",
    "\n",
    "        # Combine losses with weights\n",
    "        total_loss = L1_WEIGHT * l1 + SSIM_WEIGHT * ssim_loss + EDGE_WEIGHT * edge_loss\n",
    "\n",
    "        # Final safety check\n",
    "        if torch.isnan(total_loss) or torch.isinf(total_loss) or total_loss > 1000:\n",
    "            print(f\"Warning: Combined loss is problematic: {total_loss}. Using fallback loss.\")\n",
    "            return F.mse_loss(pred, target)  # Fallback to simple MSE\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in loss calculation: {e}\")\n",
    "        # Fallback to MSE loss in case of error\n",
    "        return F.mse_loss(pred, target)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# VALIDATION FUNCTION\n",
    "# ============================================================\n",
    "def validate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for img, depth in val_loader:\n",
    "            img, depth = img.to(device), depth.to(device)\n",
    "            pred = model(img)\n",
    "            loss = combined_loss(pred, depth, img)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    model.train()\n",
    "    return avg_val_loss\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# IMPROVED TRAINING LOOP WITH GRADIENT CLIPPING\n",
    "# ============================================================\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, scaler,\n",
    "                num_epochs, grad_accum_steps, device, max_grad_norm):\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Track batch stats for debugging\n",
    "        batch_stats = []\n",
    "\n",
    "        for i, (img, depth) in enumerate(train_loader):\n",
    "            img, depth = img.to(device), depth.to(device)\n",
    "\n",
    "            # Check for NaN values in input\n",
    "            if torch.isnan(img).any() or torch.isnan(depth).any():\n",
    "                print(f\"Warning: NaN values found in input batch {i}. Skipping batch.\")\n",
    "                continue\n",
    "\n",
    "            with autocast():\n",
    "                pred = model(img)\n",
    "                loss = combined_loss(pred, depth, img)\n",
    "\n",
    "            # Skip problematic loss values\n",
    "            if torch.isnan(loss) or torch.isinf(loss) or loss > 1000:\n",
    "                print(f\"Warning: Problematic loss value {loss.item()} in batch {i}. Skipping batch.\")\n",
    "                continue\n",
    "\n",
    "            # Scale loss for gradient accumulation\n",
    "            scaled_loss = loss / grad_accum_steps\n",
    "            scaler.scale(scaled_loss).backward()\n",
    "\n",
    "            # Track batch statistics\n",
    "            with torch.no_grad():\n",
    "                batch_stats.append({\n",
    "                    'batch': i,\n",
    "                    'loss': loss.item(),\n",
    "                    'pred_min': pred.min().item(),\n",
    "                    'pred_max': pred.max().item(),\n",
    "                    'pred_mean': pred.mean().item(),\n",
    "                    'target_min': depth.min().item(),\n",
    "                    'target_max': depth.max().item(),\n",
    "                    'target_mean': depth.mean().item()\n",
    "                })\n",
    "\n",
    "            if (i + 1) % grad_accum_steps == 0:\n",
    "                # Clip gradients to prevent explosion\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "                # Step optimizer and update scaler\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Print progress for every 10 batches\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{num_epochs}, Batch {i + 1}/{len(train_loader)}, \"\n",
    "                      f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Log batch statistics for the epoch\n",
    "        print(f\"Batch statistics for epoch {epoch + 1}:\")\n",
    "        if batch_stats:\n",
    "            avg_loss = sum(s['loss'] for s in batch_stats) / len(batch_stats)\n",
    "            avg_pred_mean = sum(s['pred_mean'] for s in batch_stats) / len(batch_stats)\n",
    "            avg_target_mean = sum(s['target_mean'] for s in batch_stats) / len(batch_stats)\n",
    "            print(f\"  Avg Loss: {avg_loss:.4f}, Avg Pred Mean: {avg_pred_mean:.4f}, \"\n",
    "                  f\"Avg Target Mean: {avg_target_mean:.4f}\")\n",
    "\n",
    "        # Step scheduler based on epoch\n",
    "        scheduler.step()\n",
    "\n",
    "        # Calculate average loss\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Validate model\n",
    "        val_loss = validate_model(model, val_loader, device)\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            print(f\"New best model with validation loss: {val_loss:.4f}\")\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "\n",
    "        # Save checkpoint every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'scaler_state_dict': scaler.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "            }, f\"checkpoint_epoch_{epoch + 1}.pt\")\n",
    "\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN CODE EXECUTION\n",
    "# ============================================================\n",
    "def main():\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    # Data loaders\n",
    "    train_dataset = DepthDataset(TRAIN_RGB_DIR, TRAIN_DEPTH_DIR, train_transform)\n",
    "    val_dataset = DepthDataset(VAL_RGB_DIR, VAL_DEPTH_DIR, val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    # Initialize model components\n",
    "    backbone_config = Dinov2Config.from_pretrained(\"facebook/dinov2-base\",\n",
    "                                                   out_features=[\"stage1\", \"stage2\", \"stage3\", \"stage4\"],\n",
    "                                                   reshape_hidden_states=False)\n",
    "    dpt_config = DPTConfig(backbone_config=backbone_config)\n",
    "    base_model = DPTForDepthEstimation(config=dpt_config)\n",
    "    base_model.to(DEVICE)\n",
    "\n",
    "    fpn_head = FPNHead(out_channels=64).to(DEVICE)\n",
    "    refinement_net = DepthRefinementNet(in_channels=1, base_channels=32).to(DEVICE)\n",
    "\n",
    "    model = DepthEstimationWithRefinement(base_model, fpn_head, refinement_net)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Validate model with a sample batch before training\n",
    "    print(\"Validating model with a sample batch...\")\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    img_sample, depth_sample = sample_batch\n",
    "    img_sample, depth_sample = img_sample.to(DEVICE), depth_sample.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            pred_sample = model(img_sample)\n",
    "            print(f\"Sample prediction shape: {pred_sample.shape}\")\n",
    "            print(f\"Sample prediction stats: min={pred_sample.min().item():.4f}, \"\n",
    "                  f\"max={pred_sample.max().item():.4f}, mean={pred_sample.mean().item():.4f}\")\n",
    "            print(\"Model validation successful!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during model validation: {e}\")\n",
    "            print(\"Please fix the model before proceeding with training.\")\n",
    "            return\n",
    "\n",
    "    # Initialize optimizer, scheduler, and scaler\n",
    "    optimizer = topt.Ranger(model.parameters(), lr=INIT_LR, weight_decay=1e-5)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2, eta_min=1e-6)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, train_loader, val_loader, optimizer, scheduler, scaler,\n",
    "                NUM_EPOCHS, GRAD_ACCUM_STEPS, DEVICE, MAX_GRAD_NORM)\n",
    "\n",
    "    # Save the final model\n",
    "    model_save_path = \"fine_tuned_dpt_refined\"\n",
    "    os.makedirs(model_save_path, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(model_save_path, \"pytorch_model.bin\"))\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11466546,
     "sourceId": 96480,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5010.815624,
   "end_time": "2025-03-29T12:29:53.340611",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-29T11:06:22.524987",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0b134efc72574182b0b2890ba09db730": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_432135f5385f4ab88a5a44ab09a61b7f",
       "max": 548.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_70d54294fe4444f0b85b850ecefa4505",
       "tabbable": null,
       "tooltip": null,
       "value": 548.0
      }
     },
     "168f6a7ea04c4766bc7679af7caee0a9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1e83dbf4895c40bba4fb97e910528133": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "42bf368b363c4f1091ba897f27797f25": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_168f6a7ea04c4766bc7679af7caee0a9",
       "placeholder": "​",
       "style": "IPY_MODEL_1e83dbf4895c40bba4fb97e910528133",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json: 100%"
      }
     },
     "432135f5385f4ab88a5a44ab09a61b7f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "47c0f07b957c4056b5c2cf3394bab240": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4ee14009a31942e380cc516ad33946cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_42bf368b363c4f1091ba897f27797f25",
        "IPY_MODEL_0b134efc72574182b0b2890ba09db730",
        "IPY_MODEL_a5dfa355fad04ceb978e5a0fe9bc58a1"
       ],
       "layout": "IPY_MODEL_76383413fb51425388255b48872f7185",
       "tabbable": null,
       "tooltip": null
      }
     },
     "70d54294fe4444f0b85b850ecefa4505": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "76383413fb51425388255b48872f7185": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a5dfa355fad04ceb978e5a0fe9bc58a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f04bc040d5a543ef9aad2998f800260d",
       "placeholder": "​",
       "style": "IPY_MODEL_47c0f07b957c4056b5c2cf3394bab240",
       "tabbable": null,
       "tooltip": null,
       "value": " 548/548 [00:00&lt;00:00, 49.7kB/s]"
      }
     },
     "f04bc040d5a543ef9aad2998f800260d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
