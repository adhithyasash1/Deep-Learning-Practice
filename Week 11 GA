{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30cf00b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T17:34:36.757275Z",
     "iopub.status.busy": "2025-03-28T17:34:36.757028Z",
     "iopub.status.idle": "2025-03-28T17:34:36.765460Z",
     "shell.execute_reply": "2025-03-28T17:34:36.764747Z"
    },
    "papermill": {
     "duration": 0.01327,
     "end_time": "2025-03-28T17:34:36.766697",
     "exception": false,
     "start_time": "2025-03-28T17:34:36.753427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nimport cv2\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport scipy.stats as stats\\nfrom typing import Dict, List, Tuple\\n\\nclass DepthEstimationDatasetAnalyzer:\\n    def __init__(self, base_path: str):\\n        self.base_path = base_path\\n        self.train_img_path = os.path.join(base_path, \\'training\\', \\'images\\')\\n        self.train_depth_path = os.path.join(base_path, \\'training\\', \\'depths\\')\\n        self.val_img_path = os.path.join(base_path, \\'validation\\', \\'images\\')\\n        self.val_depth_path = os.path.join(base_path, \\'validation\\', \\'depths\\')\\n        self.test_img_path = os.path.join(base_path, \\'testing\\', \\'images\\')\\n\\n    def get_image_stats(self, path: str) -> Dict:\\n        \"\"\"\\n        Compute comprehensive statistics for images in a given path\\n        \"\"\"\\n        images = [os.path.join(path, img) for img in os.listdir(path) if img.endswith((\\'.png\\', \\'.jpg\\'))]\\n        \\n        stats_dict = {\\n            \\'total_images\\': len(images),\\n            \\'resolutions\\': {},\\n            \\'pixel_stats\\': {\\n                \\'mean\\': [],\\n                \\'std\\': [],\\n                \\'min\\': [],\\n                \\'max\\': []\\n            },\\n            \\'color_channels\\': []\\n        }\\n\\n        for img_path in images:\\n            img = cv2.imread(img_path)\\n            \\n            # Resolution tracking\\n            resolution = f\"{img.shape[1]}x{img.shape[0]}\"\\n            stats_dict[\\'resolutions\\'][resolution] = stats_dict[\\'resolutions\\'].get(resolution, 0) + 1\\n            \\n            # Color channel analysis\\n            stats_dict[\\'color_channels\\'].append(img.shape[2] if len(img.shape) > 2 else 1)\\n            \\n            # Pixel intensity statistics\\n            for channel in range(img.shape[2] if len(img.shape) > 2 else 1):\\n                channel_data = img[:,:,channel] if len(img.shape) > 2 else img\\n                stats_dict[\\'pixel_stats\\'][\\'mean\\'].append(np.mean(channel_data))\\n                stats_dict[\\'pixel_stats\\'][\\'std\\'].append(np.std(channel_data))\\n                stats_dict[\\'pixel_stats\\'][\\'min\\'].append(np.min(channel_data))\\n                stats_dict[\\'pixel_stats\\'][\\'max\\'].append(np.max(channel_data))\\n        \\n        return stats_dict\\n\\n    def analyze_depth_maps(self, path: str) -> Dict:\\n        \"\"\"\\n        Analyze depth map characteristics\\n        \"\"\"\\n        depth_maps = [os.path.join(path, depth) for depth in os.listdir(path) if depth.endswith((\\'.png\\', \\'.npy\\'))]\\n        \\n        depth_stats = {\\n            \\'total_depth_maps\\': len(depth_maps),\\n            \\'depth_range\\': {\\'min\\': float(\\'inf\\'), \\'max\\': float(\\'-inf\\')},\\n            \\'histogram\\': None,\\n            \\'pixel_distribution\\': {}\\n        }\\n\\n        depth_values = []\\n        for depth_path in depth_maps:\\n            depth = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED)\\n            \\n            depth_values.extend(depth.flatten())\\n            depth_stats[\\'depth_range\\'][\\'min\\'] = min(depth_stats[\\'depth_range\\'][\\'min\\'], np.min(depth))\\n            depth_stats[\\'depth_range\\'][\\'max\\'] = max(depth_stats[\\'depth_range\\'][\\'max\\'], np.max(depth))\\n        \\n        # Depth value distribution\\n        depth_values = np.array(depth_values)\\n        depth_stats[\\'histogram\\'] = np.histogram(depth_values, bins=50)\\n        depth_stats[\\'pixel_distribution\\'] = {\\n            \\'mean\\': np.mean(depth_values),\\n            \\'median\\': np.median(depth_values),\\n            \\'std\\': np.std(depth_values),\\n            \\'skewness\\': stats.skew(depth_values),\\n            \\'kurtosis\\': stats.kurtosis(depth_values)\\n        }\\n\\n        return depth_stats\\n\\n    def visualize_degradation_impact(self, save_path=\\'degradation_comparison.png\\') -> None:\\n        \"\"\"\\n        Compare image characteristics before and after processing\\n        \"\"\"\\n        plt.figure(figsize=(15, 5))\\n        \\n        # Sample images\\n        clean_img_path = os.path.join(self.train_img_path, os.listdir(self.train_img_path)[0])\\n        clean_img = cv2.imread(clean_img_path)\\n        gray_img = cv2.cvtColor(clean_img, cv2.COLOR_BGR2GRAY)\\n        \\n        # Visualize original and enhanced images\\n        plt.subplot(131)\\n        plt.title(\\'Original Image\\')\\n        plt.imshow(cv2.cvtColor(clean_img, cv2.COLOR_BGR2RGB))\\n        plt.axis(\\'off\\')\\n        \\n        plt.subplot(132)\\n        plt.title(\\'Grayscale\\')\\n        plt.imshow(gray_img, cmap=\\'gray\\')\\n        plt.axis(\\'off\\')\\n        \\n        plt.subplot(133)\\n        plt.title(\\'Histogram Equalization\\')\\n        eq_img = cv2.equalizeHist(gray_img)\\n        plt.imshow(eq_img, cmap=\\'gray\\')\\n        plt.axis(\\'off\\')\\n        \\n        plt.tight_layout()\\n        plt.savefig(save_path)\\n        plt.close()\\n\\n    def extract_advanced_features(self) -> Dict:\\n        \"\"\"\\n        Extract advanced image features for model design\\n        \"\"\"\\n        advanced_features = {\\n            \\'edge_density\\': [],\\n            \\'texture_complexity\\': [],\\n            \\'brightness\\': []\\n        }\\n\\n        for img_path in os.listdir(self.train_img_path):\\n            full_path = os.path.join(self.train_img_path, img_path)\\n            img = cv2.imread(full_path)\\n            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\\n\\n            # Edge detection\\n            edges = cv2.Canny(gray, 100, 200)\\n            advanced_features[\\'edge_density\\'].append(np.sum(edges) / (edges.shape[0] * edges.shape[1]))\\n\\n            # Texture complexity (variance of Laplacian)\\n            advanced_features[\\'texture_complexity\\'].append(cv2.Laplacian(gray, cv2.CV_64F).var())\\n\\n            # Image brightness\\n            advanced_features[\\'brightness\\'].append(np.mean(gray))\\n\\n        return advanced_features\\n\\n    def generate_report(self) -> Dict:\\n        \"\"\"\\n        Comprehensive dataset analysis report\\n        \"\"\"\\n        report = {\\n            \\'training_images\\': self.get_image_stats(self.train_img_path),\\n            \\'validation_images\\': self.get_image_stats(self.val_img_path),\\n            \\'training_depths\\': self.analyze_depth_maps(self.train_depth_path),\\n            \\'validation_depths\\': self.analyze_depth_maps(self.val_depth_path),\\n            \\'advanced_features\\': self.extract_advanced_features()\\n        }\\n\\n        # Visualization\\n        self.visualize_degradation_impact()\\n\\n        return report\\n\\n# Usage\\ndef main():\\n    base_path = \\'/kaggle/input/depth-estimation/competition-data/competition-data\\'\\n    analyzer = DepthEstimationDatasetAnalyzer(base_path)\\n    \\n    try:\\n        dataset_report = analyzer.generate_report()\\n\\n        # Save report to JSON for detailed review\\n        import json\\n        with open(\\'dataset_analysis_report.json\\', \\'w\\') as f:\\n            json.dump(dataset_report, f, indent=4, default=str)\\n\\n        print(\"Dataset analysis complete. Check \\'dataset_analysis_report.json\\' and \\'degradation_comparison.png\\'\")\\n    \\n    except Exception as e:\\n        print(f\"An error occurred: {e}\")\\n        import traceback\\n        traceback.print_exc()\\n\\nif __name__ == \\'__main__\\':\\n    main()\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "class DepthEstimationDatasetAnalyzer:\n",
    "    def __init__(self, base_path: str):\n",
    "        self.base_path = base_path\n",
    "        self.train_img_path = os.path.join(base_path, 'training', 'images')\n",
    "        self.train_depth_path = os.path.join(base_path, 'training', 'depths')\n",
    "        self.val_img_path = os.path.join(base_path, 'validation', 'images')\n",
    "        self.val_depth_path = os.path.join(base_path, 'validation', 'depths')\n",
    "        self.test_img_path = os.path.join(base_path, 'testing', 'images')\n",
    "\n",
    "    def get_image_stats(self, path: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Compute comprehensive statistics for images in a given path\n",
    "        \"\"\"\n",
    "        images = [os.path.join(path, img) for img in os.listdir(path) if img.endswith(('.png', '.jpg'))]\n",
    "        \n",
    "        stats_dict = {\n",
    "            'total_images': len(images),\n",
    "            'resolutions': {},\n",
    "            'pixel_stats': {\n",
    "                'mean': [],\n",
    "                'std': [],\n",
    "                'min': [],\n",
    "                'max': []\n",
    "            },\n",
    "            'color_channels': []\n",
    "        }\n",
    "\n",
    "        for img_path in images:\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            # Resolution tracking\n",
    "            resolution = f\"{img.shape[1]}x{img.shape[0]}\"\n",
    "            stats_dict['resolutions'][resolution] = stats_dict['resolutions'].get(resolution, 0) + 1\n",
    "            \n",
    "            # Color channel analysis\n",
    "            stats_dict['color_channels'].append(img.shape[2] if len(img.shape) > 2 else 1)\n",
    "            \n",
    "            # Pixel intensity statistics\n",
    "            for channel in range(img.shape[2] if len(img.shape) > 2 else 1):\n",
    "                channel_data = img[:,:,channel] if len(img.shape) > 2 else img\n",
    "                stats_dict['pixel_stats']['mean'].append(np.mean(channel_data))\n",
    "                stats_dict['pixel_stats']['std'].append(np.std(channel_data))\n",
    "                stats_dict['pixel_stats']['min'].append(np.min(channel_data))\n",
    "                stats_dict['pixel_stats']['max'].append(np.max(channel_data))\n",
    "        \n",
    "        return stats_dict\n",
    "\n",
    "    def analyze_depth_maps(self, path: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyze depth map characteristics\n",
    "        \"\"\"\n",
    "        depth_maps = [os.path.join(path, depth) for depth in os.listdir(path) if depth.endswith(('.png', '.npy'))]\n",
    "        \n",
    "        depth_stats = {\n",
    "            'total_depth_maps': len(depth_maps),\n",
    "            'depth_range': {'min': float('inf'), 'max': float('-inf')},\n",
    "            'histogram': None,\n",
    "            'pixel_distribution': {}\n",
    "        }\n",
    "\n",
    "        depth_values = []\n",
    "        for depth_path in depth_maps:\n",
    "            depth = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED)\n",
    "            \n",
    "            depth_values.extend(depth.flatten())\n",
    "            depth_stats['depth_range']['min'] = min(depth_stats['depth_range']['min'], np.min(depth))\n",
    "            depth_stats['depth_range']['max'] = max(depth_stats['depth_range']['max'], np.max(depth))\n",
    "        \n",
    "        # Depth value distribution\n",
    "        depth_values = np.array(depth_values)\n",
    "        depth_stats['histogram'] = np.histogram(depth_values, bins=50)\n",
    "        depth_stats['pixel_distribution'] = {\n",
    "            'mean': np.mean(depth_values),\n",
    "            'median': np.median(depth_values),\n",
    "            'std': np.std(depth_values),\n",
    "            'skewness': stats.skew(depth_values),\n",
    "            'kurtosis': stats.kurtosis(depth_values)\n",
    "        }\n",
    "\n",
    "        return depth_stats\n",
    "\n",
    "    def visualize_degradation_impact(self, save_path='degradation_comparison.png') -> None:\n",
    "        \"\"\"\n",
    "        Compare image characteristics before and after processing\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Sample images\n",
    "        clean_img_path = os.path.join(self.train_img_path, os.listdir(self.train_img_path)[0])\n",
    "        clean_img = cv2.imread(clean_img_path)\n",
    "        gray_img = cv2.cvtColor(clean_img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Visualize original and enhanced images\n",
    "        plt.subplot(131)\n",
    "        plt.title('Original Image')\n",
    "        plt.imshow(cv2.cvtColor(clean_img, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(132)\n",
    "        plt.title('Grayscale')\n",
    "        plt.imshow(gray_img, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(133)\n",
    "        plt.title('Histogram Equalization')\n",
    "        eq_img = cv2.equalizeHist(gray_img)\n",
    "        plt.imshow(eq_img, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "\n",
    "    def extract_advanced_features(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract advanced image features for model design\n",
    "        \"\"\"\n",
    "        advanced_features = {\n",
    "            'edge_density': [],\n",
    "            'texture_complexity': [],\n",
    "            'brightness': []\n",
    "        }\n",
    "\n",
    "        for img_path in os.listdir(self.train_img_path):\n",
    "            full_path = os.path.join(self.train_img_path, img_path)\n",
    "            img = cv2.imread(full_path)\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Edge detection\n",
    "            edges = cv2.Canny(gray, 100, 200)\n",
    "            advanced_features['edge_density'].append(np.sum(edges) / (edges.shape[0] * edges.shape[1]))\n",
    "\n",
    "            # Texture complexity (variance of Laplacian)\n",
    "            advanced_features['texture_complexity'].append(cv2.Laplacian(gray, cv2.CV_64F).var())\n",
    "\n",
    "            # Image brightness\n",
    "            advanced_features['brightness'].append(np.mean(gray))\n",
    "\n",
    "        return advanced_features\n",
    "\n",
    "    def generate_report(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Comprehensive dataset analysis report\n",
    "        \"\"\"\n",
    "        report = {\n",
    "            'training_images': self.get_image_stats(self.train_img_path),\n",
    "            'validation_images': self.get_image_stats(self.val_img_path),\n",
    "            'training_depths': self.analyze_depth_maps(self.train_depth_path),\n",
    "            'validation_depths': self.analyze_depth_maps(self.val_depth_path),\n",
    "            'advanced_features': self.extract_advanced_features()\n",
    "        }\n",
    "\n",
    "        # Visualization\n",
    "        self.visualize_degradation_impact()\n",
    "\n",
    "        return report\n",
    "\n",
    "# Usage\n",
    "def main():\n",
    "    base_path = '/kaggle/input/depth-estimation/competition-data/competition-data'\n",
    "    analyzer = DepthEstimationDatasetAnalyzer(base_path)\n",
    "    \n",
    "    try:\n",
    "        dataset_report = analyzer.generate_report()\n",
    "\n",
    "        # Save report to JSON for detailed review\n",
    "        import json\n",
    "        with open('dataset_analysis_report.json', 'w') as f:\n",
    "            json.dump(dataset_report, f, indent=4, default=str)\n",
    "\n",
    "        print(\"Dataset analysis complete. Check 'dataset_analysis_report.json' and 'degradation_comparison.png'\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ee8ff05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T17:34:36.772249Z",
     "iopub.status.busy": "2025-03-28T17:34:36.772058Z",
     "iopub.status.idle": "2025-03-28T17:34:36.777318Z",
     "shell.execute_reply": "2025-03-28T17:34:36.776548Z"
    },
    "papermill": {
     "duration": 0.009277,
     "end_time": "2025-03-28T17:34:36.778615",
     "exception": false,
     "start_time": "2025-03-28T17:34:36.769338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nimport os\\nimport cv2\\nimport numpy as np\\n\\nclass DatasetSummarizer:\\n    def __init__(self, base_path):\\n        self.base_path = base_path\\n        self.train_img_path = os.path.join(base_path, \\'training\\', \\'images\\')\\n        self.train_depth_path = os.path.join(base_path, \\'training\\', \\'depths\\')\\n        self.val_img_path = os.path.join(base_path, \\'validation\\', \\'images\\')\\n        self.val_depth_path = os.path.join(base_path, \\'validation\\', \\'depths\\')\\n\\n    def analyze_dataset(self):\\n        # Dataset Overview\\n        print(\"Dataset Overview:\")\\n        print(f\"Training Images: {len(os.listdir(self.train_img_path))}\")\\n        print(f\"Training Depth Maps: {len(os.listdir(self.train_depth_path))}\")\\n        print(f\"Validation Images: {len(os.listdir(self.val_img_path))}\")\\n        print(f\"Validation Depth Maps: {len(os.listdir(self.val_depth_path))}\")\\n        print(\"\\n\")\\n\\n        # Image Characteristics\\n        print(\"Image Characteristics:\")\\n        self._analyze_images()\\n        \\n        # Depth Map Characteristics\\n        print(\"\\nDepth Map Characteristics:\")\\n        self._analyze_depth_maps()\\n\\n    def _analyze_images(self):\\n        # Sample images\\n        sample_images = os.listdir(self.train_img_path)[:10]\\n        \\n        resolutions = {}\\n        channel_counts = []\\n        pixel_means = []\\n        pixel_stds = []\\n\\n        for img_name in sample_images:\\n            img_path = os.path.join(self.train_img_path, img_name)\\n            img = cv2.imread(img_path)\\n            \\n            # Resolution tracking\\n            resolution = f\"{img.shape[1]}x{img.shape[0]}\"\\n            resolutions[resolution] = resolutions.get(resolution, 0) + 1\\n            \\n            # Color channel analysis\\n            channels = img.shape[2] if len(img.shape) > 2 else 1\\n            channel_counts.append(channels)\\n            \\n            # Pixel intensity statistics\\n            for channel in range(channels):\\n                channel_data = img[:,:,channel] if channels > 1 else img\\n                pixel_means.append(np.mean(channel_data))\\n                pixel_stds.append(np.std(channel_data))\\n\\n        print(\"Image Resolutions:\")\\n        for res, count in resolutions.items():\\n            print(f\"  {res}: {count} images\")\\n        \\n        print(f\"\\nColor Channels: {set(channel_counts)}\")\\n        print(f\"Pixel Intensity Mean: {np.mean(pixel_means):.2f}\")\\n        print(f\"Pixel Intensity Std: {np.mean(pixel_stds):.2f}\")\\n\\n    def _analyze_depth_maps(self):\\n        # Sample depth maps\\n        sample_depths = os.listdir(self.train_depth_path)[:10]\\n        \\n        depth_values = []\\n        depth_ranges = []\\n\\n        for depth_name in sample_depths:\\n            depth_path = os.path.join(self.train_depth_path, depth_name)\\n            depth = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED)\\n            \\n            depth_values.append(depth)\\n            depth_ranges.append((np.min(depth), np.max(depth)))\\n\\n        # Convert to numpy array for aggregate statistics\\n        depth_values = np.concatenate([d.flatten() for d in depth_values])\\n\\n        print(f\"Depth Map Value Range: {min(r[0] for r in depth_ranges)} to {max(r[1] for r in depth_ranges)}\")\\n        print(f\"Depth Pixel Mean: {np.mean(depth_values):.2f}\")\\n        print(f\"Depth Pixel Std: {np.std(depth_values):.2f}\")\\n        print(f\"Depth Pixel Median: {np.median(depth_values):.2f}\")\\n\\n# Usage\\nbase_path = \\'/kaggle/input/depth-estimation/competition-data/competition-data\\'\\nsummarizer = DatasetSummarizer(base_path)\\nsummarizer.analyze_dataset()\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class DatasetSummarizer:\n",
    "    def __init__(self, base_path):\n",
    "        self.base_path = base_path\n",
    "        self.train_img_path = os.path.join(base_path, 'training', 'images')\n",
    "        self.train_depth_path = os.path.join(base_path, 'training', 'depths')\n",
    "        self.val_img_path = os.path.join(base_path, 'validation', 'images')\n",
    "        self.val_depth_path = os.path.join(base_path, 'validation', 'depths')\n",
    "\n",
    "    def analyze_dataset(self):\n",
    "        # Dataset Overview\n",
    "        print(\"Dataset Overview:\")\n",
    "        print(f\"Training Images: {len(os.listdir(self.train_img_path))}\")\n",
    "        print(f\"Training Depth Maps: {len(os.listdir(self.train_depth_path))}\")\n",
    "        print(f\"Validation Images: {len(os.listdir(self.val_img_path))}\")\n",
    "        print(f\"Validation Depth Maps: {len(os.listdir(self.val_depth_path))}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # Image Characteristics\n",
    "        print(\"Image Characteristics:\")\n",
    "        self._analyze_images()\n",
    "        \n",
    "        # Depth Map Characteristics\n",
    "        print(\"\\nDepth Map Characteristics:\")\n",
    "        self._analyze_depth_maps()\n",
    "\n",
    "    def _analyze_images(self):\n",
    "        # Sample images\n",
    "        sample_images = os.listdir(self.train_img_path)[:10]\n",
    "        \n",
    "        resolutions = {}\n",
    "        channel_counts = []\n",
    "        pixel_means = []\n",
    "        pixel_stds = []\n",
    "\n",
    "        for img_name in sample_images:\n",
    "            img_path = os.path.join(self.train_img_path, img_name)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            # Resolution tracking\n",
    "            resolution = f\"{img.shape[1]}x{img.shape[0]}\"\n",
    "            resolutions[resolution] = resolutions.get(resolution, 0) + 1\n",
    "            \n",
    "            # Color channel analysis\n",
    "            channels = img.shape[2] if len(img.shape) > 2 else 1\n",
    "            channel_counts.append(channels)\n",
    "            \n",
    "            # Pixel intensity statistics\n",
    "            for channel in range(channels):\n",
    "                channel_data = img[:,:,channel] if channels > 1 else img\n",
    "                pixel_means.append(np.mean(channel_data))\n",
    "                pixel_stds.append(np.std(channel_data))\n",
    "\n",
    "        print(\"Image Resolutions:\")\n",
    "        for res, count in resolutions.items():\n",
    "            print(f\"  {res}: {count} images\")\n",
    "        \n",
    "        print(f\"\\nColor Channels: {set(channel_counts)}\")\n",
    "        print(f\"Pixel Intensity Mean: {np.mean(pixel_means):.2f}\")\n",
    "        print(f\"Pixel Intensity Std: {np.mean(pixel_stds):.2f}\")\n",
    "\n",
    "    def _analyze_depth_maps(self):\n",
    "        # Sample depth maps\n",
    "        sample_depths = os.listdir(self.train_depth_path)[:10]\n",
    "        \n",
    "        depth_values = []\n",
    "        depth_ranges = []\n",
    "\n",
    "        for depth_name in sample_depths:\n",
    "            depth_path = os.path.join(self.train_depth_path, depth_name)\n",
    "            depth = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED)\n",
    "            \n",
    "            depth_values.append(depth)\n",
    "            depth_ranges.append((np.min(depth), np.max(depth)))\n",
    "\n",
    "        # Convert to numpy array for aggregate statistics\n",
    "        depth_values = np.concatenate([d.flatten() for d in depth_values])\n",
    "\n",
    "        print(f\"Depth Map Value Range: {min(r[0] for r in depth_ranges)} to {max(r[1] for r in depth_ranges)}\")\n",
    "        print(f\"Depth Pixel Mean: {np.mean(depth_values):.2f}\")\n",
    "        print(f\"Depth Pixel Std: {np.std(depth_values):.2f}\")\n",
    "        print(f\"Depth Pixel Median: {np.median(depth_values):.2f}\")\n",
    "\n",
    "# Usage\n",
    "base_path = '/kaggle/input/depth-estimation/competition-data/competition-data'\n",
    "summarizer = DatasetSummarizer(base_path)\n",
    "summarizer.analyze_dataset()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a95fe4b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T17:34:36.784244Z",
     "iopub.status.busy": "2025-03-28T17:34:36.784048Z",
     "iopub.status.idle": "2025-03-28T17:34:36.790452Z",
     "shell.execute_reply": "2025-03-28T17:34:36.789806Z"
    },
    "papermill": {
     "duration": 0.010673,
     "end_time": "2025-03-28T17:34:36.791688",
     "exception": false,
     "start_time": "2025-03-28T17:34:36.781015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nimport cv2\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom scipy import stats\\n\\nclass AdvancedDatasetAnalyzer:\\n    def __init__(self, base_path):\\n        self.base_path = base_path\\n        self.train_img_path = os.path.join(base_path, \\'training\\', \\'images\\')\\n        self.train_depth_path = os.path.join(base_path, \\'training\\', \\'depths\\')\\n        self.val_img_path = os.path.join(base_path, \\'validation\\', \\'images\\')\\n        self.val_depth_path = os.path.join(base_path, \\'validation\\', \\'depths\\')\\n\\n    def comprehensive_analysis(self):\\n        \"\"\"\\n        Perform a comprehensive analysis of the depth estimation dataset\\n        \"\"\"\\n        print(\"=== DEPTH ESTIMATION DATASET COMPREHENSIVE ANALYSIS ===\\n\")\\n        \\n        # Dataset Composition\\n        self._dataset_composition()\\n        \\n        # Image Characteristics\\n        print(\"\\n=== IMAGE CHARACTERISTICS ===\")\\n        image_analysis = self._advanced_image_analysis()\\n        \\n        # Depth Map Analysis\\n        print(\"\\n=== DEPTH MAP CHARACTERISTICS ===\")\\n        depth_analysis = self._advanced_depth_map_analysis()\\n        \\n        # Degradation and Noise Assessment\\n        print(\"\\n=== DEGRADATION AND NOISE ASSESSMENT ===\")\\n        self._degradation_assessment()\\n        \\n        # Preprocessing Recommendations\\n        print(\"\\n=== PREPROCESSING RECOMMENDATIONS ===\")\\n        self._preprocessing_recommendations(image_analysis, depth_analysis)\\n        \\n        # Model Architecture Suggestions\\n        print(\"\\n=== MODEL ARCHITECTURE SUGGESTIONS ===\")\\n        self._model_architecture_suggestions(image_analysis, depth_analysis)\\n\\n    def _dataset_composition(self):\\n        \"\"\"\\n        Analyze dataset composition and splitting\\n        \"\"\"\\n        print(\"Dataset Composition:\")\\n        print(f\"Training Images: {len(os.listdir(self.train_img_path))}\")\\n        print(f\"Training Depth Maps: {len(os.listdir(self.train_depth_path))}\")\\n        print(f\"Validation Images: {len(os.listdir(self.val_img_path))}\")\\n        print(f\"Validation Depth Maps: {len(os.listdir(self.val_depth_path))}\")\\n\\n    def _advanced_image_analysis(self):\\n        \"\"\"\\n        Perform advanced analysis of training images\\n        \"\"\"\\n        image_paths = [os.path.join(self.train_img_path, img) for img in os.listdir(self.train_img_path)]\\n        \\n        # Image characteristics\\n        resolutions = {}\\n        brightness_levels = []\\n        contrast_levels = []\\n        edge_densities = []\\n\\n        for img_path in image_paths[:20]:  # Sample 20 images\\n            img = cv2.imread(img_path)\\n            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\\n            \\n            # Resolution tracking\\n            resolution = f\"{img.shape[1]}x{img.shape[0]}\"\\n            resolutions[resolution] = resolutions.get(resolution, 0) + 1\\n            \\n            # Brightness analysis\\n            brightness_levels.append(np.mean(gray))\\n            \\n            # Contrast analysis\\n            contrast_levels.append(np.std(gray))\\n            \\n            # Edge density\\n            edges = cv2.Canny(gray, 100, 200)\\n            edge_densities.append(np.sum(edges) / (edges.shape[0] * edges.shape[1]))\\n\\n        print(\"Image Resolution Distribution:\")\\n        for res, count in resolutions.items():\\n            print(f\"  {res}: {count} images\")\\n        \\n        print(\"\\nImage Characteristics:\")\\n        print(f\"Average Brightness: {np.mean(brightness_levels):.2f}\")\\n        print(f\"Brightness Variance: {np.std(brightness_levels):.2f}\")\\n        print(f\"Average Contrast: {np.mean(contrast_levels):.2f}\")\\n        print(f\"Average Edge Density: {np.mean(edge_densities):.4f}\")\\n\\n        return {\\n            \\'resolutions\\': resolutions,\\n            \\'brightness\\': {\\n                \\'mean\\': np.mean(brightness_levels),\\n                \\'std\\': np.std(brightness_levels)\\n            },\\n            \\'contrast\\': {\\n                \\'mean\\': np.mean(contrast_levels),\\n                \\'std\\': np.std(contrast_levels)\\n            },\\n            \\'edge_density\\': {\\n                \\'mean\\': np.mean(edge_densities),\\n                \\'std\\': np.std(edge_densities)\\n            }\\n        }\\n\\n    def _advanced_depth_map_analysis(self):\\n        \"\"\"\\n        Perform advanced analysis of depth maps\\n        \"\"\"\\n        depth_paths = [os.path.join(self.train_depth_path, depth) for depth in os.listdir(self.train_depth_path)]\\n        \\n        depth_values = []\\n        depth_ranges = []\\n        \\n        for depth_path in depth_paths[:20]:  # Sample 20 depth maps\\n            depth = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED)\\n            depth_values.extend(depth.flatten())\\n            depth_ranges.append((np.min(depth), np.max(depth)))\\n\\n        depth_values = np.array(depth_values)\\n\\n        print(\"Depth Map Characteristics:\")\\n        print(f\"Depth Value Range: {min(r[0] for r in depth_ranges)} to {max(r[1] for r in depth_ranges)}\")\\n        print(f\"Mean Depth: {np.mean(depth_values):.2f}\")\\n        print(f\"Median Depth: {np.median(depth_values):.2f}\")\\n        print(f\"Depth Std Deviation: {np.std(depth_values):.2f}\")\\n        print(f\"Skewness: {stats.skew(depth_values):.2f}\")\\n        print(f\"Kurtosis: {stats.kurtosis(depth_values):.2f}\")\\n\\n        return {\\n            \\'value_range\\': (min(r[0] for r in depth_ranges), max(r[1] for r in depth_ranges)),\\n            \\'mean\\': np.mean(depth_values),\\n            \\'median\\': np.median(depth_values),\\n            \\'std\\': np.std(depth_values),\\n            \\'skewness\\': stats.skew(depth_values),\\n            \\'kurtosis\\': stats.kurtosis(depth_values)\\n        }\\n\\n    def _degradation_assessment(self):\\n        \"\"\"\\n        Assess potential image degradations\\n        \"\"\"\\n        print(\"Potential Degradation Indicators:\")\\n        \\n        # Sample images\\n        sample_paths = [os.path.join(self.train_img_path, img) for img in os.listdir(self.train_img_path)[:10]]\\n        \\n        noise_levels = []\\n        blur_levels = []\\n\\n        for img_path in sample_paths:\\n            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\\n            \\n            # Estimate noise using variance of Laplacian\\n            noise_estimate = cv2.Laplacian(img, cv2.CV_64F).var()\\n            noise_levels.append(noise_estimate)\\n            \\n            # Estimate blur using variance of Laplacian\\n            blur_estimate = cv2.Laplacian(img, cv2.CV_64F).var()\\n            blur_levels.append(blur_estimate)\\n\\n        print(f\"Average Noise Level: {np.mean(noise_levels):.2f}\")\\n        print(f\"Average Blur Level: {np.mean(blur_levels):.2f}\")\\n\\n    def _preprocessing_recommendations(self, image_analysis, depth_analysis):\\n        \"\"\"\\n        Generate preprocessing recommendations\\n        \"\"\"\\n        print(\"Recommended Preprocessing Techniques:\")\\n        \\n        # Resolution handling\\n        if len(image_analysis[\\'resolutions\\']) > 1:\\n            print(\"- Resize images to a consistent input size (e.g., 256x256)\")\\n        \\n        # Brightness and contrast\\n        if image_analysis[\\'brightness\\'][\\'std\\'] > 30:\\n            print(\"- Apply histogram equalization\")\\n        \\n        # Depth map normalization\\n        if depth_analysis[\\'std\\'] > 100:\\n            print(\"- Normalize depth maps using min-max or z-score normalization\")\\n\\n    def _model_architecture_suggestions(self, image_analysis, depth_analysis):\\n        \"\"\"\\n        Suggest potential model architectures\\n        \"\"\"\\n        print(\"Potential Model Architectures:\")\\n        \\n        # Based on depth map characteristics\\n        if depth_analysis[\\'skewness\\'] > 1:\\n            print(\"- Consider models with robust loss functions (e.g., Huber loss)\")\\n        \\n        # Based on edge density and image complexity\\n        if image_analysis[\\'edge_density\\'][\\'mean\\'] > 0.1:\\n            print(\"- Recommended Models:\")\\n            print(\"  1. U-Net based architectures\")\\n            print(\"  2. DeepLab V3+\")\\n            print(\"  3. Transformer-based models (ViT, Swin Transformer)\")\\n        \\n        print(\"\\nPretrained Model Suggestions from Hugging Face:\")\\n        print(\"1. Facebook/AdaBins - Adaptive Bin-based Depth Estimation\")\\n        print(\"2. Intel/midas-v2_small - MiDaS model for robust depth estimation\")\\n        print(\"3. Salesforce/blip-depth-estimator - BLIP-based depth estimation\")\\n\\n# Usage\\nbase_path = \\'/kaggle/input/depth-estimation/competition-data/competition-data\\'\\nanalyzer = AdvancedDatasetAnalyzer(base_path)\\nanalyzer.comprehensive_analysis()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "class AdvancedDatasetAnalyzer:\n",
    "    def __init__(self, base_path):\n",
    "        self.base_path = base_path\n",
    "        self.train_img_path = os.path.join(base_path, 'training', 'images')\n",
    "        self.train_depth_path = os.path.join(base_path, 'training', 'depths')\n",
    "        self.val_img_path = os.path.join(base_path, 'validation', 'images')\n",
    "        self.val_depth_path = os.path.join(base_path, 'validation', 'depths')\n",
    "\n",
    "    def comprehensive_analysis(self):\n",
    "        \"\"\"\n",
    "        Perform a comprehensive analysis of the depth estimation dataset\n",
    "        \"\"\"\n",
    "        print(\"=== DEPTH ESTIMATION DATASET COMPREHENSIVE ANALYSIS ===\\n\")\n",
    "        \n",
    "        # Dataset Composition\n",
    "        self._dataset_composition()\n",
    "        \n",
    "        # Image Characteristics\n",
    "        print(\"\\n=== IMAGE CHARACTERISTICS ===\")\n",
    "        image_analysis = self._advanced_image_analysis()\n",
    "        \n",
    "        # Depth Map Analysis\n",
    "        print(\"\\n=== DEPTH MAP CHARACTERISTICS ===\")\n",
    "        depth_analysis = self._advanced_depth_map_analysis()\n",
    "        \n",
    "        # Degradation and Noise Assessment\n",
    "        print(\"\\n=== DEGRADATION AND NOISE ASSESSMENT ===\")\n",
    "        self._degradation_assessment()\n",
    "        \n",
    "        # Preprocessing Recommendations\n",
    "        print(\"\\n=== PREPROCESSING RECOMMENDATIONS ===\")\n",
    "        self._preprocessing_recommendations(image_analysis, depth_analysis)\n",
    "        \n",
    "        # Model Architecture Suggestions\n",
    "        print(\"\\n=== MODEL ARCHITECTURE SUGGESTIONS ===\")\n",
    "        self._model_architecture_suggestions(image_analysis, depth_analysis)\n",
    "\n",
    "    def _dataset_composition(self):\n",
    "        \"\"\"\n",
    "        Analyze dataset composition and splitting\n",
    "        \"\"\"\n",
    "        print(\"Dataset Composition:\")\n",
    "        print(f\"Training Images: {len(os.listdir(self.train_img_path))}\")\n",
    "        print(f\"Training Depth Maps: {len(os.listdir(self.train_depth_path))}\")\n",
    "        print(f\"Validation Images: {len(os.listdir(self.val_img_path))}\")\n",
    "        print(f\"Validation Depth Maps: {len(os.listdir(self.val_depth_path))}\")\n",
    "\n",
    "    def _advanced_image_analysis(self):\n",
    "        \"\"\"\n",
    "        Perform advanced analysis of training images\n",
    "        \"\"\"\n",
    "        image_paths = [os.path.join(self.train_img_path, img) for img in os.listdir(self.train_img_path)]\n",
    "        \n",
    "        # Image characteristics\n",
    "        resolutions = {}\n",
    "        brightness_levels = []\n",
    "        contrast_levels = []\n",
    "        edge_densities = []\n",
    "\n",
    "        for img_path in image_paths[:20]:  # Sample 20 images\n",
    "            img = cv2.imread(img_path)\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Resolution tracking\n",
    "            resolution = f\"{img.shape[1]}x{img.shape[0]}\"\n",
    "            resolutions[resolution] = resolutions.get(resolution, 0) + 1\n",
    "            \n",
    "            # Brightness analysis\n",
    "            brightness_levels.append(np.mean(gray))\n",
    "            \n",
    "            # Contrast analysis\n",
    "            contrast_levels.append(np.std(gray))\n",
    "            \n",
    "            # Edge density\n",
    "            edges = cv2.Canny(gray, 100, 200)\n",
    "            edge_densities.append(np.sum(edges) / (edges.shape[0] * edges.shape[1]))\n",
    "\n",
    "        print(\"Image Resolution Distribution:\")\n",
    "        for res, count in resolutions.items():\n",
    "            print(f\"  {res}: {count} images\")\n",
    "        \n",
    "        print(\"\\nImage Characteristics:\")\n",
    "        print(f\"Average Brightness: {np.mean(brightness_levels):.2f}\")\n",
    "        print(f\"Brightness Variance: {np.std(brightness_levels):.2f}\")\n",
    "        print(f\"Average Contrast: {np.mean(contrast_levels):.2f}\")\n",
    "        print(f\"Average Edge Density: {np.mean(edge_densities):.4f}\")\n",
    "\n",
    "        return {\n",
    "            'resolutions': resolutions,\n",
    "            'brightness': {\n",
    "                'mean': np.mean(brightness_levels),\n",
    "                'std': np.std(brightness_levels)\n",
    "            },\n",
    "            'contrast': {\n",
    "                'mean': np.mean(contrast_levels),\n",
    "                'std': np.std(contrast_levels)\n",
    "            },\n",
    "            'edge_density': {\n",
    "                'mean': np.mean(edge_densities),\n",
    "                'std': np.std(edge_densities)\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def _advanced_depth_map_analysis(self):\n",
    "        \"\"\"\n",
    "        Perform advanced analysis of depth maps\n",
    "        \"\"\"\n",
    "        depth_paths = [os.path.join(self.train_depth_path, depth) for depth in os.listdir(self.train_depth_path)]\n",
    "        \n",
    "        depth_values = []\n",
    "        depth_ranges = []\n",
    "        \n",
    "        for depth_path in depth_paths[:20]:  # Sample 20 depth maps\n",
    "            depth = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED)\n",
    "            depth_values.extend(depth.flatten())\n",
    "            depth_ranges.append((np.min(depth), np.max(depth)))\n",
    "\n",
    "        depth_values = np.array(depth_values)\n",
    "\n",
    "        print(\"Depth Map Characteristics:\")\n",
    "        print(f\"Depth Value Range: {min(r[0] for r in depth_ranges)} to {max(r[1] for r in depth_ranges)}\")\n",
    "        print(f\"Mean Depth: {np.mean(depth_values):.2f}\")\n",
    "        print(f\"Median Depth: {np.median(depth_values):.2f}\")\n",
    "        print(f\"Depth Std Deviation: {np.std(depth_values):.2f}\")\n",
    "        print(f\"Skewness: {stats.skew(depth_values):.2f}\")\n",
    "        print(f\"Kurtosis: {stats.kurtosis(depth_values):.2f}\")\n",
    "\n",
    "        return {\n",
    "            'value_range': (min(r[0] for r in depth_ranges), max(r[1] for r in depth_ranges)),\n",
    "            'mean': np.mean(depth_values),\n",
    "            'median': np.median(depth_values),\n",
    "            'std': np.std(depth_values),\n",
    "            'skewness': stats.skew(depth_values),\n",
    "            'kurtosis': stats.kurtosis(depth_values)\n",
    "        }\n",
    "\n",
    "    def _degradation_assessment(self):\n",
    "        \"\"\"\n",
    "        Assess potential image degradations\n",
    "        \"\"\"\n",
    "        print(\"Potential Degradation Indicators:\")\n",
    "        \n",
    "        # Sample images\n",
    "        sample_paths = [os.path.join(self.train_img_path, img) for img in os.listdir(self.train_img_path)[:10]]\n",
    "        \n",
    "        noise_levels = []\n",
    "        blur_levels = []\n",
    "\n",
    "        for img_path in sample_paths:\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            # Estimate noise using variance of Laplacian\n",
    "            noise_estimate = cv2.Laplacian(img, cv2.CV_64F).var()\n",
    "            noise_levels.append(noise_estimate)\n",
    "            \n",
    "            # Estimate blur using variance of Laplacian\n",
    "            blur_estimate = cv2.Laplacian(img, cv2.CV_64F).var()\n",
    "            blur_levels.append(blur_estimate)\n",
    "\n",
    "        print(f\"Average Noise Level: {np.mean(noise_levels):.2f}\")\n",
    "        print(f\"Average Blur Level: {np.mean(blur_levels):.2f}\")\n",
    "\n",
    "    def _preprocessing_recommendations(self, image_analysis, depth_analysis):\n",
    "        \"\"\"\n",
    "        Generate preprocessing recommendations\n",
    "        \"\"\"\n",
    "        print(\"Recommended Preprocessing Techniques:\")\n",
    "        \n",
    "        # Resolution handling\n",
    "        if len(image_analysis['resolutions']) > 1:\n",
    "            print(\"- Resize images to a consistent input size (e.g., 256x256)\")\n",
    "        \n",
    "        # Brightness and contrast\n",
    "        if image_analysis['brightness']['std'] > 30:\n",
    "            print(\"- Apply histogram equalization\")\n",
    "        \n",
    "        # Depth map normalization\n",
    "        if depth_analysis['std'] > 100:\n",
    "            print(\"- Normalize depth maps using min-max or z-score normalization\")\n",
    "\n",
    "    def _model_architecture_suggestions(self, image_analysis, depth_analysis):\n",
    "        \"\"\"\n",
    "        Suggest potential model architectures\n",
    "        \"\"\"\n",
    "        print(\"Potential Model Architectures:\")\n",
    "        \n",
    "        # Based on depth map characteristics\n",
    "        if depth_analysis['skewness'] > 1:\n",
    "            print(\"- Consider models with robust loss functions (e.g., Huber loss)\")\n",
    "        \n",
    "        # Based on edge density and image complexity\n",
    "        if image_analysis['edge_density']['mean'] > 0.1:\n",
    "            print(\"- Recommended Models:\")\n",
    "            print(\"  1. U-Net based architectures\")\n",
    "            print(\"  2. DeepLab V3+\")\n",
    "            print(\"  3. Transformer-based models (ViT, Swin Transformer)\")\n",
    "        \n",
    "        print(\"\\nPretrained Model Suggestions from Hugging Face:\")\n",
    "        print(\"1. Facebook/AdaBins - Adaptive Bin-based Depth Estimation\")\n",
    "        print(\"2. Intel/midas-v2_small - MiDaS model for robust depth estimation\")\n",
    "        print(\"3. Salesforce/blip-depth-estimator - BLIP-based depth estimation\")\n",
    "\n",
    "# Usage\n",
    "base_path = '/kaggle/input/depth-estimation/competition-data/competition-data'\n",
    "analyzer = AdvancedDatasetAnalyzer(base_path)\n",
    "analyzer.comprehensive_analysis()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25f68fa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T17:34:36.797730Z",
     "iopub.status.busy": "2025-03-28T17:34:36.797496Z",
     "iopub.status.idle": "2025-03-28T17:38:23.373724Z",
     "shell.execute_reply": "2025-03-28T17:38:23.372803Z"
    },
    "papermill": {
     "duration": 226.580846,
     "end_time": "2025-03-28T17:38:23.375249",
     "exception": false,
     "start_time": "2025-03-28T17:34:36.794403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\r\n",
      "Collecting torch\r\n",
      "  Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\r\n",
      "Collecting torchvision\r\n",
      "  Downloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)\r\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\r\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\r\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\r\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting triton==3.2.0 (from torch)\r\n",
      "  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\r\n",
      "Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\r\n",
      "  Attempting uninstall: nvidia-nccl-cu12\r\n",
      "    Found existing installation: nvidia-nccl-cu12 2.23.4\r\n",
      "    Uninstalling nvidia-nccl-cu12-2.23.4:\r\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\r\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\r\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\r\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\r\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\r\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\r\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\r\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\r\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 2.5.1+cu121\r\n",
      "    Uninstalling torch-2.5.1+cu121:\r\n",
      "      Successfully uninstalled torch-2.5.1+cu121\r\n",
      "  Attempting uninstall: torchvision\r\n",
      "    Found existing installation: torchvision 0.20.1+cu121\r\n",
      "    Uninstalling torchvision-0.20.1+cu121:\r\n",
      "      Successfully uninstalled torchvision-0.20.1+cu121\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 24.10.0 requires pylibraft-cu12==24.10.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 24.10.0 requires rmm-cu12==24.10.*, but you have rmm-cu12 25.2.0 which is incompatible.\r\n",
      "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 torch-2.6.0 torchvision-0.21.0 triton-3.2.0\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\r\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2.4.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.4->pandas) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.22.4->pandas) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.22.4->pandas) (2024.2.0)\r\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\r\n",
      "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.2->opencv-python) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.2->opencv-python) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.2->opencv-python) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.21.2->opencv-python) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.21.2->opencv-python) (2024.2.0)\r\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.5)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\r\n",
      "Requirement already satisfied: numpy<2,>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.9.0.post0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib) (2.4.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2,>=1.20->matplotlib) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2,>=1.20->matplotlib) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2,>=1.20->matplotlib) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2,>=1.20->matplotlib) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2,>=1.20->matplotlib) (2024.2.0)\r\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\r\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\r\n"
     ]
    }
   ],
   "source": [
    "# Pip installs for required libraries\n",
    "!pip install torch torchvision --upgrade  # Core PyTorch and torchvision\n",
    "!pip install numpy  # Numerical operations\n",
    "!pip install pandas  # Data handling\n",
    "!pip install opencv-python  # Image processing (cv2)\n",
    "!pip install matplotlib  # Visualization\n",
    "!pip install scikit-learn  # For train_test_split\n",
    "!pip install pillow  # For PIL (Image handling)\n",
    "!pip install tqdm  # Progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fa9fc43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T17:38:23.469284Z",
     "iopub.status.busy": "2025-03-28T17:38:23.468962Z",
     "iopub.status.idle": "2025-03-28T18:04:25.581746Z",
     "shell.execute_reply": "2025-03-28T18:04:25.580696Z"
    },
    "papermill": {
     "duration": 1562.163463,
     "end_time": "2025-03-28T18:04:25.583189",
     "exception": false,
     "start_time": "2025-03-28T17:38:23.419726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 180MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Batch 1, Train Loss: 2.2195, Train RMSE: 0.2519\n",
      "Batch 101, Train Loss: 1.0675, Train RMSE: 0.1435\n",
      "Batch 201, Train Loss: 0.9956, Train RMSE: 0.1407\n",
      "Batch 301, Train Loss: 1.0661, Train RMSE: 0.1433\n",
      "Batch 401, Train Loss: 0.7143, Train RMSE: 0.1075\n",
      "Batch 501, Train Loss: 1.1815, Train RMSE: 0.1743\n",
      "Batch 601, Train Loss: 0.9847, Train RMSE: 0.1406\n",
      "Batch 701, Train Loss: 0.7462, Train RMSE: 0.1035\n",
      "Batch 801, Train Loss: 0.7801, Train RMSE: 0.1043\n",
      "Train Epoch Loss: 0.9087, Train Epoch RMSE: 0.1260\n",
      "Validation Epoch Loss: 0.8204, Validation Epoch RMSE: 0.1122\n",
      "Saved best model\n",
      "Time taken: 1 mins 33 secs\n",
      "\n",
      "Epoch: 2\n",
      "Batch 1, Train Loss: 0.8715, Train RMSE: 0.1227\n",
      "Batch 101, Train Loss: 0.7083, Train RMSE: 0.1038\n",
      "Batch 201, Train Loss: 0.6617, Train RMSE: 0.0907\n",
      "Batch 301, Train Loss: 1.0369, Train RMSE: 0.1699\n",
      "Batch 401, Train Loss: 0.7463, Train RMSE: 0.1183\n",
      "Batch 501, Train Loss: 0.9757, Train RMSE: 0.1467\n",
      "Batch 601, Train Loss: 0.8242, Train RMSE: 0.1039\n",
      "Batch 701, Train Loss: 0.8376, Train RMSE: 0.1523\n",
      "Batch 801, Train Loss: 0.5749, Train RMSE: 0.0751\n",
      "Train Epoch Loss: 0.8018, Train Epoch RMSE: 0.1142\n",
      "Validation Epoch Loss: 0.7846, Validation Epoch RMSE: 0.1079\n",
      "Saved best model\n",
      "Time taken: 1 mins 1 secs\n",
      "\n",
      "Epoch: 3\n",
      "Batch 1, Train Loss: 0.6900, Train RMSE: 0.0873\n",
      "Batch 101, Train Loss: 0.8802, Train RMSE: 0.1533\n",
      "Batch 201, Train Loss: 0.9652, Train RMSE: 0.1518\n",
      "Batch 301, Train Loss: 0.6787, Train RMSE: 0.0984\n",
      "Batch 401, Train Loss: 0.6816, Train RMSE: 0.0913\n",
      "Batch 501, Train Loss: 0.7898, Train RMSE: 0.1091\n",
      "Batch 601, Train Loss: 0.7179, Train RMSE: 0.1121\n",
      "Batch 701, Train Loss: 0.8042, Train RMSE: 0.1247\n",
      "Batch 801, Train Loss: 0.9250, Train RMSE: 0.1272\n",
      "Train Epoch Loss: 0.7685, Train Epoch RMSE: 0.1098\n",
      "Validation Epoch Loss: 0.7454, Validation Epoch RMSE: 0.1067\n",
      "Saved best model\n",
      "Time taken: 1 mins 0 secs\n",
      "\n",
      "Epoch: 4\n",
      "Batch 1, Train Loss: 0.6600, Train RMSE: 0.1012\n",
      "Batch 101, Train Loss: 0.4953, Train RMSE: 0.0657\n",
      "Batch 201, Train Loss: 0.8762, Train RMSE: 0.1128\n",
      "Batch 301, Train Loss: 1.1450, Train RMSE: 0.1637\n",
      "Batch 401, Train Loss: 0.7370, Train RMSE: 0.1126\n",
      "Batch 501, Train Loss: 0.8106, Train RMSE: 0.1224\n",
      "Batch 601, Train Loss: 0.5871, Train RMSE: 0.0787\n",
      "Batch 701, Train Loss: 0.7813, Train RMSE: 0.1164\n",
      "Batch 801, Train Loss: 0.6116, Train RMSE: 0.0862\n",
      "Train Epoch Loss: 0.7372, Train Epoch RMSE: 0.1061\n",
      "Validation Epoch Loss: 0.7532, Validation Epoch RMSE: 0.1040\n",
      "Time taken: 1 mins 0 secs\n",
      "\n",
      "Epoch: 5\n",
      "Batch 1, Train Loss: 0.5995, Train RMSE: 0.0832\n",
      "Batch 101, Train Loss: 0.8720, Train RMSE: 0.1383\n",
      "Batch 201, Train Loss: 0.6519, Train RMSE: 0.0911\n",
      "Batch 301, Train Loss: 0.9788, Train RMSE: 0.1481\n",
      "Batch 401, Train Loss: 0.9917, Train RMSE: 0.1431\n",
      "Batch 501, Train Loss: 0.6822, Train RMSE: 0.0916\n",
      "Batch 601, Train Loss: 0.4667, Train RMSE: 0.0626\n",
      "Batch 701, Train Loss: 0.6087, Train RMSE: 0.0883\n",
      "Batch 801, Train Loss: 0.5262, Train RMSE: 0.0769\n",
      "Train Epoch Loss: 0.7131, Train Epoch RMSE: 0.1030\n",
      "Validation Epoch Loss: 0.6897, Validation Epoch RMSE: 0.0966\n",
      "Saved best model\n",
      "Time taken: 1 mins 0 secs\n",
      "\n",
      "Epoch: 6\n",
      "Batch 1, Train Loss: 0.8520, Train RMSE: 0.1299\n",
      "Batch 101, Train Loss: 0.5134, Train RMSE: 0.0708\n",
      "Batch 201, Train Loss: 0.8738, Train RMSE: 0.1078\n",
      "Batch 301, Train Loss: 0.9151, Train RMSE: 0.1289\n",
      "Batch 401, Train Loss: 0.4822, Train RMSE: 0.0682\n",
      "Batch 501, Train Loss: 0.7850, Train RMSE: 0.1128\n",
      "Batch 601, Train Loss: 0.7998, Train RMSE: 0.1099\n",
      "Batch 701, Train Loss: 0.5991, Train RMSE: 0.0843\n",
      "Batch 801, Train Loss: 0.4199, Train RMSE: 0.0576\n",
      "Train Epoch Loss: 0.6902, Train Epoch RMSE: 0.0996\n",
      "Validation Epoch Loss: 0.6667, Validation Epoch RMSE: 0.0943\n",
      "Saved best model\n",
      "Time taken: 1 mins 0 secs\n",
      "\n",
      "Epoch: 7\n",
      "Batch 1, Train Loss: 0.6013, Train RMSE: 0.0987\n",
      "Batch 101, Train Loss: 0.5363, Train RMSE: 0.0819\n",
      "Batch 201, Train Loss: 0.5241, Train RMSE: 0.0787\n",
      "Batch 301, Train Loss: 0.6529, Train RMSE: 0.1109\n",
      "Batch 401, Train Loss: 0.5136, Train RMSE: 0.0753\n",
      "Batch 501, Train Loss: 0.5533, Train RMSE: 0.0747\n",
      "Batch 601, Train Loss: 0.8691, Train RMSE: 0.1153\n",
      "Batch 701, Train Loss: 0.5124, Train RMSE: 0.0737\n",
      "Batch 801, Train Loss: 0.9217, Train RMSE: 0.1216\n",
      "Train Epoch Loss: 0.6658, Train Epoch RMSE: 0.0971\n",
      "Validation Epoch Loss: 0.6545, Validation Epoch RMSE: 0.0930\n",
      "Saved best model\n",
      "Time taken: 1 mins 0 secs\n",
      "\n",
      "Epoch: 8\n",
      "Batch 1, Train Loss: 0.7650, Train RMSE: 0.1009\n",
      "Batch 101, Train Loss: 0.8488, Train RMSE: 0.1425\n",
      "Batch 201, Train Loss: 0.6785, Train RMSE: 0.0940\n",
      "Batch 301, Train Loss: 0.7788, Train RMSE: 0.1109\n",
      "Batch 401, Train Loss: 0.6366, Train RMSE: 0.1053\n",
      "Batch 501, Train Loss: 0.6746, Train RMSE: 0.0943\n",
      "Batch 601, Train Loss: 0.9369, Train RMSE: 0.1518\n",
      "Batch 701, Train Loss: 0.7505, Train RMSE: 0.1368\n",
      "Batch 801, Train Loss: 0.4023, Train RMSE: 0.0539\n",
      "Train Epoch Loss: 0.6421, Train Epoch RMSE: 0.0939\n",
      "Validation Epoch Loss: 0.6073, Validation Epoch RMSE: 0.0868\n",
      "Saved best model\n",
      "Time taken: 1 mins 0 secs\n",
      "\n",
      "Epoch: 9\n",
      "Batch 1, Train Loss: 0.4748, Train RMSE: 0.0729\n",
      "Batch 101, Train Loss: 0.6441, Train RMSE: 0.0971\n",
      "Batch 201, Train Loss: 0.4433, Train RMSE: 0.0664\n",
      "Batch 301, Train Loss: 0.6941, Train RMSE: 0.0992\n",
      "Batch 401, Train Loss: 0.7515, Train RMSE: 0.1328\n",
      "Batch 501, Train Loss: 0.7100, Train RMSE: 0.1094\n",
      "Batch 601, Train Loss: 0.6252, Train RMSE: 0.0964\n",
      "Batch 701, Train Loss: 0.7608, Train RMSE: 0.1210\n",
      "Batch 801, Train Loss: 0.6688, Train RMSE: 0.0901\n",
      "Train Epoch Loss: 0.6210, Train Epoch RMSE: 0.0913\n",
      "Validation Epoch Loss: 0.8520, Validation Epoch RMSE: 0.1182\n",
      "Time taken: 1 mins 0 secs\n",
      "\n",
      "Epoch: 10\n",
      "Batch 1, Train Loss: 0.5231, Train RMSE: 0.0783\n",
      "Batch 101, Train Loss: 0.7488, Train RMSE: 0.1128\n",
      "Batch 201, Train Loss: 0.5063, Train RMSE: 0.0771\n",
      "Batch 301, Train Loss: 0.4626, Train RMSE: 0.0754\n",
      "Batch 401, Train Loss: 0.5052, Train RMSE: 0.0816\n",
      "Batch 501, Train Loss: 0.7418, Train RMSE: 0.1029\n",
      "Batch 601, Train Loss: 0.4360, Train RMSE: 0.0675\n",
      "Batch 701, Train Loss: 0.5683, Train RMSE: 0.0859\n",
      "Batch 801, Train Loss: 0.5122, Train RMSE: 0.0773\n",
      "Train Epoch Loss: 0.6020, Train Epoch RMSE: 0.0891\n",
      "Validation Epoch Loss: 0.9104, Validation Epoch RMSE: 0.1279\n",
      "Time taken: 1 mins 0 secs\n",
      "\n",
      "Epoch: 11\n",
      "Batch 1, Train Loss: 0.4718, Train RMSE: 0.0696\n",
      "Batch 101, Train Loss: 0.5434, Train RMSE: 0.0793\n",
      "Batch 201, Train Loss: 0.4663, Train RMSE: 0.0661\n",
      "Batch 301, Train Loss: 0.4311, Train RMSE: 0.0639\n",
      "Batch 401, Train Loss: 0.5254, Train RMSE: 0.0746\n",
      "Batch 501, Train Loss: 0.3729, Train RMSE: 0.0527\n",
      "Batch 601, Train Loss: 0.6536, Train RMSE: 0.0935\n",
      "Batch 701, Train Loss: 0.8913, Train RMSE: 0.1478\n",
      "Batch 801, Train Loss: 0.5991, Train RMSE: 0.0912\n",
      "Train Epoch Loss: 0.5840, Train Epoch RMSE: 0.0866\n",
      "Validation Epoch Loss: 0.6624, Validation Epoch RMSE: 0.0949\n",
      "Time taken: 1 mins 0 secs\n",
      "\n",
      "Epoch: 12\n",
      "Batch 1, Train Loss: 0.5226, Train RMSE: 0.0753\n",
      "Batch 101, Train Loss: 0.5020, Train RMSE: 0.0715\n",
      "Batch 201, Train Loss: 0.6166, Train RMSE: 0.0929\n",
      "Batch 301, Train Loss: 0.5392, Train RMSE: 0.0761\n",
      "Batch 401, Train Loss: 0.5180, Train RMSE: 0.0733\n",
      "Batch 501, Train Loss: 0.5822, Train RMSE: 0.0787\n",
      "Batch 601, Train Loss: 0.6287, Train RMSE: 0.0880\n",
      "Batch 701, Train Loss: 0.5972, Train RMSE: 0.0852\n",
      "Batch 801, Train Loss: 0.3818, Train RMSE: 0.0577\n",
      "Train Epoch Loss: 0.5692, Train Epoch RMSE: 0.0851\n",
      "Validation Epoch Loss: 0.8687, Validation Epoch RMSE: 0.1243\n",
      "Time taken: 1 mins 0 secs\n",
      "\n",
      "Epoch: 13\n",
      "Batch 1, Train Loss: 0.6259, Train RMSE: 0.1069\n",
      "Batch 101, Train Loss: 0.4502, Train RMSE: 0.0782\n",
      "Batch 201, Train Loss: 0.4347, Train RMSE: 0.0674\n",
      "Batch 301, Train Loss: 0.4036, Train RMSE: 0.0615\n",
      "Batch 401, Train Loss: 0.6053, Train RMSE: 0.0863\n",
      "Batch 501, Train Loss: 0.4359, Train RMSE: 0.0714\n",
      "Batch 601, Train Loss: 0.5750, Train RMSE: 0.0821\n",
      "Batch 701, Train Loss: 0.3327, Train RMSE: 0.0486\n",
      "Batch 801, Train Loss: 0.5288, Train RMSE: 0.0811\n",
      "Train Epoch Loss: 0.5246, Train Epoch RMSE: 0.0800\n",
      "Validation Epoch Loss: 0.5318, Validation Epoch RMSE: 0.0787\n",
      "Saved best model\n",
      "Time taken: 1 mins 0 secs\n",
      "\n",
      "Epoch: 14\n",
      "Batch 1, Train Loss: 0.4858, Train RMSE: 0.0731\n",
      "Batch 101, Train Loss: 0.4267, Train RMSE: 0.0613\n",
      "Batch 201, Train Loss: 0.7096, Train RMSE: 0.0959\n",
      "Batch 301, Train Loss: 0.3677, Train RMSE: 0.0562\n",
      "Batch 401, Train Loss: 0.5275, Train RMSE: 0.0780\n",
      "Batch 501, Train Loss: 0.5006, Train RMSE: 0.0948\n",
      "Batch 601, Train Loss: 0.3145, Train RMSE: 0.0505\n",
      "Batch 701, Train Loss: 0.6297, Train RMSE: 0.0969\n",
      "Batch 801, Train Loss: 0.4733, Train RMSE: 0.0719\n",
      "Train Epoch Loss: 0.5073, Train Epoch RMSE: 0.0777\n",
      "Validation Epoch Loss: 0.5027, Validation Epoch RMSE: 0.0752\n",
      "Saved best model\n",
      "Time taken: 1 mins 0 secs\n",
      "\n",
      "Epoch: 15\n",
      "Batch 1, Train Loss: 0.5613, Train RMSE: 0.1040\n",
      "Batch 101, Train Loss: 0.5099, Train RMSE: 0.0914\n",
      "Batch 201, Train Loss: 0.6140, Train RMSE: 0.0897\n",
      "Batch 301, Train Loss: 0.3661, Train RMSE: 0.0620\n",
      "Batch 401, Train Loss: 0.5211, Train RMSE: 0.0725\n",
      "Batch 501, Train Loss: 0.4997, Train RMSE: 0.0754\n",
      "Batch 601, Train Loss: 0.3579, Train RMSE: 0.0487\n",
      "Batch 701, Train Loss: 0.7470, Train RMSE: 0.1202\n",
      "Batch 801, Train Loss: 0.5223, Train RMSE: 0.0831\n",
      "Train Epoch Loss: 0.4990, Train Epoch RMSE: 0.0765\n",
      "Validation Epoch Loss: 0.6181, Validation Epoch RMSE: 0.0897\n",
      "Time taken: 1 mins 0 secs\n",
      "\n",
      "Epoch: 16\n",
      "Batch 1, Train Loss: 0.2923, Train RMSE: 0.0410\n",
      "Batch 101, Train Loss: 0.4380, Train RMSE: 0.0632\n",
      "Batch 201, Train Loss: 0.4017, Train RMSE: 0.0643\n",
      "Batch 301, Train Loss: 0.2955, Train RMSE: 0.0427\n",
      "Batch 401, Train Loss: 0.5077, Train RMSE: 0.0797\n",
      "Batch 501, Train Loss: 0.6034, Train RMSE: 0.0848\n",
      "Batch 601, Train Loss: 0.4621, Train RMSE: 0.0807\n",
      "Batch 701, Train Loss: 0.3571, Train RMSE: 0.0552\n",
      "Batch 801, Train Loss: 0.6004, Train RMSE: 0.0828\n",
      "Train Epoch Loss: 0.4878, Train Epoch RMSE: 0.0752\n",
      "Validation Epoch Loss: 0.7591, Validation Epoch RMSE: 0.1141\n",
      "Time taken: 1 mins 0 secs\n",
      "\n",
      "Epoch: 17\n",
      "Batch 1, Train Loss: 0.3965, Train RMSE: 0.0600\n",
      "Batch 101, Train Loss: 0.4398, Train RMSE: 0.0628\n",
      "Batch 201, Train Loss: 0.5041, Train RMSE: 0.0722\n",
      "Batch 301, Train Loss: 0.3841, Train RMSE: 0.0553\n",
      "Batch 401, Train Loss: 0.5553, Train RMSE: 0.0764\n",
      "Batch 501, Train Loss: 0.4834, Train RMSE: 0.0733\n",
      "Batch 601, Train Loss: 0.3954, Train RMSE: 0.0553\n",
      "Batch 701, Train Loss: 0.3866, Train RMSE: 0.0570\n",
      "Batch 801, Train Loss: 0.3831, Train RMSE: 0.0580\n",
      "Train Epoch Loss: 0.4775, Train Epoch RMSE: 0.0738\n",
      "Validation Epoch Loss: 0.4699, Validation Epoch RMSE: 0.0708\n",
      "Saved best model\n",
      "Time taken: 1 mins 0 secs\n",
      "\n",
      "Epoch: 18\n",
      "Batch 1, Train Loss: 0.3357, Train RMSE: 0.0545\n",
      "Batch 101, Train Loss: 0.4884, Train RMSE: 0.0703\n",
      "Batch 201, Train Loss: 0.3182, Train RMSE: 0.0522\n",
      "Batch 301, Train Loss: 0.3170, Train RMSE: 0.0486\n",
      "Batch 401, Train Loss: 0.4118, Train RMSE: 0.0613\n",
      "Batch 501, Train Loss: 0.3129, Train RMSE: 0.0474\n",
      "Batch 601, Train Loss: 0.6390, Train RMSE: 0.0997\n",
      "Batch 701, Train Loss: 0.6453, Train RMSE: 0.1034\n",
      "Batch 801, Train Loss: 0.5679, Train RMSE: 0.0822\n",
      "Train Epoch Loss: 0.4709, Train Epoch RMSE: 0.0731\n",
      "Validation Epoch Loss: 0.4912, Validation Epoch RMSE: 0.0739\n",
      "Time taken: 1 mins 0 secs\n",
      "\n",
      "Epoch: 19\n",
      "Batch 1, Train Loss: 0.3531, Train RMSE: 0.0558\n",
      "Batch 101, Train Loss: 0.3726, Train RMSE: 0.0568\n",
      "Batch 201, Train Loss: 0.4378, Train RMSE: 0.0589\n",
      "Batch 301, Train Loss: 0.3333, Train RMSE: 0.0505\n",
      "Batch 401, Train Loss: 0.5485, Train RMSE: 0.0881\n",
      "Batch 501, Train Loss: 0.5796, Train RMSE: 0.0910\n",
      "Batch 601, Train Loss: 0.4046, Train RMSE: 0.0582\n",
      "Batch 701, Train Loss: 0.2830, Train RMSE: 0.0441\n",
      "Batch 801, Train Loss: 0.4722, Train RMSE: 0.0769\n",
      "Train Epoch Loss: 0.4636, Train Epoch RMSE: 0.0723\n",
      "Validation Epoch Loss: 0.8736, Validation Epoch RMSE: 0.1335\n",
      "Time taken: 1 mins 0 secs\n",
      "\n",
      "Epoch: 20\n",
      "Batch 1, Train Loss: 0.3321, Train RMSE: 0.0500\n",
      "Batch 101, Train Loss: 0.3545, Train RMSE: 0.0471\n",
      "Batch 201, Train Loss: 0.2958, Train RMSE: 0.0431\n",
      "Batch 301, Train Loss: 0.3023, Train RMSE: 0.0436\n",
      "Batch 401, Train Loss: 0.4960, Train RMSE: 0.0820\n",
      "Batch 501, Train Loss: 0.4474, Train RMSE: 0.0694\n",
      "Batch 601, Train Loss: 0.6209, Train RMSE: 0.1381\n",
      "Batch 701, Train Loss: 0.5149, Train RMSE: 0.0776\n",
      "Batch 801, Train Loss: 0.5152, Train RMSE: 0.0913\n",
      "Train Epoch Loss: 0.4566, Train Epoch RMSE: 0.0710\n",
      "Validation Epoch Loss: 0.4643, Validation Epoch RMSE: 0.0702\n",
      "Saved best model\n",
      "Time taken: 1 mins 0 secs\n",
      "\n",
      "Epoch: 21\n",
      "Batch 1, Train Loss: 0.4195, Train RMSE: 0.0767\n",
      "Batch 101, Train Loss: 0.4375, Train RMSE: 0.0718\n",
      "Batch 201, Train Loss: 0.3707, Train RMSE: 0.0541\n",
      "Batch 301, Train Loss: 0.2882, Train RMSE: 0.0428\n",
      "Batch 401, Train Loss: 0.3932, Train RMSE: 0.0590\n",
      "Batch 501, Train Loss: 0.4885, Train RMSE: 0.0711\n",
      "Batch 601, Train Loss: 0.4037, Train RMSE: 0.0601\n",
      "Batch 701, Train Loss: 0.4688, Train RMSE: 0.0634\n",
      "Batch 801, Train Loss: 0.4759, Train RMSE: 0.0746\n",
      "Train Epoch Loss: 0.4466, Train Epoch RMSE: 0.0699\n",
      "Validation Epoch Loss: 0.4665, Validation Epoch RMSE: 0.0704\n",
      "Time taken: 1 mins 0 secs\n",
      "\n",
      "Epoch: 22\n",
      "Batch 1, Train Loss: 0.7238, Train RMSE: 0.1094\n",
      "Batch 101, Train Loss: 0.3449, Train RMSE: 0.0490\n",
      "Batch 201, Train Loss: 0.3260, Train RMSE: 0.0491\n",
      "Batch 301, Train Loss: 0.3137, Train RMSE: 0.0438\n",
      "Batch 401, Train Loss: 0.3826, Train RMSE: 0.0592\n",
      "Batch 501, Train Loss: 0.4551, Train RMSE: 0.0853\n",
      "Batch 601, Train Loss: 0.4102, Train RMSE: 0.0621\n",
      "Batch 701, Train Loss: 0.3474, Train RMSE: 0.0516\n",
      "Batch 801, Train Loss: 0.4346, Train RMSE: 0.0690\n",
      "Train Epoch Loss: 0.4393, Train Epoch RMSE: 0.0688\n",
      "Validation Epoch Loss: 0.4976, Validation Epoch RMSE: 0.0752\n",
      "Time taken: 1 mins 0 secs\n",
      "\n",
      "Epoch: 23\n",
      "Batch 1, Train Loss: 0.4975, Train RMSE: 0.0722\n",
      "Batch 101, Train Loss: 0.4101, Train RMSE: 0.0557\n",
      "Batch 201, Train Loss: 0.3924, Train RMSE: 0.0557\n",
      "Batch 301, Train Loss: 0.4476, Train RMSE: 0.0831\n",
      "Batch 401, Train Loss: 0.4805, Train RMSE: 0.0715\n",
      "Batch 501, Train Loss: 0.3226, Train RMSE: 0.0490\n",
      "Batch 601, Train Loss: 0.5784, Train RMSE: 0.0848\n",
      "Batch 701, Train Loss: 0.4316, Train RMSE: 0.0669\n",
      "Batch 801, Train Loss: 0.3413, Train RMSE: 0.0511\n",
      "Train Epoch Loss: 0.4348, Train Epoch RMSE: 0.0681\n",
      "Validation Epoch Loss: 0.4510, Validation Epoch RMSE: 0.0685\n",
      "Saved best model\n",
      "Time taken: 1 mins 0 secs\n",
      "\n",
      "Epoch: 24\n",
      "Batch 1, Train Loss: 0.5599, Train RMSE: 0.1030\n",
      "Batch 101, Train Loss: 0.3019, Train RMSE: 0.0430\n",
      "Batch 201, Train Loss: 0.4992, Train RMSE: 0.0767\n",
      "Batch 301, Train Loss: 0.3120, Train RMSE: 0.0480\n",
      "Batch 401, Train Loss: 0.5029, Train RMSE: 0.0838\n",
      "Batch 501, Train Loss: 0.5024, Train RMSE: 0.0716\n",
      "Batch 601, Train Loss: 0.4489, Train RMSE: 0.0716\n",
      "Batch 701, Train Loss: 0.3723, Train RMSE: 0.0573\n",
      "Batch 801, Train Loss: 0.5732, Train RMSE: 0.0874\n",
      "Train Epoch Loss: 0.4296, Train Epoch RMSE: 0.0674\n",
      "Validation Epoch Loss: 0.5279, Validation Epoch RMSE: 0.0786\n",
      "Time taken: 1 mins 0 secs\n",
      "\n",
      "Epoch: 25\n",
      "Batch 1, Train Loss: 0.5012, Train RMSE: 0.0749\n",
      "Batch 101, Train Loss: 0.5655, Train RMSE: 0.0811\n",
      "Batch 201, Train Loss: 0.5363, Train RMSE: 0.0869\n",
      "Batch 301, Train Loss: 0.3205, Train RMSE: 0.0459\n",
      "Batch 401, Train Loss: 0.5176, Train RMSE: 0.0914\n",
      "Batch 501, Train Loss: 0.3867, Train RMSE: 0.0681\n",
      "Batch 601, Train Loss: 0.3679, Train RMSE: 0.0664\n",
      "Batch 701, Train Loss: 0.4828, Train RMSE: 0.0910\n",
      "Batch 801, Train Loss: 0.4352, Train RMSE: 0.0700\n",
      "Train Epoch Loss: 0.4235, Train Epoch RMSE: 0.0669\n",
      "Validation Epoch Loss: 0.5539, Validation Epoch RMSE: 0.0845\n",
      "Time taken: 1 mins 0 secs\n",
      "\n",
      "Training Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Predictions: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No images found in folder: predictions\n",
      "Submission CSV created: submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import glob\n",
    "\n",
    "# ----------------- Model and Dataset Definition -----------------\n",
    "class AttentionGate(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(AttentionGate, self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.GroupNorm(16, F_int)\n",
    "        )\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.GroupNorm(16, F_int)\n",
    "        )\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.GroupNorm(1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        # Upsample x to match spatial dims of g1\n",
    "        x = F.interpolate(x, size=g1.shape[2:], mode='bilinear', align_corners=False)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "class EnhancedUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnhancedUNet, self).__init__()\n",
    "        \n",
    "        # Pre-trained ResNet18 Encoder\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.enc1 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu)  # Removed maxpool for higher resolution\n",
    "        self.enc2 = resnet.layer1  # 64x64x64 -> 64x64x64\n",
    "        self.enc3 = resnet.layer2  # 64x64x64 -> 32x32x128\n",
    "        self.enc4 = resnet.layer3  # 32x32x128 -> 16x16x256\n",
    "        \n",
    "        # Bottleneck with Dilated Convolution\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 3, padding=2, dilation=2),\n",
    "            nn.GroupNorm(16, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n",
    "            nn.GroupNorm(16, 512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Decoder with Attention Gates\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.att3 = AttentionGate(F_g=256, F_l=128, F_int=128)\n",
    "        self.dec3 = self._conv_block(384, 256)  # upconv3 (256) + att3 (128) = 384\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.att2 = AttentionGate(F_g=128, F_l=64, F_int=64)\n",
    "        self.dec2 = self._conv_block(192, 128)  # upconv2 (128) + att2 (64) = 192\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.att1 = AttentionGate(F_g=64, F_l=64, F_int=32)\n",
    "        self.dec1 = self._conv_block(128, 64)   # upconv1 (64) + att1 (64) = 128\n",
    "        \n",
    "        # Output Layer\n",
    "        self.out = nn.Conv2d(64, 1, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def _conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.GroupNorm(16, out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.GroupNorm(16, out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.enc1(x)    # e.g., 64x64\n",
    "        enc2 = self.enc2(enc1) # 64x64\n",
    "        enc3 = self.enc3(enc2) # 32x32\n",
    "        enc4 = self.enc4(enc3) # 16x16\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(enc4)  # 16x16x512\n",
    "        \n",
    "        # Decoder with Attention\n",
    "        up3 = self.upconv3(bottleneck)  # 32x32x256\n",
    "        att3 = self.att3(g=up3, x=enc3)  # 32x32x128\n",
    "        dec3 = self.dec3(torch.cat([up3, att3], dim=1))  # 32x32x384 -> 32x32x256\n",
    "        \n",
    "        up2 = self.upconv2(dec3)  # 64x64x128\n",
    "        att2 = self.att2(g=up2, x=enc2)  # 64x64x64\n",
    "        dec2 = self.dec2(torch.cat([up2, att2], dim=1))  # 64x64x192 -> 64x64x128\n",
    "        \n",
    "        up1 = self.upconv1(dec2)  # 128x128x64\n",
    "        att1 = self.att1(g=up1, x=enc1)  # 128x128x64\n",
    "        dec1 = self.dec1(torch.cat([up1, att1], dim=1))  # 128x128x128 -> 128x128x64\n",
    "        \n",
    "        # Output\n",
    "        out = self.out(dec1)  # 128x128x1\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# Dataset with Augmentation\n",
    "class CompetitionDataset(Dataset):\n",
    "    def __init__(self, dataframe, is_train=True):\n",
    "        self.dataframe = dataframe\n",
    "        self.is_train = is_train\n",
    "        if is_train:\n",
    "            self.aug_transform = transforms.Compose([\n",
    "                transforms.ColorJitter(brightness=0.5, contrast=0.5),\n",
    "                transforms.Resize((128, 128)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.05),  # Gaussian noise\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.aug_transform = transforms.Compose([\n",
    "                transforms.Resize((128, 128)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        self.depth_transform = transforms.Compose([\n",
    "            transforms.Resize((128, 128)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        rgb_path = self.dataframe.iloc[idx]['rgb']\n",
    "        depth_path = self.dataframe.iloc[idx]['depth']\n",
    "        rgb_image = Image.open(rgb_path).convert('RGB')\n",
    "        depth_image = Image.open(depth_path).convert('L')\n",
    "        rgb_image = self.aug_transform(rgb_image)\n",
    "        depth_image = self.depth_transform(depth_image)\n",
    "        return rgb_image, depth_image\n",
    "\n",
    "# ----------------- Data Preparation -----------------\n",
    "BASE_PATH = \"/kaggle/input/depth-estimation/competition-data/competition-data\"\n",
    "train_rgb_files = sorted(glob.glob(os.path.join(BASE_PATH, \"training/images/*.png\")))\n",
    "train_depth_files = sorted(glob.glob(os.path.join(BASE_PATH, \"training/depths/*.png\")))\n",
    "df_train = pd.DataFrame({'rgb': train_rgb_files, 'depth': train_depth_files})\n",
    "\n",
    "val_rgb_files = sorted(glob.glob(os.path.join(BASE_PATH, \"validation/images/*.png\")))\n",
    "val_depth_files = sorted(glob.glob(os.path.join(BASE_PATH, \"validation/depths/*.png\")))\n",
    "df_val = pd.DataFrame({'rgb': val_rgb_files, 'depth': val_depth_files})\n",
    "\n",
    "train_dataset = CompetitionDataset(df_train, is_train=True)\n",
    "val_dataset = CompetitionDataset(df_val, is_train=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
    "\n",
    "# ----------------- Model, Loss, Optimizer, Scheduler -----------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = EnhancedUNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "def combined_loss(y_pred, y_true):\n",
    "    l1_loss = torch.mean(torch.abs(y_pred - y_true))\n",
    "    mse_loss = torch.mean((y_pred - y_true) ** 2)\n",
    "    return 10.0 * l1_loss + 1.0 * mse_loss\n",
    "\n",
    "def compute_rmse(y_pred, y_true):\n",
    "    return torch.sqrt(torch.mean((y_pred - y_true) ** 2))\n",
    "\n",
    "# ----------------- Training and Validation Functions -----------------\n",
    "def train(model, dataset, optimizer, device, scaler):\n",
    "    model.train()\n",
    "    train_loss_batch = []\n",
    "    train_rmse_batch = []\n",
    "    for idx, (images, labels) in enumerate(dataset):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            output = model(images)\n",
    "            loss = combined_loss(output, labels)\n",
    "            rmse = compute_rmse(output, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        train_loss_batch.append(loss.item())\n",
    "        train_rmse_batch.append(rmse.item())\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Batch {idx + 1}, Train Loss: {loss.item():.4f}, Train RMSE: {rmse.item():.4f}\")\n",
    "    epoch_loss = sum(train_loss_batch) / len(dataset)\n",
    "    epoch_rmse = sum(train_rmse_batch) / len(dataset)\n",
    "    print(f\"Train Epoch Loss: {epoch_loss:.4f}, Train Epoch RMSE: {epoch_rmse:.4f}\")\n",
    "    return epoch_loss, epoch_rmse\n",
    "\n",
    "def validate(model, dataset, device):\n",
    "    model.eval()\n",
    "    val_loss_batch = []\n",
    "    val_rmse_batch = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (images, labels) in enumerate(dataset):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = model(images)\n",
    "            loss = combined_loss(output, labels)\n",
    "            rmse = compute_rmse(output, labels)\n",
    "            val_loss_batch.append(loss.item())\n",
    "            val_rmse_batch.append(rmse.item())\n",
    "            if idx % 50 == 0:\n",
    "                one_width = output.shape[3]\n",
    "                total_width = one_width * 3\n",
    "                new_image = Image.new('RGB', (total_width, output.shape[2]))\n",
    "                img_np = images[0].permute(1, 2, 0).detach().cpu().numpy()\n",
    "                label_np = labels[0].repeat(3, 1, 1).permute(1, 2, 0).detach().cpu().numpy()\n",
    "                out_np = output[0].repeat(3, 1, 1).permute(1, 2, 0).detach().cpu().numpy()\n",
    "                img_np = (img_np * np.array([0.229, 0.224, 0.225]) +\n",
    "                          np.array([0.485, 0.456, 0.406])) * 255\n",
    "                new_image.paste(Image.fromarray(np.uint8(img_np.clip(0, 255))), (0, 0))\n",
    "                new_image.paste(Image.fromarray(np.uint8(label_np * 255)), (one_width, 0))\n",
    "                new_image.paste(Image.fromarray(np.uint8(out_np * 255)), (one_width * 2, 0))\n",
    "                new_image.save(f'val_image_{idx}.png')\n",
    "    epoch_loss = sum(val_loss_batch) / len(dataset)\n",
    "    epoch_rmse = sum(val_rmse_batch) / len(dataset)\n",
    "    print(f\"Validation Epoch Loss: {epoch_loss:.4f}, Validation Epoch RMSE: {epoch_rmse:.4f}\")\n",
    "    return epoch_loss, epoch_rmse\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time  # Correct variable name\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "# ----------------- Training Loop -----------------\n",
    "num_epochs = 25\n",
    "best_val_loss = float('inf')\n",
    "scaler = GradScaler()\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.monotonic()\n",
    "    print(f\"Epoch: {epoch + 1}\")\n",
    "    \n",
    "    train_loss, train_rmse = train(model, train_loader, optimizer, device, scaler)\n",
    "    val_loss, val_rmse = validate(model, val_loader, device)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': val_loss\n",
    "        }, 'best_model.pt')\n",
    "        print(\"Saved best model\")\n",
    "    \n",
    "    end_time = time.monotonic()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    print(f\"Time taken: {epoch_mins} mins {epoch_secs} secs\\n\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"Training Complete\")\n",
    "\n",
    "# ----------------- Test Prediction and Submission CSV Generation -----------------\n",
    "# Define a simple test transform (similar to validation)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Assume test images are stored in BASE_PATH/test/images/\n",
    "test_img_dir = os.path.join(BASE_PATH, \"test/images\")\n",
    "test_img_files = sorted(glob.glob(os.path.join(test_img_dir, \"*.png\")))\n",
    "\n",
    "# Folder to store prediction images\n",
    "predictions_folder = \"predictions\"\n",
    "os.makedirs(predictions_folder, exist_ok=True)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, img_path in enumerate(tqdm(test_img_files, desc=\"Generating Predictions\")):\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        input_img = test_transform(image).unsqueeze(0).to(device)\n",
    "        output = model(input_img)\n",
    "        # The output is [0,1] due to sigmoid; convert to uint8 image\n",
    "        pred = output.squeeze().detach().cpu().numpy()\n",
    "        pred = (pred * 255).astype(np.uint8)\n",
    "        # Save predicted image\n",
    "        filename = os.path.basename(img_path)\n",
    "        cv2.imwrite(os.path.join(predictions_folder, filename), pred)\n",
    "\n",
    "# Function to convert images to CSV with metadata (integrated from imgs2csv.py)\n",
    "def images_to_csv_with_metadata(image_folder, output_csv):\n",
    "    data = []\n",
    "    for idx, filename in enumerate(sorted(os.listdir(image_folder))):\n",
    "        if filename.endswith(\".png\"):\n",
    "            filepath = os.path.join(image_folder, filename)\n",
    "            image = cv2.imread(filepath, cv2.IMREAD_UNCHANGED)\n",
    "            image = cv2.resize(image, (128, 128))\n",
    "            image = image / 255.\n",
    "            image = (image - np.min(image)) / (np.max(image) - np.min(image) + 1e-6)\n",
    "            image = np.uint8(image * 255.)\n",
    "            image_flat = image.flatten()\n",
    "            row = [idx, filename] + image_flat.tolist()\n",
    "            data.append(row)\n",
    "    if data:\n",
    "        num_columns = len(data[0]) - 2\n",
    "        column_names = [\"id\", \"ImageID\"] + [i for i in range(num_columns)]\n",
    "        df = pd.DataFrame(data, columns=column_names)\n",
    "        df.to_csv(output_csv, index=False)\n",
    "    else:\n",
    "        print(\"No images found in folder:\", image_folder)\n",
    "\n",
    "# Create submission CSV file\n",
    "submission_csv = \"submission.csv\"\n",
    "images_to_csv_with_metadata(predictions_folder, submission_csv)\n",
    "print(\"Submission CSV created:\", submission_csv)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11466546,
     "sourceId": 96480,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1795.133118,
   "end_time": "2025-03-28T18:04:28.262791",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-28T17:34:33.129673",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
