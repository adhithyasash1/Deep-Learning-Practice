{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5649001d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T13:01:24.565887Z",
     "iopub.status.busy": "2025-03-29T13:01:24.565550Z",
     "iopub.status.idle": "2025-03-29T13:01:30.597867Z",
     "shell.execute_reply": "2025-03-29T13:01:30.596786Z"
    },
    "papermill": {
     "duration": 6.036908,
     "end_time": "2025-03-29T13:01:30.599316",
     "exception": false,
     "start_time": "2025-03-29T13:01:24.562408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\r\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.5)\r\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\r\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\r\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\r\n",
      "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\r\n",
      "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\r\n",
      "Collecting pytorch-msssim\r\n",
      "  Downloading pytorch_msssim-1.0.0-py3-none-any.whl.metadata (8.0 kB)\r\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\r\n",
      "Collecting torch_optimizer\r\n",
      "  Downloading torch_optimizer-0.3.0-py3-none-any.whl.metadata (55 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.9.0.post0)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\r\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\r\n",
      "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.11.0a2)\r\n",
      "Requirement already satisfied: albucore==0.0.19 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.19)\r\n",
      "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\r\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\r\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.19->albumentations) (3.11.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\r\n",
      "Collecting pytorch-ranger>=0.1.1 (from torch_optimizer)\r\n",
      "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl.metadata (509 bytes)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.29.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\r\n",
      "Downloading pytorch_msssim-1.0.0-py3-none-any.whl (7.7 kB)\r\n",
      "Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\r\n",
      "Installing collected packages: pytorch-ranger, pytorch-msssim, torch_optimizer\r\n",
      "Successfully installed pytorch-msssim-1.0.0 pytorch-ranger-0.1.1 torch_optimizer-0.3.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python matplotlib scikit-learn pillow tqdm torch torchvision opencv-contrib-python albumentations pytorch-msssim transformers torch_optimizer pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bce97b3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T13:01:30.606203Z",
     "iopub.status.busy": "2025-03-29T13:01:30.605924Z",
     "iopub.status.idle": "2025-03-29T13:01:53.920065Z",
     "shell.execute_reply": "2025-03-29T13:01:53.919001Z"
    },
    "papermill": {
     "duration": 23.318974,
     "end_time": "2025-03-29T13:01:53.921335",
     "exception": false,
     "start_time": "2025-03-29T13:01:30.602361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "Epoch 1/20 [Train]:   0%|          | 0/835 [00:02<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in main function: Given groups=8, weight of size [8, 1, 3, 3], expected input[8, 1, 256, 256] to have 8 channels, but got 1 channels instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from pytorch_msssim import SSIM\n",
    "import random\n",
    "import warnings\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "TRAIN_RGB_DIR = \"/kaggle/input/depth-estimation/competition-data/competition-data/training/images\"\n",
    "TRAIN_DEPTH_DIR = \"/kaggle/input/depth-estimation/competition-data/competition-data/training/depths\"\n",
    "VAL_RGB_DIR = \"/kaggle/input/depth-estimation/competition-data/competition-data/validation/images\"\n",
    "VAL_DEPTH_DIR = \"/kaggle/input/depth-estimation/competition-data/competition-data/validation/depths\"\n",
    "TEST_RGB_DIR = \"/kaggle/input/depth-estimation/competition-data/competition-data/testing/images\"\n",
    "PREDICTIONS_FOLDER = \"predictions\"\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 20\n",
    "BASE_LR = 2e-4\n",
    "WEIGHT_DECAY = 1e-3\n",
    "IMAGE_SIZE = 256\n",
    "NUM_WORKERS = 4\n",
    "TTA_FLIPS = True\n",
    "ENSEMBLE_COUNT = 3\n",
    "USE_AMP = True  # Mixed precision training\n",
    "\n",
    "# Enhanced preprocessing for low-light conditions\n",
    "def preprocess_low_light(image):\n",
    "    try:\n",
    "        # Histogram equalization for better contrast\n",
    "        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "        hsv[:,:,2] = cv2.equalizeHist(hsv[:,:,2])\n",
    "        enhanced = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "        return enhanced\n",
    "    except Exception as e:\n",
    "        print(f\"Error in low-light preprocessing: {e}\")\n",
    "        return image\n",
    "\n",
    "# Edge detection for edge-aware loss\n",
    "def detect_edges(depth_map):\n",
    "    try:\n",
    "        # Sobel edge detection\n",
    "        sobelx = cv2.Sobel(depth_map, cv2.CV_64F, 1, 0, ksize=3)\n",
    "        sobely = cv2.Sobel(depth_map, cv2.CV_64F, 0, 1, ksize=3)\n",
    "        edges = np.sqrt(sobelx**2 + sobely**2)\n",
    "        return edges / (np.max(edges) + 1e-8)  # Normalize\n",
    "    except Exception as e:\n",
    "        print(f\"Error in edge detection: {e}\")\n",
    "        return np.zeros_like(depth_map)\n",
    "\n",
    "# Dataset with enhanced preprocessing\n",
    "class DepthDataset(Dataset):\n",
    "    def __init__(self, rgb_dir, depth_dir, transform=None, enhance_low_light=True):\n",
    "        self.rgb_dir = rgb_dir\n",
    "        self.depth_dir = depth_dir\n",
    "        self.transform = transform\n",
    "        self.enhance_low_light = enhance_low_light\n",
    "        \n",
    "        try:\n",
    "            self.filenames = sorted([f for f in os.listdir(rgb_dir) if \n",
    "                                    os.path.isfile(os.path.join(depth_dir, f))])\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing dataset: {e}\")\n",
    "            self.filenames = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            fname = self.filenames[idx]\n",
    "            rgb_path = os.path.join(self.rgb_dir, fname)\n",
    "            depth_path = os.path.join(self.depth_dir, fname)\n",
    "            \n",
    "            # Load and preprocess RGB image\n",
    "            image = cv2.imread(rgb_path)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Failed to load image: {rgb_path}\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Enhanced preprocessing for low-light conditions\n",
    "            if self.enhance_low_light:\n",
    "                image = preprocess_low_light(image)\n",
    "            \n",
    "            # Load and normalize depth map\n",
    "            depth = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED)\n",
    "            if depth is None:\n",
    "                raise ValueError(f\"Failed to load depth map: {depth_path}\")\n",
    "            \n",
    "            depth = depth.astype(np.float32)\n",
    "            depth = cv2.resize(depth, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "            \n",
    "            # Store original min/max for later rescaling\n",
    "            depth_min, depth_max = np.min(depth), np.max(depth)\n",
    "            depth = (depth - depth_min) / (depth_max - depth_min + 1e-8)\n",
    "            \n",
    "            # Apply transformations\n",
    "            if self.transform:\n",
    "                augmented = self.transform(image=image, mask=depth)\n",
    "                image = augmented[\"image\"]\n",
    "                depth = augmented[\"mask\"]\n",
    "            \n",
    "            depth = torch.tensor(depth, dtype=torch.float32).unsqueeze(0)\n",
    "            return image, depth, torch.tensor([depth_min, depth_max], dtype=torch.float32)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample {idx}: {e}\")\n",
    "            # Return a placeholder in case of error\n",
    "            placeholder_img = torch.zeros((3, IMAGE_SIZE, IMAGE_SIZE), dtype=torch.float32)\n",
    "            placeholder_depth = torch.zeros((1, IMAGE_SIZE, IMAGE_SIZE), dtype=torch.float32)\n",
    "            placeholder_params = torch.tensor([0, 1], dtype=torch.float32)\n",
    "            return placeholder_img, placeholder_depth, placeholder_params\n",
    "\n",
    "# UNet blocks with skip connections\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "# Enhanced UNet with skip connections\n",
    "class UNetEnhanced(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder path\n",
    "        self.inc = DoubleConv(in_channels, 64)\n",
    "        self.down1 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(64, 128))\n",
    "        self.down2 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(128, 256))\n",
    "        self.down3 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(256, 512))\n",
    "        self.down4 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(512, 1024))\n",
    "        \n",
    "        # Decoder path with skip connections\n",
    "        self.up1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.conv1 = DoubleConv(1024, 512)  # 512 + 512 (skip connection)\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.conv2 = DoubleConv(512, 256)  # 256 + 256 (skip connection)\n",
    "        \n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.conv3 = DoubleConv(256, 128)  # 128 + 128 (skip connection)\n",
    "        \n",
    "        self.up4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.conv4 = DoubleConv(128, 64)  # 64 + 64 (skip connection)\n",
    "        \n",
    "        self.outc = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        x1 = self.inc(x)        # Skip connection 1\n",
    "        x2 = self.down1(x1)     # Skip connection 2\n",
    "        x3 = self.down2(x2)     # Skip connection 3\n",
    "        x4 = self.down3(x3)     # Skip connection 4\n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        # Decoding with skip connections\n",
    "        x = self.up1(x5)\n",
    "        x = torch.cat([x, x4], dim=1)  # Skip connection 4\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = self.up2(x)\n",
    "        x = torch.cat([x, x3], dim=1)  # Skip connection 3\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        x = self.up3(x)\n",
    "        x = torch.cat([x, x2], dim=1)  # Skip connection 2\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        x = self.up4(x)\n",
    "        x = torch.cat([x, x1], dim=1)  # Skip connection 1\n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        return torch.sigmoid(self.outc(x))  # Sigmoid for 0-1 range\n",
    "\n",
    "# Combined loss function\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, beta=0.3, gamma=0.2):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # L1 loss weight\n",
    "        self.beta = beta    # SSIM loss weight\n",
    "        self.gamma = gamma  # Edge loss weight\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.ssim_module = SSIM(data_range=1.0, size_average=True, channel=1)\n",
    "        \n",
    "    def edge_loss(self, pred, target):\n",
    "        # Compute edges\n",
    "        pred_edges = self._compute_edges(pred)\n",
    "        target_edges = self._compute_edges(target)\n",
    "        # L1 loss on edges\n",
    "        return self.l1_loss(pred_edges, target_edges)\n",
    "    \n",
    "    def _compute_edges(self, x):\n",
    "        # Simple edge detection using sobel filter\n",
    "        batch_size = x.size(0)\n",
    "        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], \n",
    "                              dtype=torch.float32, device=x.device).view(1, 1, 3, 3).repeat(batch_size, 1, 1, 1)\n",
    "        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], \n",
    "                              dtype=torch.float32, device=x.device).view(1, 1, 3, 3).repeat(batch_size, 1, 1, 1)\n",
    "        \n",
    "        edge_x = F.conv2d(x, sobel_x, padding=1, groups=batch_size)\n",
    "        edge_y = F.conv2d(x, sobel_y, padding=1, groups=batch_size)\n",
    "        edges = torch.sqrt(edge_x**2 + edge_y**2 + 1e-8)\n",
    "        return edges\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # L1 loss\n",
    "        l1 = self.l1_loss(pred, target)\n",
    "        \n",
    "        # SSIM loss (1 - SSIM for minimization)\n",
    "        ssim_loss = 1 - self.ssim_module(pred, target)\n",
    "        \n",
    "        # Edge loss\n",
    "        edge = self.edge_loss(pred, target)\n",
    "        \n",
    "        # Combined loss\n",
    "        return self.alpha * l1 + self.beta * ssim_loss + self.gamma * edge\n",
    "\n",
    "# Training function with enhanced features\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device):\n",
    "    best_val_loss = float('inf')\n",
    "    scaler = GradScaler(enabled=USE_AMP)  # Gradient scaling for mixed precision\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        for images, depths, _ in progress_bar:\n",
    "            images, depths = images.to(device), depths.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Mixed precision training\n",
    "            with autocast(enabled=USE_AMP):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, depths)\n",
    "            \n",
    "            # Scaled backpropagation\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "            for images, depths, _ in progress_bar:\n",
    "                images, depths = images.to(device), depths.to(device)\n",
    "                \n",
    "                with autocast(enabled=USE_AMP):\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, depths)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_val_loss,\n",
    "            }, 'best_model.pt')\n",
    "            \n",
    "            # Save intermediate model every 5 epochs as backup\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': val_loss,\n",
    "                }, f'model_epoch_{epoch+1}.pt')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Test-Time Augmentation (TTA)\n",
    "def tta_predict(model, image, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        with autocast(enabled=USE_AMP):\n",
    "            # Original prediction\n",
    "            pred_orig = model(image)\n",
    "            \n",
    "            predictions = [pred_orig]\n",
    "            \n",
    "            if TTA_FLIPS:\n",
    "                # Horizontal flip\n",
    "                image_hflip = torch.flip(image, dims=[3])\n",
    "                pred_hflip = model(image_hflip)\n",
    "                pred_hflip = torch.flip(pred_hflip, dims=[3])\n",
    "                predictions.append(pred_hflip)\n",
    "                \n",
    "                # Vertical flip\n",
    "                image_vflip = torch.flip(image, dims=[2])\n",
    "                pred_vflip = model(image_vflip)\n",
    "                pred_vflip = torch.flip(pred_vflip, dims=[2])\n",
    "                predictions.append(pred_vflip)\n",
    "                \n",
    "                # Both horizontal and vertical flip\n",
    "                image_hvflip = torch.flip(image, dims=[2, 3])\n",
    "                pred_hvflip = model(image_hvflip)\n",
    "                pred_hvflip = torch.flip(pred_hvflip, dims=[2, 3])\n",
    "                predictions.append(pred_hvflip)\n",
    "            \n",
    "            # Average predictions\n",
    "            final_pred = torch.mean(torch.stack(predictions), dim=0)\n",
    "            \n",
    "            return final_pred\n",
    "\n",
    "# Ensemble prediction function\n",
    "def ensemble_predict(models, image, device):\n",
    "    predictions = []\n",
    "    \n",
    "    for model in models:\n",
    "        # TTA prediction from each model\n",
    "        pred = tta_predict(model, image, device)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    # Average predictions from all models\n",
    "    final_pred = torch.mean(torch.stack(predictions), dim=0)\n",
    "    return final_pred\n",
    "\n",
    "# Test function with TTA and ensemble\n",
    "def test(models, test_dir, output_folder, device):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    test_transform = A.Compose([\n",
    "        A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        filenames = sorted(os.listdir(test_dir))\n",
    "        \n",
    "        for fname in tqdm(filenames, desc=\"Processing test images\"):\n",
    "            try:\n",
    "                img_path = os.path.join(test_dir, fname)\n",
    "                image = cv2.imread(img_path)\n",
    "                if image is None:\n",
    "                    raise ValueError(f\"Failed to load test image: {img_path}\")\n",
    "                \n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Apply low-light enhancement\n",
    "                image = preprocess_low_light(image)\n",
    "                \n",
    "                augmented = test_transform(image=image)\n",
    "                image = augmented[\"image\"].unsqueeze(0).to(device)\n",
    "                \n",
    "                # Ensemble prediction with TTA\n",
    "                output = ensemble_predict(models, image, device)\n",
    "                output = output.squeeze().cpu().numpy()\n",
    "                \n",
    "                # Scale output to 0-255 for saving\n",
    "                output = (output * 255).astype(np.uint8)\n",
    "                cv2.imwrite(os.path.join(output_folder, fname), output)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing test image {fname}: {e}\")\n",
    "                # Save a black image as fallback\n",
    "                blank = np.zeros((IMAGE_SIZE, IMAGE_SIZE), dtype=np.uint8)\n",
    "                cv2.imwrite(os.path.join(output_folder, fname), blank)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during testing: {e}\")\n",
    "\n",
    "def create_model_ensemble(num_models, device):\n",
    "    models = []\n",
    "    \n",
    "    try:\n",
    "        # Load the best model if available\n",
    "        if os.path.exists('best_model.pt'):\n",
    "            checkpoint = torch.load('best_model.pt', map_location=device)\n",
    "            model = UNetEnhanced().to(device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            models.append(model)\n",
    "            print(\"Loaded best model for ensemble\")\n",
    "            \n",
    "            # If we have intermediate models, load them too\n",
    "            model_files = [f for f in os.listdir('.') if f.startswith('model_epoch_')]\n",
    "            model_files.sort()\n",
    "            \n",
    "            for i, model_file in enumerate(model_files):\n",
    "                if i >= num_models - 1:  # -1 because we already loaded the best model\n",
    "                    break\n",
    "                    \n",
    "                checkpoint = torch.load(model_file, map_location=device)\n",
    "                model = UNetEnhanced().to(device)\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                models.append(model)\n",
    "                print(f\"Loaded {model_file} for ensemble\")\n",
    "        \n",
    "        # If we don't have enough models, create new ones\n",
    "        while len(models) < num_models:\n",
    "            model = UNetEnhanced().to(device)\n",
    "            models.append(model)\n",
    "            print(f\"Created new model for ensemble (total: {len(models)})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating model ensemble: {e}\")\n",
    "        # Fallback to a single model\n",
    "        model = UNetEnhanced().to(device)\n",
    "        models = [model]\n",
    "    \n",
    "    return models\n",
    "\n",
    "def main():\n",
    "    # Enhanced training transforms with low-light simulation\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.3),\n",
    "        A.RandomRotate90(p=0.3),\n",
    "        A.RandomBrightnessContrast(brightness_limit=(-0.3, 0.3), contrast_limit=(-0.3, 0.3), p=0.7),\n",
    "        A.GaussNoise(var_limit=(10, 50), p=0.5),  # Simulating low-light noise\n",
    "        A.GaussianBlur(blur_limit=(3, 7), p=0.3),  # Simulating motion blur\n",
    "        A.RandomGamma(gamma_limit=(80, 120), p=0.5),  # Gamma adjustment\n",
    "        A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.5),  # Contrast Limited Adaptive Histogram Equalization\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ], additional_targets={'mask': 'mask'})\n",
    "    \n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ], additional_targets={'mask': 'mask'})\n",
    "    \n",
    "    try:\n",
    "        # Initialize datasets\n",
    "        train_dataset = DepthDataset(TRAIN_RGB_DIR, TRAIN_DEPTH_DIR, train_transform)\n",
    "        val_dataset = DepthDataset(VAL_RGB_DIR, VAL_DEPTH_DIR, val_transform)\n",
    "        \n",
    "        # Initialize data loaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            shuffle=True, \n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=True,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            shuffle=False,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Create and initialize model\n",
    "        model = UNetEnhanced().to(DEVICE)\n",
    "        \n",
    "        # Combined loss\n",
    "        criterion = CombinedLoss(alpha=0.5, beta=0.3, gamma=0.2)\n",
    "        \n",
    "        # AdamW optimizer with weight decay\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(), \n",
    "            lr=BASE_LR, \n",
    "            weight_decay=WEIGHT_DECAY\n",
    "        )\n",
    "        \n",
    "        # Cosine annealing learning rate scheduler\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, \n",
    "            T_max=NUM_EPOCHS, \n",
    "            eta_min=BASE_LR / 100\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        model = train(model, train_loader, val_loader, criterion, optimizer, scheduler, NUM_EPOCHS, DEVICE)\n",
    "        \n",
    "        # Create model ensemble for testing\n",
    "        models = create_model_ensemble(ENSEMBLE_COUNT, DEVICE)\n",
    "        \n",
    "        # Test with ensemble and TTA\n",
    "        test(models, TEST_RGB_DIR, PREDICTIONS_FOLDER, DEVICE)\n",
    "        \n",
    "        # Generate CSV for submission\n",
    "        try:\n",
    "            from imgs2csv import images_to_csv_with_metadata\n",
    "            images_to_csv_with_metadata(PREDICTIONS_FOLDER, \"predictions.csv\")\n",
    "            print(\"Successfully generated predictions CSV\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating CSV: {e}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main function: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11466546,
     "isSourceIdPinned": false,
     "sourceId": 96480,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 35.203891,
   "end_time": "2025-03-29T13:01:56.378404",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-29T13:01:21.174513",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
